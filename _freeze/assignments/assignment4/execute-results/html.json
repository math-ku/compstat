{
  "hash": "2dc3f0430e4a3afa1f975e5e8c9de3ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Assignment 4\nsubtitle: Stochastic Optimization\nimage: ../assets/images/assignment4.svg\ndescription:\n  Implement and compare stochastic optimization algorithms for logistic\n  regression smoothing or log-logistic dose-response curves.\n---\n\n\n\nThe last assignment topic is on stochastic optimization. If you draw the topic\n\"Stochastic optimization\" at the oral exam, you will have to present a solution\nof one of the two assignments below.\n\nRemember the five points:\n\n- How can you test that your implementation is correct?\n- Can you implement alternative solutions?\n- Can the code be restructured e.g. by modularization, abstraction or object\n  oriented programming to improve readability?\n- How does the implementation perform (benchmarking)?\n- Where are the bottlenecks (profiling), and what can you do about them?\n\nAs for the other assignments, performance tests are important, but remember to\nmake correct comparisons. Benchmarks are interesting for e.g. gradient\nevaluations, and convergence of the optimization algorithm should be\ninvestigated as a function of real time (not iterations).\n\nWith stochastic optimization algorithms it is of particular interest to\ninvestigate how convergence is affected by the various control parameters for\nthe different algorithms such as size of minibatch and learning rate for SGD.\n\n# A: Logistic Regression Smoothing\n\nConsider the logistic regression model with $y_i \\in \\{0, 1\\}$ and\n$x_i \\in \\mathbb{R}$ such that with $p_i(\\beta) = P(Y_i = 1 \\mid X_i = x_i)$\n\n$$\n\\log \\frac{p_i(\\beta)}{1 - p_i(\\beta)} = f(x_i \\mid \\beta) = (\\varphi_1(x_i), \\ldots, \\varphi_p(x_i))^T \\beta\n$$\n\nfor some $\\beta \\in \\mathbb{R}^p$ and fixed basis functions\n$\\varphi_1, \\ldots, \\varphi_p : \\mathbb{R} \\to \\mathbb{R}$. The assignment is on\nminimizing the _penalized_ negative log-likelihood\n\n$$\nH(\\beta) = - \\frac{1}{N} \\sum_{i=1}^N \\Big(y_i \\log p_i(\\beta) + (1 - y_i) \\log (1 - p_i(\\beta)) \\Big)+ \\lambda \\| f_{\\beta}'' \\|_2^2\n$$\n\nover the basis coefficients $\\beta \\in \\mathbb{R}^p$. Implement stochastic\noptimization algorithms for estimating $\\beta$ for fixed $\\lambda$. Use e.g.\npolynomial or B-spline basis functions, and test the regression model using the\n[`horses.csv`](/data/horses.csv) as well as simulated data. For the former, the\nbinary variable indicates if the horse dies after hospital admission, and $x$ is\nthe temperature of the horse when admitted.\n\nCompare the stochastic gradient algorithms to other optimization algorithms and\ninvestigate how different choices of basis (e.g. the default B-spline basis or\nthe Demmler-Reinsch basis), affect convergence.\n\n# B: Log-Logistic Dose-Response Curves\n\nThe four-parameter log-logistic dose-response model is the nonlinear model of\nthe mean of a real valued random variable $Y$ given $x$ defined as\n$$f(x \\mid \\alpha, \\beta, \\gamma, \\rho) = \\gamma + \\frac{\\rho - \\gamma}{1 + e^{\\beta \\log(x) - \\alpha}}.$$\nHere, $\\alpha, \\beta, \\gamma, \\rho \\in \\mathbb{R}$.\n\nIn this assignment you will consider the model with observations\n$$Y_i = f(x_{i} \\mid \\alpha, \\beta, \\gamma, \\rho) + \\varepsilon_{i}$$ for\n$i = 1, \\ldots, N$, with $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^2)$\nindependent. The parameter vector\n$(\\alpha, \\beta, \\gamma, \\delta) \\in \\mathbb{R}^4$ will be estimated by\nnonlinear least squares, that is, by minimizing $$ \\frac{1}{N} \\sum*{i=1}^N\n\\big(y*{i} - f(x_i \\mid \\alpha, \\beta, \\gamma, \\rho)\\big)^2.$$\n\nImplement stochastic optimization algorithms for estimating the parameters of\nthis model. Compare the resulting optimization algorithm(s) with non-stochastic\nalgorithms e.g. gradient descent or the Newton algorithm. It is particularly\ninteresting to investigate convergence of the algorithms for $N$ large.\n\nYou can sample the $x$-s either from a grid, e.g. $e, e^2, \\ldots, e^{15}$, or\ngenerate $\\log(x)$ from a $\\mathcal{N}(0, \\omega^2)$-distribution. Consider how\nyou could exploit if the $x$-s all fall in a small number of grid points in your\nimplementation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}