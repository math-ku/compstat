{
  "hash": "f6c64d306808ed1db5cdd2e9ed0c3c1e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Assignment 3\nsubtitle: The EM Algorithm\nimage: ../assets/images/assignment3.svg\ndescription:\n  Implement and compare EM algorithms for the $t$-distribution and its mixtures.\n---\n\n\n\nThe third assignment topic is optimization via the EM algorithm. If you draw the\ntopic \"EM algorithm\" at the oral exam, you will have to present a solution of\none of the two assignments below.\n\nRemember the five points:\n\n- How can you test that your implementation is correct?\n- Can you implement alternative solutions?\n- Can the code be restructured e.g. by modularization, abstraction or object\n  oriented programming to improve readability?\n- How does the implementation perform (benchmarking)?\n- Where are the bottlenecks (profiling), and what can you do about them?\n\nAs for the other assignments, performance tests are important, but remember to\nmake correct comparisons. That is, if you benchmark two implementations against\neach other, use exactly the same convergence criterion. Alternatively,\ninvestigate how the algorithms converge as a function of _real_ time (not\niterations).\n\nTest of convergence and experiments with convergence criteria are important to\ninvestigate. Robustness towards the initial choice of parameters is important,\nand testing of the implementation on several (simulated) data sets is a good\nidea.\n\n# A: The EM-Algorithm for the $t$-Distribution\n\nLet $Y = (X, W) \\in \\mathbb{R} \\times (0, \\infty)$ have joint density\n\n$$\nf(x, w) = \\frac{1}{ \\sqrt{ \\pi \\nu \\sigma^2} 2^{(\\nu + 1) /2} \\Gamma(\\nu/2)} w^{\\frac{\\nu - 1}{2}} e^{- \\frac{w}{2}\\left(1 + \\frac{(x - \\mu)^2}{\\nu \\sigma^2}\\right)}.\n$$\n\nThen the marginal density of $X$ is the $t$-distribution (why?) with location\nparameter $\\mu \\in \\mathbb{R}$, scale parameter $\\sigma > 0$, and shape\nparameter $\\nu > 0$. You can initially regard $\\nu$ as fixed. Maximize the\ncomplete data log-likelihood $$\\sum_{i=1}^n \\log f(y_i, w_i)$$ for i.i.d.\nobservations over $(\\mu, \\sigma^2)$ and implement this as a function. Then\nimplement the EM algorithm for estimating $(\\mu, \\sigma^2)$, compare it to other\noptimization algorithms based on the marginal log-likelihood, and investigate\nhow to compute the Fisher information. You can consider generalizing to\nestimating the shape parameter $\\nu$ as well, but then the M-step cannot be\ncomputed analytically and you need the digamma function.\n\n::: {.callout-tip}\n\nWe can think of the joint density of $Y$ as generating first $W$ from a\n$\\chi^2_{\\nu}$-distribution, and then generating $X$ conditionally on $W = w$\nfrom a $\\mathcal{N}(\\mu, \\nu \\sigma^2 / w)$-distribution. For the E-step observe\nthat the density of $W \\mid X = x$ is $\\propto f(x, w)$ with $x$ fixed and\nrecognize this as a $\\Gamma$-distribution.\n\n:::\n\n# B: Mixtures of $t$-Distributions\n\nThe $t$-distribution with location parameter $\\mu \\in \\mathbb{R}$, scale\nparameter $\\sigma > 0$ and shape parameter $\\nu > 0$ has density\n\n$$\nf(x \\mid \\mu, \\sigma^2, \\nu) = \\frac{\\Gamma((\\nu + 1)/2)}{\\sqrt{ \\pi \\nu \\sigma^2} \\Gamma(\\nu/2)} \\left(1 + \\frac{(x- \\mu)^2}{\\nu \\sigma^2}\\right)^{- (\\nu + 1)/2}.\n$$\n\nIn this assignment you will consider iid observations $x_1, \\ldots, x_n$ from\nthe two-component mixture of $t$-distributions, that is, the density is\n\n$$\np f(x \\mid \\mu_1, \\sigma_1^2, \\nu_1) + (1 - p) f(x \\mid \\mu_2, \\sigma_2^2, \\nu_2)\n$$\n\nfor the five parameters $p \\in (0, 1)$, $\\mu_1, \\mu_2 \\in \\mathbb{R}$,\n$\\sigma_1, \\sigma_2 > 0$. The shape parameters $\\nu_1, \\nu_2 > 0$ can be fixed.\n\nImplement the $Q$-function and its gradient, and implement a generalized EM\nalgorithm for estimating the parameters of the two-component mixture of\n$t$-distributions. Compare the EM algorithm with e.g. gradient descent or\nanother optimization algorithm. Investigate also how to implement a computation\nof the Fisher information.\n\nUse simulated data to test your implementations. It may be of interest to\ncompare estimates of location and scale parameters with estimates from a\ntwo-component Gaussian mixture, in particular if you contaminate the sample with\nsome outliers.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}