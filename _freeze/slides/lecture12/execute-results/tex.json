{
  "hash": "a1f818af447b5a43a95766c7ca28f72d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stochastic Gradient Descent\"\n---\n\n\n\n## Today\n\n### Stochastic Gradient Descent\n\nUseful stochastic method for optimization\n\n. . .\n\nCan be used in a **mini-batch** version.\n\n## Minimizing Sums\n\nMany of the functions we are trying to minimize are of the form\n$$\\frac{1}{n} \\sum_{i=1}^n f_i(x).$$\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Gradient Descent\n\nSince $\\nabla \\left(\\sum_{i=1}^n f_i(x)\\right) = \\sum_{i=1}^n \\nabla f_i(x)$, GD\nsteps are $$x_{k+1} = x_k - \\gamma \\sum_{i=1}^n \\nabla f_i(x_k).$$\n\n:::\n\n::: {.column width=\"47%\"}\n\n### Stochastic Gradient Descent\n\nSGD instead takes steps $$x_{k+1} = x_k - \\gamma \\nabla f_{i}(x_k)$$ where $i$\nis an index in $\\{1, \\ldots, n\\}$ drawn at random.\n\n:::\n\n::::\n\n## Unbiasedness\n\nIf $i$ is drawn uniformly at random from $\\{1, \\ldots, n\\}$ then\n$$\\operatorname{E}(\\nabla f_i(x)) = \\nabla f(x).$$\n\nSo SGD gradients are **unbiased** estimates of the full gradient.\n\n. . .\n\n## Motivation\n\nWhy consider SGD?\n\n- Iteration cost is lower: $O(p)$ vs. $O(np)$.\n- Can be more robust to local minima.\n\n### Example: Logistic Regression\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nWe have $$f_i(\\beta) = \\log(1 + e^{-y_i x^T_i \\beta})$$ and\n$$\\nabla f_i(\\beta) = - \\frac{y_i x_i}{1 + e^{y_i x^T_i \\beta}}.$$\n\nSGD typically converges quickly at the start but slows down as it approaches the\nminimum.\n\n:::\n\n::: {.column width=\"47%\"}\n\n![](../images/lecture12-gd-vs-sgd.png)\n\n:::\n\n::::\n\n![](../images/lecture12-gd-sgd-convergence1.png)\n\n![](../images/lecture12-gd-sgd-convergence2.png)\n\n## The Robbins–Monro Convergence Theorem\n\nSuppose $f$ is strongly convex and\n$$\\operatorname{E}(\\rVert \\nabla f(x))\\lVert_2^2) \\leq A + B \\lVert x\\rVert_2^2$$\nfor some constants $A$ and $B$. If $x^*$ is the global minimizer of $f$ then\n$x_n$ converges almost surely toward $x^*$ if \\begin{equation*}\n\\sum*{k=1}^{\\infty} \\gamma_k^2 < \\infty \\quad \\text{and} \\quad\n\\sum*{k=1}^{\\infty} \\gamma_k = \\infty. \\end{equation*}\n\n. . .\n\nThe theorem applies to learning rates satisfying\n$$\\gamma_k \\propto \\frac{1}{k}.$$\n\n### Diminishing Step Sizes\n\nAssume cyclic rule and set $\\gamma_k = \\gamma$ for $n$ updates. We get\n$$x_{k + n} = x_k - \\gamma \\sum_{i=1}^n \\nabla f_i(x_{k + i - 1}).$$\n\n. . .\n\nMeanwhile, **full** gradient descent with step size $n\\gamma$ would give\n$$x_{k + 1} = x_k - \\gamma \\sum_{i=1}^n \\nabla f_i(x_k).$$\n\n. . .\n\nDifference between the two is\n$$\\gamma \\sum_{i=1}^n \\left( \\nabla f_i(x_{k + i - 1}) - \\nabla f_i(x_k) \\right)$$\nwhich does not tend to zero if $\\gamma$ is constant.\n\n### Convergence Rates\n\n#### Gradient Descent\n\nFor convex $f$, GD with diminishing step sizes converges at rate\n$$f(x_n) - f^* = O(1/\\sqrt{k}).$$\n\n. . .\n\nWhen $f$ is differentiable with Lipschitz gradient, we get\n$$f(x_n) - f^* = O(1/k).$$\n\n. . .\n\n#### SGD\n\nFor convex $f$ with diminishing step sizes, SGD converges at rate\n$$\\operatorname{E}f(x_n) - f^* = O(1/\\sqrt{k}).$$\n\n. . .\n\nBut this **does not improve** when $f$ is differentiable with Lipschitz\ngradient.\n\n### Rates for Strongly Convex Functions\n\nWhen $f$ is strongly convex and has Lipschitz gradient, GD satisfies\n$$f(x_n) - f^* = O(\\delta^k), \\qquad \\delta \\in (0, 1).$$\n\n. . .\n\nUnder same conditions, SGD gives us $$\\operatorname{E}(f(x_n) - f^*) = O(1/k).$$\n\n. . .\n\nSo SGD does **not** enjoy linear convergence rates under strong convexity.\n\n## (Mini-)Batch Gradient Descent\n\nIdea: take (mini-)batch $A_k$ of size $b$ of the data and iterate through\n$$x_{k+1} = x_k - \\frac{\\gamma}{b} \\sum_{i \\in A_k} \\nabla f_i(x_k).$$\n\n. . .\n\nEstimates are still unbiased,\n$$\\operatorname{E}\\frac{1}{b}\\sum_{i \\in A_k}\\nabla f_i(x) = \\nabla f(x),$$ but\nvariance is reduced by a factor of $1/b$.\n\n. . .\n\nUnder Lipschitz gradient, rate goes from $O(1/\\sqrt{k})$ to\n$O(1/\\sqrt{bk} + 1/k)^3$.\n\n. . .\n\nTypically more efficient than standard SGD for various computational reasons,\nbut somewhat **less** robust to local minima.\n\n![](../images/lecture12-batch-sizes.png)\n\n## So, Why Use SGD?\n\n- If $n \\gg p$, then we can pick a batch size that is reasonably accurate but\n  still much smaller than $n$. --\n\n- In many applications (e.g. deep learning), we don't care about optimizing to\n  high accuracy. --\n\n- Stochastic gradients help us escape local minima.\n\n## Learning Rate\n\nThe learning rate $\\gamma$ is crucial for SGD, but hard to set.\n\nConvergence theorem does not give much of a hint.\n\n. . .\n\n### A Class of Decay Schedules\n\n$$\\gamma_k = \\frac{\\gamma_0 K}{K + k^a} = \\frac{\\gamma_0 }{1 + K^{-1} k^{a}}$$\nwith initial learning rate $\\gamma_0 > 0$ and constants $K, a > 0$.\n\n. . .\n\nConvergence is ensured by Robbins–Monro if $a \\in (0.5, 1]$.\n\n. . .\n\nFixing the exponent $a$ and picking a target rate, $\\gamma_1$, to be reached\nafter $k_1$ steps, we can solve for $K$ and get\n$$K = \\frac{k_1^a \\gamma_1}{\\gamma_0 - \\gamma_1}.$$\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Examples of learning rate schedules.](lecture12_files/figure-beamer/decay-fig-1.pdf)\n:::\n:::\n\n\n### So, Is The Problem of Setting Learning Rate Solved?\n\nNo, unfortunately not\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n![](../images/lecture12-lrdecay-fast.png)\n\n:::\n\n::: {.column width=\"47%\"}\n\n![](../images/lecture12-lrdecay-slow.png)\n\n:::\n\n::::\n\n## Example: Least Squares Loss\n\nConsider an objective of the type\n$$f(\\beta) = \\frac{1}{2n} \\sum_{i=1}^n  (y_i - \\mu(x_i, \\beta))^2.$$\n\n. . .\n\nGives us gradient\n\n$$\\nabla f_i(\\beta) = -\\nabla_{\\beta} \\mu(x_i, \\beta) (y_i - \\mu(x_i, \\beta) ).$$\n\n. . .\n\nFor a linear model, $\\mu(x_i, \\beta) = x_i^T \\beta$, and\n$\\nabla_{\\beta} \\mu(x_i, \\beta) = x_i$ so\n$$\\nabla f_i(\\beta) = - x_i (y_i - x_i^T \\beta).$$\n\n### A Poisson Regression Model\n\n$$Y_i \\mid Z_i = z_i \\sim \\mathrm{Poisson(e^{\\beta_0 + \\beta_1 z_i})}$$ for\n$\\beta = (\\beta_0, \\beta_1)^T$ and $Z_i$ uniformly distributed in $(-1, 1)$.\n\n. . .\n\nThe conditional mean of $Y_i$ given $Z_i = z_i$ is thus\n$$\\mu(z_i, \\beta) = e^{\\beta_0 + \\beta_1 z_i}.$$\n\n### Least Squares for the Poisson Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 5000\nbeta_true <- c(2, 3)\nmu <- function(z, beta) exp(beta[1] + beta[2] * z)\nbeta <- vector(\"list\", n)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrate <- decay_scheduler(gamma0 = 0.0004, K = 100)\nbeta[[1]] <- c(beta0 = 1, beta1 = 1)\n```\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 2:n) {\n  # Simulating a new data point\n  z <- runif(1, -1, 1)\n  y <- rpois(1, mu(z, beta_true))\n  # Update via squared error gradient\n  mu_old <- mu(z, beta[[i - 1]])\n  beta[[i]] <- beta[[i - 1]] - rate(i) * mu_old * (mu_old - y) * c(1, z)\n}\nbeta[[n]] # Compare this to beta_true\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   beta0    beta1 \n2.037990 2.987941 \n```\n\n\n:::\n:::\n\n\n### Log-Likelihood Loss\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrate <- decay_scheduler(gamma0 = 0.01, K = 100)\nbeta[[1]] <- c(beta0 = 1, beta1 = 1)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 2:n) {\n  # Simulating a new data point\n  z <- runif(1, -1, 1)\n  y <- rpois(1, mu(z, beta_true))\n  # Update via log-likelihood gradient\n  mu_old <- mu(z, beta[[i - 1]])\n  beta[[i]] <- beta[[i - 1]] - rate(i) * (mu_old - y) * c(1, z)\n}\nbeta[[n]] # Compare this to beta_true\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   beta0    beta1 \n2.008452 2.987035 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Convergence for Poisson regression model.](lecture12_files/figure-beamer/pois-sgd-1.pdf)\n:::\n:::\n\n\n## Exercise\n\n- Implement an SGD algorithm to fit standard least-squares regression, i.e.\n  minimize $$\\frac{1}{2n} \\lVert y - X\\beta\\rVert_2^2.$$ --\n\n- Run the algorithm on a simple linear regression problem in two dimensions.\n- **mvtnorm::rmvnorm()** to generate $X$ from a multivariate Gaussian\n  distribution, specifying some non-diagonal covariance structure. --\n\n- Try to visualize both the convergence of the algorithm and the path it takes\n  in the parameter space. For the latter, try something like the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1 <- seq(-1, 3, length = 100)\nbeta2 <- seq(-1, 3, length = 100)\nloss_2d_vectorized <- Vectorize(function(b1, b2) loss_2d(b1, b2, X, y))\nz <- outer(beta1, beta2, loss_2d_vectorized)\n\ncontour(beta1, beta2, z)\n```\n:::\n\n\n## Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgd <- function(\n  par,\n  grad, # Function of parameter and observation index\n  n, # Sample size\n  gamma, # Decay schedule or a fixed learning rate\n  maxiter = 100, # Max epoch iterations\n  sampler = sample, # How data is resampled. Default is a random permutation #<<\n  cb = NULL,\n  ...\n) {\n  gamma <- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)\n  for (k in 1:maxiter) {\n    if (!is.null(cb)) {\n      cb()\n    }\n    samp <- sampler(n)\n    for (j in 1:n) {\n      i <- samp[j]\n      par <- par - gamma[k] * grad(par, i, ...)\n    }\n  }\n  par\n}\n```\n:::\n\n\n## Poisson Regression Model, Batch Learning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50 # Small sample size #<<\nz <- runif(n, -1, 1)\ny <- rpois(n, mu(z, beta_true))\ngrad_pois <- function(par, i) (mu(z[i], par) - y[i]) * c(1, z[i])\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CSwR)\npois_SG_tracer <- tracer(\"par\", Delta = 0)\nrate <- decay_scheduler(gamma0 = 0.02, gamma1 = 0.001, n1 = 1000)\nsgd(c(0, 0), grad_pois, n, rate, 1000, cb = pois_SG_tracer$tracer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.905394 3.162559\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- coefficients(glm(y ~ z, family = poisson))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbeta_hat: 1.898305 3.15986\n```\n\n\n:::\n:::\n\n\n## Poisson Regression Model, Batch Learning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 500 # Larger sample size #<<\nz <- runif(n, -1, 1)\ny <- rpois(n, mu(z, beta_true))\npois_SG_tracer_2 <- tracer(\"par\", Delta = 0)\nrate <- decay_scheduler(gamma0 = 0.02, gamma1 = 0.001, n1 = 100)\nsgd(c(0, 0), grad_pois, n = n, gamma = rate, cb = pois_SG_tracer_2$tracer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.994688 3.022137\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat_2 <- coefficients(glm(y ~ z, family = poisson))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbeta_hat_2: 1.98893 3.027715\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Convergence for the Poisson regression problem using different batch sizes.](lecture12_files/figure-beamer/batch-pois-sgd-fig-1.pdf)\n:::\n:::\n\n\n## A Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nls_model <- function(X, y) {\n  n <- length(y)\n  X <- unname(X) # Strips X of names\n  list(\n    # Initial parameter value\n    par0 = rep(0, ncol(X)),\n    # Objective function\n    f = function(beta) {\n      norm(y - X %*% beta, \"2\")^2 / (2 * n)\n    },\n    # Gradient in a single observation\n    grad = function(beta, i) {\n      xi <- X[i, ]\n      xi * drop(xi %*% beta - y[i])\n    }\n  )\n}\n```\n:::\n\n\n### Data Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews <- readr::read_csv(here::here(\"data\", \"OnlineNewsPopularity.csv\"))\ndim(news)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39644    61\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews <- dplyr::select(\n  news,\n  -url,\n  -timedelta,\n  -is_weekend,\n  -n_non_stop_words,\n  -n_non_stop_unique_tokens,\n  -self_reference_max_shares,\n  -kw_min_max\n)\n# The model matrix without an explicit intercept is constructed using\n# all variables remaining in the data set but the target variable 'shares'\nX <- model.matrix(shares ~ . - 1, data = news)\ny <- log(news$shares)\n```\n:::\n\n\n### Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_raw <- X\n# Standardization\nX <- scale(X, center = FALSE)\nlibrary(zeallot)\n# The '%<-%' unpacking operator is from the zeallot package\nc(par0, f, grad_obs) %<-% ls_model(X, y)\n```\n:::\n\n\n. . .\n\nFor the linear model it's straightforward to compute the solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(lm_news <- lm.fit(X, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.165   0.135   0.080 \n```\n\n\n:::\n\n```{.r .cell-code}\npar_hat <- coefficients(lm_news)\n```\n:::\n\n\n### Objective and Gradient Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3781595\n```\n\n\n:::\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(grad_obs(rep(0, ncol(X)), 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -26.326265   8.755599\n```\n\n\n:::\n\n```{.r .cell-code}\nrange(grad_obs(par_hat, 100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.7749877  0.9229064\n```\n\n\n:::\n:::\n\n\n### Running the SGD Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgd_tracer <- tracer(\"value\", expr = quote(value <- f(par)), Delta = 0)\nsgd(\n  par0,\n  grad = grad_obs,\n  n = nrow(X),\n  gamma = 1e-5,\n  maxiter = 50,\n  cb = sgd_tracer$tracer\n)\nsgd_trace_low <- summary(sgd_tracer)\n```\n:::\n\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntail(sgd_trace_low)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       value    .time\n45 0.4268483 4.708661\n46 0.4262983 4.811584\n47 0.4259705 4.915601\n48 0.4251088 5.019160\n49 0.4245424 5.123923\n50 0.4240064 5.231238\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3781595\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n### Three Additional Variants\n\n\n::: {.cell}\n\n:::\n\n\n#### Fixed but higher learning rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgd_tracer$clear()\nsgd(\n  par0,\n  grad_obs,\n  n = nrow(X),\n  gamma = 5e-5,\n  maxiter = 50,\n  cb = sgd_tracer$tracer\n)\nsgd_trace_high <- summary(sgd_tracer)\n```\n:::\n\n\n. . .\n\n#### Decay Schedule\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgd_tracer$clear()\nrate <- decay_scheduler(gamma0 = 1e-3, gamma1 = 1e-5, a = 0.6, n1 = 50)\nsgd(\n  par0,\n  grad_obs,\n  n = nrow(X),\n  gamma = rate,\n  maxiter = 50,\n  cb = sgd_tracer$tracer\n)\nsgd_trace_decay <- summary(sgd_tracer)\n```\n:::\n\n\n--\n\n#### Full-Batch Gradient Descent with Line Search\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad_f <- function(beta) crossprod(X, X %*% beta - y) / nrow(X)\ngd_tracer <- tracer(\"value\", Delta = 10)\ngd(par0, f, grad_f, gamma = 1, maxiter = 800, cb = gd_tracer$tracer)\ngd_trace <- summary(gd_tracer)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A comparison of the four algorithms](lecture12_files/figure-beamer/news-trace-plot-1.pdf)\n:::\n:::\n\n\n## Exercise\n\nRecall that our learning rate schedule is\n$$\\gamma_k = \\frac{\\gamma_0 K}{K + k^a}$$ with initial learning rate\n$\\gamma_0 > 0$ and constants $K, a > 0$, with convergence ensured if\n$a \\in (0.5, 1]$.\n\n. . .\n\n### Steps\n\n- Take your SGD algorithm and implement the learning rate scheduler that we\n  introduced previously. --\n\n- Run the algorithm on the same simple linear regression problem in two\n  dimensions. --\n\n- Experiment with different values of $K$ and $a$.\n\n## Summary\n\n- Stochastic gradient descent is a popular optimization method for large-scale\n  optimization --\n\n- Convergence results are relatively weak, but for many applications this does\n  not matter. --\n\n- In practice, most implementations use mini-batches.\n\n. . .\n\n## Next Time\n\n- How to improve SGD using momentum and adaptive learning rates\n- Use Rcpp to improve performance of our SGD algorithms\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}