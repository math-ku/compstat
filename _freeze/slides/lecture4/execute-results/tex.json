{
  "hash": "230e6be9d90163bffa8a4337a6bbaadc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scatterplot Smoothing\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## Today\n\n### Nearest neighbors\n\nSimple algorithm for nonparametric smoothing.\n\n### Smoothing Splines\n\nA generalization of polynomial regression.\n\n### S3 Smoother\n\nWe develop a S3 smoother for ggplot2.\n\n## Nuuk Temperature Data\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\np <- ggplot(nuuk_year, aes(Year, Temperature)) +\n  geom_point()\np\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nData available [here](../data/nuuk.dat.txt).\n\n::: {.notes}\n\n- Annual average temperature in Nuuk, Greenland\n\n:::\n\n## Nearest Neighbor Estimation\n\nData: $(x_1, y_1), \\ldots, (x_n, y_n)$\n\nThe $k$ nearest neighbor smoother in $x$ is defined as\n$$\\hat{f}(x) = \\frac{1}{k} \\sum_{j \\in N(x)} y_j$$ where $N(x)$ is the set of\nindices for the $k$ nearest neighbors of $x$.\n\n. . .\n\nThis is an estimator of $$f(x) = E(Y \\mid X = x).$$\n\n::: {.notes}\n\n- $N(x)$ is the neighborhood of $x$.\n\n:::\n\n## Nearest Neighbor Estimation\n\nThe estimator in $x_i$ is $$\\hat{f}_i = \\frac{1}{k} \\sum_{j \\in N_i} y_j$$ where\n$N_i = N(x_i)$. --\n\n$S_{ij} = \\frac{1}{k} \\mathbf{1}_{N_i}(j)$, $\\mathbf{S} = (S_{ij})$, and\n$$\\hat{\\mathbf{f}} = (\\hat{f}_i) = \\mathbf{S} \\mathbf{y}.$$\n\n. . .\n\n$\\hat{\\mathbf{f}}$ is an estimator of the vector $(f(x_i))$.\n\n::: {.notes}\n\n- The vector $\\hat{f}$ can be computed as a linear function of $\\mathbf{y}$.\n- $\\mathbf{S}$ is a relatively sparse $n \\times n$ matrix.\n\n:::\n\n## Linear Smoothers\n\n$(f(x_i))$ of the form $\\mathbf{S} \\mathbf{y}$ for a _smoother matrix_\n$\\mathbf{S}$ is called a _linear smoother_.\n\n. . .\n\nThe $k$ nearest neighbor smoother is a simple example of a linear smoother that\nworks for $x$-values in any metric space.\n\n. . .\n\nThe representation of a linear smoother as a matrix-vector product,\n$$\\mathbf{S} \\mathbf{y}$$ is theoretically useful, but often not the best way to\nactually compute $\\hat{\\mathbf{f}}$.\n\n## Running Mean\n\nWhen $x_i \\in \\mathbb{R}$ we can sort data according to $x$-values and then use\na _symmetric_ neighbor definition:\n\n$$N_i = \\{i - (k - 1) / 2, i - (k - 1) / 2 + 1, \\ldots, i - 1 , i, i + 1, \\ldots,   i + (k - 1) / 2\\}$$\n\n(for $k$ odd.)\n\n. . .\n\nThis simplifies computations: we don't need to keep track of metric comparisons,\nonly the order matters.\n\n::: {.notes}\n\n- Symmetric neighborhoods do not take distance into account.\n- Only need to sort\n- Only order matters\n\n:::\n\n## Running Mean (Naive Implementation)\n\nAssume $y$ is sorted and $k$ is odd.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrun_mean_naive <- function(y, k) {\n  n <- length(y)\n  m <- (k - 1) / 2\n  y <- y / k\n  s <- rep(NA, n)\n\n  for (i in (m + 1):(n - m - 1)) {\n    s[i] <- mean(y[(i - m):(i + m)])\n  }\n\n  s\n}\n```\n:::\n\n\n::: {.notes}\n\n- Can speed this up by replacing mean computation\n\n:::\n\n## Running Mean\n\nImplementation (assuming $y$ is sorted) using the identity\n$$\\hat{f}_{i+1} = \\hat{f}_{i} - \\frac{y_{i - (k-1)/2}}{k} + \\frac{y_{i + (k + 1)/2}}{k}.$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrun_mean <- function(y, k) {\n  n <- length(y)\n  m <- floor((k - 1) / 2)\n  k <- 2 * m + 1 # Ensures k is odd and m = (k - 1) / 2\n  y <- y / k\n  s <- rep(NA, n)\n  s[m + 1] <- sum(y[1:k])\n\n  for (i in (m + 1):(n - m - 1)) {\n    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]\n  }\n\n  s\n}\n```\n:::\n\n\n::: {.notes}\n\n- Ensures $k$ is odd\n\n:::\n\n## Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf_hat <- run_mean(nuuk_year$Temperature, 11)\np + geom_line(aes(y = f_hat), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-NN-plot2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.notes}\n\n- Takes 5 to the right and 5 to the left\n\n:::\n\n## `stats::filter()`\n\nThe R function `stats::filter()` applies linear filtering (moving averages or\nautoregression).\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf_hat_filter <- stats::filter(nuuk_year$Temperature, rep(1 / 11, 11))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]    NA    NA    NA    NA    NA -1.85 -1.72 -1.55 -1.52 -1.48 -0.55\n[12] -0.52 -0.15 -0.23 -0.19 -0.12    NA    NA    NA    NA    NA\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n\ntest_that(\"filter() and run_mean() work the same\", {\n  f_hat <- run_mean(nuuk_year$Temperature, 11)\n  f_hat_filter <- stats::filter(nuuk_year$Temperature, rep(1 / 11, 11))\n  expect_equivalent(f_hat, as.numeric(f_hat_filter))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed \n```\n\n\n:::\n:::\n\n\n. . .\n\n^[There is a `filter()` function in the **dplyr** package (part of the\ntidyverse), so look out for name clashes. Safest to call `stats::filter()`.]\n\n::: {.notes}\n\n- `filter()` general linear filtering function\n- Here, `rep(1, /11, 11)` are MA coefficients\n\n:::\n\n## Benchmarking (Using `bench::press()`)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/runMeanBench-1.pdf)\n:::\n:::\n\n\nThe Matrix-vector multiplication is $O(n^2)$.\n\n. . .\n\nThe two other algorithms are $O(n)$.\n\n## Exercise\n\n$\\mathbf{S}$ is a sparse matrix but it is stored in a dense format currently.\n\n**Note:** The following is **not** the real $\\mathbf{S}$. It is just used for\nillustration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 11\ny <- rnorm(n)\nw <- c(rep(1, k * n), rep(0, n * (n - k)))\nS <- matrix(w, n, n, byrow = TRUE)\n```\n:::\n\n\nUse the [Matrix package](https://CRAN.R-project.org/package=Matrix) to convert\n$\\mathbf{S}$ to a sparse format using `Matrix::Matrix(S, sparse = TRUE)` and\nthen benchmark the sparse multiplication against the dense one. Parameterize the\nbenchmark with $n$ and $k$.\n\nDon't include the time used to construct the matrix in the benchmark.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/ex-sparse-result-1.pdf)\n:::\n:::\n\n\n## Mean-Squared Error\n\nIf data is i.i.d. with $V(Y \\mid X) = \\sigma^2$ and $f(x) = E(Y \\mid X = x)$,\n\n$$\n\\begin{aligned}\n  \\mathrm{MSE}(x) & = \\operatorname{E}(f(x) - \\hat{f}(x))^2 \\\\ & =\n  \\operatorname{Var}\\big(f(x)-\\hat{f}(x)\\big) +\n  \\big(\\operatorname{E}(f(x)-\\hat{f}(x))\\big)^2\\\\ & =\n  \\underbrace{\\frac{\\sigma^2}{k}}_{\\text{variance}} + \\underbrace{\\left(f(x) -\n  \\frac{1}{k} \\sum_{l \\in N(x)} f(x*l)\\right)^2}*{\\text{squared bias}}.\n\\end{aligned}\n$$\n\n. . .\n\n### Bias-Variance Trade-Off\n\n- Small $k$ gives large variance and small bias (if $f$ is smooth).\n- Large $k$ gives small variance and potentially large bias (if $f$ is not\n  constant).\n\n## Leave-One Out Cross-Validation (LOOCV)\n\nThe running mean/nearest neighbour smoother is a _linear smoother_,\n$\\hat{\\mathbf{f}} = \\mathbf{S} \\mathbf{Y}$.\n\n. . .\n\nHow to predict $y_i$ if $(x_i, y_i)$ is left out?\n\n. . .\n\nA _definition_ for a linear smoother is\n$$\\hat{f}^{-i}_i = \\sum_{j \\neq i} \\frac{S_{ij}y_j}{1 - S_{ii}}.$$\n\n. . .\n\nFor many smoothing procedures with a natural \"out-of-sample\" prediction method,\nthe identity above holds.\n\n::: {.notes}\n\n- Can for running mean in principle compute this without leaving anything out.\n\n:::\n\n## LOOCV\n\nIt follows that for **leave-one-out cross validation,**\n\n$$\n\\mathrm{LOOCV} = \\sum_{i} (y_i - \\hat{f}^{-i}_i)^2 =\n\\sum_{i} \\left(\\frac{y_i - \\hat{f}_i}{1 - S_{ii}}\\right)^2\n$$\n\n. . .\n\n### Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloocv <- function(k, y) {\n  f_hat <- run_mean(y, k)\n  mean(((y - f_hat) / (1 - 1 / k))^2, na.rm = TRUE)\n}\n```\n:::\n\n\n^[The implementation removes missing values due to the way we handle the\nboundaries, and it uses `mean()` instead of `sum()` to correctly adjust for\nthis.]\n\n::: {.notes}\n\n- For nearest neighbors, $S_{ii} = \\frac{1}{k}$ for all $i$.\n- **Derive the formula on the blackboard!**\n\n:::\n\n## LOOCV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- seq(3, 40, 2)\ncv_error <- sapply(k, function(kk) loocv(kk, nuuk_year$Temperature))\nk_opt <- k[which.min(cv_error)]\n```\n:::\n\n\n--\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-running-loocv-plot-1.pdf)\n:::\n:::\n\n\n## LOOCV\n\nThe optimal choice of $k$ is 15.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-NN-plot3-1.pdf)\n:::\n:::\n\n\n## Wiggliness\n\n- Running mean is wiggly!\n- We can use kernel smoothing (as we did last week) to fix this\n- But another idea is to use smoothing splines instead.\n\n# Smoothing Splines\n\n## Smoothing Splines\n\nThe minimizer of\n$$L(f) = \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\underbrace{\\int f''(z)^2 \\mathrm{d} z}_{\\|f''\\|_2^2}$$\nis a **cubic spline** --\n\nwith **knots** in the data points $x_i$, that is, a function\n$$f = \\sum_j \\beta_j \\varphi_j$$ where $\\varphi_j$ is a basis function for the\n$n$-dimensional space of such splines.\n\n. . .\n\nCubic splines are piecewise degree 3 polynomials in between knots.\n\n::: {.notes}\n\n- Without penalty term, any interpolating function would work well.\n- Penalty term penalizes solutions with large second derivatives (in norm).\n- **Derive the minimizer on the blackboard!**\n\n:::\n\n### Loss Function\n\nIn vector notation $$\\hat{\\mathbf{f}} = \\boldsymbol{\\Phi}\\hat{\\beta}$$ with\n$\\boldsymbol{\\Phi}_{ij} = \\varphi_j(x_i)$,\n\nand\n\n$$\n\\begin{aligned} L(\\mathbf{f}) & = (\\mathbf{y} - \\mathbf{f})^T (\\mathbf{y} -\n\\mathbf{f}) + \\lambda \\|f''\\|\\_2^2 \\\\ & = ( \\mathbf{y} -\n\\boldsymbol{\\Phi}\\beta)^T (\\mathbf{y} - \\boldsymbol{\\Phi}\\beta) + \\lambda\n\\beta^T \\mathbf{\\Omega} \\beta \\end{aligned}\n$$\n\nwith\n\n$$\n\\mathbf{\\Omega}_{jk} = \\int \\varphi_j''(z) \\varphi_k''(z) \\mathrm{d}z.\n$$\n\n::: {.notes}\n\n- Can rewrite penalty term in quadratic form\n  $\\lambda \\beta^T \\mathbf{\\Omega} \\beta$.\n\n:::\n\n### Solution\n\nThe minimizer is\n$$\\hat{\\beta} = (\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T \\mathbf{y}$$\n\n. . .\n\nwith resulting smoother\n\n$$\n\\hat{\\mathbf{f}} = \\underbrace{\\boldsymbol{\\Phi} (\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T}_{\\mathbf{S}_{\\lambda}} \\mathbf{y}.\n$$\n\n. . .\n\nWe recognize this as a _linear smoother_ with smoother matrix\n$\\mathbf{S}_{\\lambda}$.\n\n::: {.notes}\n\n- Similar to ridge (Tikhonov) regression: OLS + squared l2 penalty.\n\n:::\n\n## Splines in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)\nlibrary(splines)\n# Note the specification of repeated boundary knots\nknots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)\nxx <- seq(0, 1, 0.005)\nb_splines <- splineDesign(knots, xx)\nmatplot(xx, b_splines, type = \"l\", lty = 1)\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/splines-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.notes}\n\n- Repeated knots at boundaries are necessary to ensure function goes throgh\n  control points and is smooth.\n\n:::\n\n## Penalty Matrix\n\n\n::: {.cell}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nomega <- pen_mat(seq(0, 1, 0.1))\n\nimage(Matrix(omega))\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/Omega-Simpson3-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n. . .\n\n^[See [slide source](/lecture4.Rmd) for implementation of `pen_mat()`.]\n\n## Fitting a Smoothing Spline\n\nWe implement the matrix-algebra directly for computing\n$\\mathbf{S}_{\\lambda} \\mathbf{y}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_knots <- nuuk_year$Year\n\n# Note that order does not matter\nknots <- c(rep(range(inner_knots), 3), inner_knots)\n\nphi <- splineDesign(knots, inner_knots)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nomega <- pen_mat(inner_knots)\nsmoother <- function(lambda) {\n  phi %*%\n    solve(\n      crossprod(phi) + lambda * omega, # Phi^T Phi + lambda Omega\n      crossprod(phi, nuuk_year$Temperature) # Phi^T y\n    )\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_line(aes(y = smoother(10)), color = \"steelblue4\") + # Undersmooth\n  geom_line(aes(y = smoother(1000)), color = \"red\") + # Smooth\n  geom_line(aes(y = smoother(100000)), color = \"orange\") # Oversmooth\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-smoother-plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Generalized Cross-Validation\n\nWith $\\mathrm{df} = \\mathrm{trace}(\\mathbf{S}) = \\sum_{i=1}^n S_{ii}$ we replace\n$S_{ii}$ in LOOCV by $\\mathrm{df} / n$ to get the **generalized**\ncross-validation criterion\n$$\\mathrm{GCV} = \\sum_{i=1}^n \\left(\\frac{y_i - \\hat{f}_i}{1 - \\mathrm{df} / n}\\right)^2.$$\n--\n\n### Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngcv <- function(lambda, y) {\n  S <- phi %*% solve(crossprod(phi) + lambda * omega, t(phi))\n  df <- sum(diag(S)) # The trace of the smoother matrix\n  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE)\n}\n```\n:::\n\n\n### GCV-Optimal $\\lambda$\n\nApply `gcv()` across grid of $\\lambda$-values and choose $\\lambda$ that\nminimizes GCV.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- seq(50, 250, 2)\ngcv <- sapply(lambda, gcv, y = nuuk_year$Temperature)\nlambda_opt <- lambda[which.min(gcv)]\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/gc-plot-1.pdf)\n:::\n:::\n\n\n### GCV-Optimal Smoothing Spline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmooth_opt <- smoother(lambda_opt)\np + geom_line(aes(y = smooth_opt), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-spline-opt-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Using `smooth.spline()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmooth <- smooth.spline(nuuk_year$Year, nuuk_year$Temperature, all.knots = TRUE)\np + geom_line(aes(y = smooth$y), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/nuuk-spline-opt2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.notes}\n\n- Disable fast heuristic to have similar results as us.\n\n:::\n\n## Efficient Computations\n\nIn practice we use $p < n$ basis functions.\n\nUsing the singular value decomposition $$\\Phi = \\mathbf{U} D \\mathbf{V}^T$$ --\n\nit holds that\n$$\\mathbf{S}_{\\lambda} = \\widetilde{\\mathbf{U}}  (I + \\lambda  \\Gamma)^{-1} \\widetilde{\\mathbf{U}}^T$$\nwhere $\\widetilde{\\mathbf{U}} = \\mathbf{U} \\mathbf{W}$ and\n$$D^{-1} \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V} D^{-1} = \\mathbf{W} \\Gamma \\mathbf{W}^T.$$\n\n::: {.notes}\n\n- Can be done whether we use all or a subset of our basis functions\n- $S_\\lambda$ is diagonalized\n- DERIVE $\\mathbf{S}_\\lambda$ ON BLACKBOARD!\n\n:::\n\n### Interpretation\n\n- The coefficients, $\\hat{\\beta} = \\widetilde{\\mathbf{U}}^Ty$, are computed for\n  expanding $y$ in the basis given by the columns of $\\widetilde{\\mathbf{U}}$.\n  --\n\n- The $i$-th coefficient is shrunk towards 0,\n  $$\\hat{\\beta}_i(\\lambda) = \\frac{\\hat{\\beta}_i}{1 + \\lambda \\gamma_i}.$$ --\n\n- The smoothed values, $\\widetilde{\\mathbf{U}} \\hat{\\beta}(\\lambda)$, are\n  computed as an expansion using the shrunken coefficients.\n\n### The Demmler-Reinsch Basis (Columns of $\\widetilde{\\mathbf{U}}$)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/spline-diagonalization-1.pdf)\n:::\n:::\n\n\n### The Eigenvalues $\\gamma_i$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/spline-gamma-1.pdf)\n:::\n:::\n\n\n## S3 Smoother\n\n## A LOESS Smoother\n\nLOESS: locally weighted scatterplot smoothing\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth()\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/loess-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n^[Default behavior in ggplot2 is to use LOESS for small data sets and\ngeneralized additive models (GAMs) for larger data sets.]\n\n## LOESS\n\n- A mix of nearest neighbors and smoothing splines.\n- Implemented in `loess()`\n- Does not automatically select the span (tuning parameter)\n- Default span is 0.75, often too large\n- Nonlinear, so the formulas for linear smoothers do not apply.\n- Instead use 5- or 10-fold cross validation for tuning.\n- Or use the GCV criterion (using `trace.hat` entry in the returned object).\n\n. . .\n\nLoess is a **robust** smoother (linear smoothers are not) and relatively\ninsensitive to outliers.\n\n## Another LOESS Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"loess\", span = 0.5)\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/loess2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## A Linear \"Smoother\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/linear-smoother-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## A Polynomial Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"lm\", formula = y ~ poly(x, 5))\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/poly-smoother-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Another Polynomial Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"lm\", formula = y ~ poly(x, 20))\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/poly-smother2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## A Spline Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"gam\", formula = y ~ s(x))\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/spline-smoother-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Another Spline Smoother\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"gam\", formula = y ~ s(x, k = 100))\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/spline-smoother2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Smoothing with ggplot2\n\nThe `geom_smooth()` function easily adds miscellaneous model fits or scatter\nplot smoothers to the scatter plot.\n\n. . .\n\nSpline smoothing is performed via the `gam()` function in the mgcv package,\nwhereas loess smoothing is via the `loess()` function in the stats package.\n\n. . .\n\nAny \"smoother\" can be used that supports a formula interface and has a\nprediction function adhering to the standards of `predict.lm()`.\n\n## Our Running Mean Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrun_mean <- function(y, k) {\n  n <- length(y)\n  m <- floor((k - 1) / 2)\n  k <- 2 * m + 1 # Ensures k is odd and m = (k - 1) / 2\n  y <- y / k\n  s <- rep(NA, n)\n  s[m + 1] <- sum(y[1:k])\n\n  for (i in (m + 1):(n - m - 1)) {\n    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]\n  }\n\n  s\n}\n```\n:::\n\n\n## An Interface for `geom_smooth()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunning_mean <- function(..., data, k = 5) {\n  ord <- order(data$x)\n  s <- run_mean(data$y[ord], k = k)\n  structure(\n    list(x = data$x[ord], y = s),\n    class = \"running_mean\"\n  )\n}\n```\n:::\n\n\n. . .\n\n### Prediction Method\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict.running_mean <- function(object, newdata, ...) {\n  approx(object$x, object$y, newdata$x)$y # Linear interpolation\n}\n```\n:::\n\n\n## A Running Mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\np + geom_smooth(method = \"running_mean\", se = FALSE, n = 200)\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Specifying Arguments to Our Method\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_smooth(\n    method = \"running_mean\",\n    se = FALSE,\n    n = 200,\n    method.args = list(k = 13)\n  )\n```\n\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Handling Boundary Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunning_mean <- function(..., data, k = 5, boundary = NULL) {\n  ord <- order(data$x)\n  y <- data$y[ord]\n  n <- length(y)\n  m <- floor((k - 1) / 2)\n  if (m > 0 && !is.null(boundary)) {\n    if (boundary == \"pad\") {\n      y <- c(rep(y[1], m), y, rep(y[n], m))\n    }\n    if (boundary == \"rev\") {\n      y <- c(y[m:1], y, y[n:(n - m + 1)])\n    }\n  }\n  s <- run_mean(y, k = k)\n  if (!is.null(boundary)) {\n    s <- na.omit(s)\n  }\n  structure(list(x = data$x[ord], y = s), class = \"running_mean\")\n}\n```\n:::\n\n\n### No Boundary Adjustment\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-7-1.pdf)\n:::\n:::\n\n\n### Padding\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n### Reversion\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture4_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}