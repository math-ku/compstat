{
  "hash": "3485849841557730297d9e121c4960a3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Variations on Stochastic Gradient Descent\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## Last Time\n\n:::: {.columns}\n\n::: {.column width=\"39%\"}\n\n### SGD\n\nIntroduced stochastic gradient descent (SGD) and mini-batch version thereof.\n\n### Problems\n\nWe indicated that there were problems with vanilla SGD: poor convergence,\nerratic behavior.\n\n:::\n\n::: {.column width=\"51%\"}\n\n### Mini-Batch SGD\n\n\\begin{algorithm}[H] \\KwData{$\\gamma_0 >0$} \\For{$k \\gets 1, 2, \\dots$}{\n$A_k \\gets$ random mini-batch of $m$ samples\\;\n$x_k \\gets x_{k-1} - \\frac{\\gamma_k }{|A_k|} \\sum_{i \\in A_k} \\nabla f_i(x_{k-1})$\\;\n} \\caption{Mini-Batch SGD} \\end{algorithm}\n\n:::\n\n::::\n\n---\n\n## Today\n\nHow can we improve stochastic gradient descent?\n\n### Momentum\n\nBase update on combination of gradient step and previous point.\n\nTwo versions: Polyak and Nesterov momentum\n\n### Adaptive Gradients\n\nAdapt learning rate to particular feature.\n\n---\n\n## Momentum\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n### Basic Idea\n\nGive the particle **momentum**: like a heavy ball\n\nNot specific to stochastic GD!\n\n### Polyak Momentum\n\nClassical version (Polyak, 1964)\n\n- $\\mu \\in [0, 1)$ decides strength of momentum; $\\mu = 0$ gives standard\n  gradient descent\n- Guaranteed convergence for quadratic functions\n\n:::\n\n::: {.column width=\"45%\"}\n\n### GD with Polyak Momentum\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $\\mu \\in [0,1)$, $x_{-1} = x_0$}\n\\For{$k \\gets 1, 2, \\dots$}{ $x*k \\gets x*{k-1} - \\gamma \\nabla f(x*{k-1}) + \\mu\n(x*{k-1} - x\\_{k-2}) $\\; } \\caption{GD with Polyak Momentum} \\end{algorithm}\n\n:::\n\n::::\n\n## Polyak Momentum in Practice\n\n![Trajectories of GD for different momentum values for a least-squares problem](../images/momentum-surface.pdf)\n\n## Local Minima\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nFor nonconvex $f$, gradient descent may get stuck at a local minimum.\n\n:::\n\n::: {.column width=\"45%\"}\n\n![](../images/escape-minima-gd-0.pdf)\n\n$\\mu = 0$\n\n:::\n\n::::\n\n---\n\n## Escaping Local Minima\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nWith momentum, we can (sometimes) remedy this problem.\n\n:::\n\n::: {.column width=\"45%\"}\n\n![](../images/escape-minima-mom-0.pdf)\n\n$\\mu = 0.8$\n\n:::\n\n::::\n\n---\n\n## Convergence Failure\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nFor some problems, the momentum method may fail to converge [Lessard et al.,\n2016].\n\nConsider\n\n$$\nf(x) =\n\\begin{cases}\n  \\frac{25x^2}{2}            & \\text{if } x < 1,        \\\\\n  \\frac{x^2}{2} +24x - 12    & \\text{if } 1 \\leq x < 2, \\\\\n  \\frac{25x^2}{2} - 24x + 36 & \\text{if } x \\geq 2.\n\\end{cases}\n$$\n\nFor an \"optimal\" step size $1 = 1/L$ with $L = 25$, GD momentum steps converge\nto three limit points.\n\n:::\n\n::: {.column width=\"45%\"}\n\n![](../images/momentum-failure.pdf)\n\nInitialized at $x_0 = 3.2$, the algorithm fails to converge.\n\n:::\n\n::::\n\n---\n\n## Nesterov Momentum\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $\\mu \\in [0,1)$}\n\\For{$i \\gets 1,2,\\dots$}{ $v_k \\gets x_{k-1} - \\gamma \\nabla f(x_{k-1})$\\;\n$x_k \\gets v_{k} + \\mu (v_{k} - v_{k-1})$\\; } \\caption{GD with Nesterov\nMomentum} \\end{algorithm}\n\n- Overcomes convergence problem of classical (Polyak) momentum.\n- First appeared in article by Nesterov (1983).\n\n---\n\n## Nesterov: Sutskever Perspective\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nConsider two iterations of Nesterov algorithm:\n\n$$\n\\begin{aligned}\n  v_{k}               & = x_{k-1} - \\gamma \\nabla f\\left(x_{k-1} \\right)         \\\\\n  x_{k}   & = v_{k} + \\mu\\left(v_{k} - v_{{k-1}} \\right) \\\\\n  v_{k+1} & = x_{k} - \\gamma \\nabla f\\left(x_{k} \\right) \\\\\n  x_{k+1}             & = v_{k+1} + \\mu\\left(v_{k+1} - v_{k} \\right)\n\\end{aligned}\n$$\n\n:::\n\n::: {.column width=\"45%\"}\n\nIdea: focus on interim step:\n\n$$\n\\begin{aligned}\n  x_{k}   & = v_{k} + \\mu\\left(v_{k} - v_{{k-1}} \\right) \\\\\n  v_{k+1} & = x_{k} - \\gamma \\nabla f\\left(x_{k} \\right)\n\\end{aligned}\n$$\n\nReindex:\n\n$$\n\\begin{aligned}\n  x_{k} & = v_{k-1} + \\mu\\left(v_{k-1} - v_{{k-2}} \\right) \\\\\n  v_{k} & = x_{k} - \\gamma \\nabla f\\left(x_{k} \\right)\n\\end{aligned}\n$$\n\n:::\n\n::::\n\n---\n\nBut since $x_k = v_k$ for $k=1$ by construction, we can swap $x_k$ for $v_k$ and\nget the update\n\n$$\nx_k = x_{k-1} + \\mu(x_{k-1} - x_{k-2}) - \\gamma \\nabla f(x_{k} + \\mu(x_{k-1} - x_{k-2})).\n$$\n\n![Illustration of Nesterov and Polyak momentum](../images/momentum-illustration.pdf)\n\n---\n\n## Optimal Momentum\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nFor gradient descent with $\\gamma = 1/L$, the optimal choice of $\\mu_k$ for\ngeneral convex and smooth $f$ is\n\n$$\n\\mu_k = \\frac{a_{k-1} - 1}{a_{k}}\n$$\n\nfor a series of\n\n$$\na_k = \\frac{1 + \\sqrt{4a_{k-1}^2 + 1}}{2}\n$$\n\nwith $a_0 = 1$ (and hence $\\mu_1 = 0$).\n\nFirst step ($k = 1$) is just standard gradient descent.\n\n:::\n\n::: {.column width=\"45%\"}\n\n![Optimal momentum for Nesterov acceleration (for GD).](../images/nesterov-weights.pdf)\n\n:::\n\n::::\n\n---\n\n## Convergence\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n- Convergence rate with Nesterov acceleration goes from $O(1/k)$ to $O(1/k^2)$\n- This is **optimal** for a first-order method.\n- Convergence improves further for quadratic and strongly convex!\n\n:::\n\n::: {.column width=\"45%\"}\n\n![Suboptimality plot for a logistic regression problem with $n = 1,000$, $p = 100$.](../images/momentum-convergence.pdf)\n\n:::\n\n::::\n\n---\n\n## Exercise: Rosenbrock's Banana\n\n### Steps\n\n1. Minimize $f(x_1, x_2) = (a - x_1)^2 + b(x_2 - x_1^2)^2$ with $a = 1$ and\n   $b = 100$ using GD with Polyak momentum. Optimum is $(a, a^2)$.\n2. Implement gradient descent.\n3. Add Polyak momentum.\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $\\mu \\in [0,1)$}\n\\For{$k \\gets 1, 2, \\dots$}{ $x*k \\gets x*{k-1} - \\gamma \\nabla f(x*{k-1}) + \\mu\n(x*{k-1} - x\\_{k-2}) $\\; } \\caption{GD with Polyak Momentum} \\end{algorithm}\n\n:::\n\n::: {.column width=\"45%\"}\n\n### Plot Contours\n\n```r\nx1 <- seq(-2, 2, length.out = 100)\nx2 <- seq(-1, 3, length.out = 100)\nz <- outer(x1, x2, f)\ncontour(x1, x2, z, nlevels = 20)\n```\n\n:::\n\n::::\n\n---\n\n## What About SGD?\n\n- So far, we have mostly talked about standard GD, but we can use momentum\n  (Polyak or Nesterov) for SGD as well.\n- For standard GD, Nesterov is the dominating method for achieving acceleration;\n  for SGD, Polyak momentum is actually quite common.\n- In term of convergence, all bets are now off.\n- No optimal rates anymore, just heuristics.\n\n## Adaptive Gradients\n\n### General Idea\n\nSome directions may be important, but feature information is **sparse**.\n\n---\n\n### AdaGrad\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\nStore matrix of gradient history,\n\n$$\nG_k = \\sum_{i = 1}^k \\nabla f(x_k) \\nabla f(x_k)^\\intercal,\n$$\n\nand update by multiplying gradient with $G_k^{-1/2}$.\n\n:::\n\n::: {.column width=\"45%\"}\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $G = 0$} \\For{$k \\gets 1, 2, \\dots$}{\n$G_k \\gets G_{k-1} + \\nabla f(x_{k-1}) \\nabla f(x_{k-1})^\\intercal$\\;\n$x_k \\gets x_{k-1} - \\gamma G_k^{-1/2} \\nabla f(x_{k-1})$\\; } \\caption{AdaGrad}\n\\end{algorithm}\n\n:::\n\n::::\n\n---\n\n### Effects\n\n- Larger learning rates for sparse features\n- Step-sizes adapt to curvature.\n\n---\n\n## AdaGrad In Practice\n\n### Simplified Version\n\nComputing $\\nabla f \\nabla f^\\intercal$ is $O(p^2)$; expensive!\n\nReplace $G^{-1/2}_k$ with $\\mathrm{diag}(G_k)^{-1/2}$\n\n---\n\n### Avoid Singularities\n\nAdd a small $\\epsilon$ to diagonal.\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $\\epsilon > 0$}\n\\For{$k \\gets 1, 2, \\dots$}{\n$G_k \\gets G_{k-1} + \\mathrm{diag}\\big(\\nabla f(x_{k-1})^2\\big)$\\;\n$x_k \\gets x_{k-1} - \\gamma \\mathrm{diag}\\big(\\epsilon I_p + G_k\\big)^{-1/2} \\nabla f(x_{k-1})$\\;\n} \\caption{Simplified AdaGrad} \\end{algorithm}\n\n---\n\n## RMSProp\n\nAcronym for **R**oot **M**ean **S**quare **Prop**agation [Hinton, 2018]\n\n### Idea\n\nDivide learning rate by running average of magnitude of recent gradients:\n\n$$\nv(x,k) = \\xi v(x, k -1) + (1 - \\xi)\\nabla f(x_k)^2\n$$\n\nwhere $\\xi$ is the **forgetting factor**.\n\nSimilar to AdaGrad, but uses **forgetting** to gradually decrease influence of\nold data.\n\n---\n\n\\begin{algorithm}[H] \\KwData{$\\gamma >0$, $\\xi > 0$} \\For{$k \\gets 1, 2, \\dots$,\n$\\xi \\in [0, 1)$}{ $v_k = \\xi v_{k-1} + (1 - \\xi) \\nabla f(x_{k-1})$\\;\n$x_k \\gets x_{k-1} - \\frac{\\gamma}{\\sqrt{v_k}} \\odot \\nabla f(x_{k-1})$\\; }\n\\caption{RMSProp} \\end{algorithm}\n\n---\n\n## Adam\n\nAcronym for **Ada**ptive **m**oment estimation [Kingma & Ba, 2015]\n\nBasically RMSProp + _momentum_ (for both gradients and second moments thereof)\n\nPopular and still in much use today.\n\n---\n\n## Implementation Aspects of SGD\n\n### Loops\n\nAny language (e.g. R) that imposes overhead for loops, will have a difficult\ntime with SGD.\n\n---\n\n### Storage Order\n\nIn a regression setting, when indexing a single observation at a time, slicing\nrows is not efficient when $n$ is large.\n\nWe can either transpose first or use a row-major storage order (not possible in\nR).\n\n---\n\n## Example: Nonlinear Least Squares\n\n:::: {.columns}\n\n::: {.column width=\"42%\"}\n\nLet's assume we're trying to solve a least-squares type of problem:\n\n$$\nf(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n  \\left(y_i - g(\\theta; x_i, y_i)\\right)^2\n$$\n\nwith $\\theta = (\\alpha, \\beta)$ and\n\n$$\ng(\\theta; x, y) = \\alpha \\cos(\\beta x).\n$$\n\nThen\n\n$$\n\\nabla_{\\theta} f(\\theta) =\n\\begin{bmatrix}\n  \\cos(\\beta x) \\\\ - \\alpha x \\sin(\\beta x)\n\\end{bmatrix}.\n$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n![Simulation from problem](../images/nonlinear-data.pdf)\n\n:::\n\n::::\n\n---\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n![Perspective plot of function](../images/nonlinear-persp.pdf)\n\n:::\n\n::: {.column width=\"50%\"}\n\n![Contour plot of $f$](../images/nonlinear-contour.png)\n\n:::\n\n::::\n\n---\n\n## Variants\n\nWe will consider three variants:\n\n- Batch gradient descent\n- Batch gradient descent **with momentum**\n- Adam\n\nIn each case, we'll use a batch of size $m=50$.\n\nWe initialize at different starting values and see how well the algorithm\nconverges.\n\n---\n\n![Convergence of different algorithms](../images/nonlinear-convergence-0.pdf)\n\n---\n\n![Updates of $\\alpha$ parameter over time for the different algorithms over different starting values.](../images/nonlinear-alpha.pdf)\n\n---\n\n![Updates of $\\beta$ parameter over time for the different algorithms over different starting values.](../images/nonlinear-beta.pdf)\n\n---\n\n## Rcpp\n\nVery attractive for stochastic methods due to all the loop constructs and\nslicing.\n\nHowever, Rcpp lacks linear algebra functions.\n\n### Approaches\n\n- Still use only Rcpp (but then you need to write your own linear algebra\n  functions[^not-recommended])\n- Use RcppEigen or RcppArmadillo.\n\n[^not-recommended]: Not recommended!\n\n---\n\n## Exercise: Rosenbrock Revisited\n\n### Steps\n\n1. Convert your gradient descent algorithm to C++ through Rcpp.\n2. Modify it to be a stochastic gradient descent algorithm instead.\n\n### Hints\n\n- Use the Rcpp function `Rcpp::sugar()` to sample indices.\n- Don't bother with a stopping criterion to begin with; just set a maximum\n  number of iterations.\n- You can return a list by calling\n  `Rcpp::List::create(Rcpp::named(\"name\") = x)`.\n- Use a pure Rcpp implementation.\n\n---\n\n## Summary\n\nWe introduced several new concepts:\n\n- Polyak momentum,\n- Nesterov acceleration (momentum), and\n- adaptive gradients (AdaGrad),\n\nWe practically implemented versions of gradient descent and stochastic gradient\ndescent with momentum.\n\n### Additional Resources\n\n- [@gohWhyMomentumReally2017] is an article on momentum in gradient descent with\n  lots of interactive visualizations.\n\n## Next Time\n\n### Reproducibility\n\nHow to make your code reproducible\n\nWe build an R package.\n\n### Summary\n\nWe summarize the course.\n\n### Exam Advice\n\nWe talk about the upcoming oral examinations.\n\n---\n\nThank you!\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}