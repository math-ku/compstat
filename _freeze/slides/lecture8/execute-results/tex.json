{
  "hash": "f84912283b1debafa571c9fcb6baba24",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimization\"\n---\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Today\n\n### Optimization\n\n- What is an optimization problem?\n- Types of optimization problems\n- Zero, first, and second order methods\n- Line search and backtracking\n- Poisson regression example\n\n# Optimization\n\n## Optimization\n\n- ## Optimization is the process of trying to optimize an objective function $f$.\n\n- Typically, we are interested in either **minimizing** or **maximizing** the\n  function. --\n\n- Since minimizing $f$ is equivalent to maximizing $-f$, we will focus on\n  minimization.\n\n. . .\n\n### Examples\n\n- Fitting a generalized linear model by minimizing the negative log-likelihood\n- Finding the shortest path in a network\n- Minimizing variance in a portfolio\n- Finding the best hyperparameters for a model\n\n## Standard Form\n\nAn optimization problem is typically written in the form\n\n$$\n\\begin{aligned}\n  &\\operatorname*{minimize}_x     && f(x) \\\\\n  &\\operatorname{subject to}      && g_i(x) \\leq 0, \\quad i = 1, \\ldots, m \\\\\n  &                               && h_j(x) = 0, \\quad j = 1, \\ldots, k.\n  \\end{aligned}\n$$\n\nwhere\n\n- $f$ is the objective function,\n- $g_i$ are inequality constraints, and\n- $h_j$ are equality constraints.\n\n. . .\n\n### Solution\n\n$$x^* = \\operatorname{argmin}_x f(x)$$\n\n### Example: OLS\n\nOrdinary least squares is an optimization problem.\n\nIn standard form, it is\n\n$$\n  \\begin{aligned}\n  &\\operatorname*{minimize}_{\\beta \\in \\mathbb{R}^p}     &&f(\\beta) = \\frac{1}{2} \\lVert y - X\\beta\\rVert_2^2.\n  \\end{aligned}\n$$\n\n. . .\n\n### Solution\n\n$$\\beta^* = \\operatorname{argmin}_{\\beta \\in \\mathbb{R}^p} f(\\beta).$$\n\n# Types of Problems\n\n## Taxonomy\n\n### Convexity\n\n- **Convex**\n- Quasiconvex\n- Nonconvex\n\n. . .\n\n### Smoothness\n\n- **Smooth**\n- Nonsmooth\n\n. . .\n\n### Constraints\n\n- **Unconstrained**\n- Constrained\n\n## Convex Optimization Problems\n\nStandard form:\n\n$$\n\\begin{aligned}\n  &\\operatorname*{minimize}_x     && f(x) \\\\\n  &\\operatorname{subject to}      && g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n  &                               && a_i^T x = b_i, \\quad j = 1, \\ldots, k,\n  \\end{aligned}\n$$\n\nwhere $f$ and $g_i$ are convex.\n\n### Convex Functions\n\nA function $f$ is convex iff for all $x, y \\in \\operatorname{dom} f$ and\n$\\lambda \\in [0, 1]$\n$$f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1 - \\lambda) f(y).$$\n\n. . .\n\n#### First-Order Conditions\n\nIf $f$ is **differentiable**, then the condition is equivalent to\n$$f(y) \\geq f(x) + \\nabla f(x)^T(y - x).$$\n\n### Optimality\n\nAny (local) minimum is **global** (but not neccessarily unique).\n\n#### First-Order Conditions\n\nFor differentiable $f$, $x$ is optimal iff\n$$\\nabla f(x)^T(y - x) \\geq 0\\quad\\text{for all}\\quad y \\in C,$$ where $C$ is\nthe feasible set.\n\n. . .\n\n#### Second-Order Conditions\n\nIf $f$ is twice-differentiable, then $x$ is optimal iff\n$$\\nabla^2f(x) \\succeq 0.$$\n\n### Quasiconvex Optimization Problems\n\nStandard form:\n\n$$\n\\begin{aligned}\n  &\\operatorname*{minimize}_x     && f(x) \\\\\n  &\\operatorname{subject to}      && g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n  &                               && A^T x = b,\n  \\end{aligned}\n$$\n\nwhere $g_i$ are convex but $f$ is **quasiconvex**.\n\n#### Examples\n\n- Utility functions (economics)\n- The floor function $f(x) = \\lfloor x \\rfloor$\n\n. . .\n\n#### Algorithms\n\nWon't cover here, but for instance the **bisection** method.\n\n## Nonconvex Optimization Problems\n\n- Generally hard to solve.\n- No standard form (can be anything).\n- Local minima are not necessarily global.\n\n. . .\n\n### Examples\n\n- Deep learning\n- Hyperparameter tuning (cross-validation)\n- Combinatorial problems\n\n. . .\n\n### Algorithms\n\nWon't cover here, but typically **stochastic algorithms** (next week) or\n**global optimization methods** (not in this course).\n\n$x^2$ (convex), $\\sin(x + \\pi)$ (non-convex), and $\\sqrt{|x|}$ (quasi-convex).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture8_files/figure-beamer/convexity-types-1.pdf)\n:::\n:::\n\n\n# Solving Optimization Problems\n\n## Types of Optimization Methods\n\n### Direct Methods\n\n- Solve a system of equations\n- Get solution up to machine precision\n- **Examples:** solving a linear system of equations\n\n. . .\n\n### Iterative Optimization\n\n- Incrementally update optimization variable\n- Stop after some number of iterations based on convergence criterion. --\n\n- **Examples:** gradient descent, hill-climbing\n\n### Direct Methods\n\nAnalytical solution, up to machine precision.\n\n#### Example: OLS\n\nSolve $$X^TX \\beta = X^Ty$$ for $\\beta \\in \\mathbb{R}^p$.\n\nHow do you solve this in practice? In R you simply call\n\n```r\nsolve(crossprod(X), crossprod(X, y))\n```\n\nbut what happens under the hood?\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### Zero-Order Methods\n\nUse only the function value.\n\n#### Examples\n\n- Grid search (cross validation)\n- Genetic algorithms\n- Bisection\n\n:::\n\n::: {.column width=\"50%\"}\n\n### First-Order Methods\n\nUse the gradient of the function (**first** derivative).\n\n#### Examples\n\n- **Gradient descent**\n- Conjugate gradient\n\n:::\n\n::::\n\n### Second-Order Methods\n\nUse the gradient of the function (**first** derivative).\n\n#### Examples\n\n- **Newton's Method**\n- Trust-region methods ]\n\n## Gradient Descent\n\nThe quintessential first-order method\n\nRepeat until convergence: $$x^{(k+1)} = x^{(k)} - t \\nabla f(x^{(k)}).$$ with\n$t > 0$ being the step size.\n\n. . .\n\n### Properties\n\n- Converges to a local minimum if $f$ is convex and differentiable and $t$ small\n  enough.\n- Requires $f$ to be differentiable and the gradient to be continuous.\n\n### Derivation of Update\n\nTake the second-order Taylor expansion of $f$ around $x$:\n$$f(y) \\approx \\hat{f}(y) = f(x) + \\nabla f(x)^T (y - x) + \\frac{1}{2}(y- x)^T \\nabla^2 f(x) (y - x).$$\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nReplace $\\nabla^2 f(x)$ with $\\frac{1}{t} I$ and minimize w.r.t $y$ to get\n$$x^+ = x - t\\nabla f(x).$$\n\nBut how do we choose $t$?\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](../images/gd-approx.png){width=85%}\n\n:::\n\n::::\n\n### Too Small Step Size\n\n![](../images/gd-stepsize-small.png){width=50%}\n\n### Too Large Step Size\n\n![](../images/gd-stepsize-large.png){width=50%}\n\n### Perfect!\n\n![](../images/gd-stepsize-good.png){width=50%}\n\n### Analytical Step Size\n\n#### Theorem\n\nFor gradient descent with step size $t \\leq 1/L$ and Lipschitz constant of\n$\\nabla f$ being $L$, we have\n$$f(x^{(k)}) - f(x^*) \\leq \\frac{1}{2tk} \\lVert x^{(0)} - x^*\\rVert_2^2.$$\n\n. . .\n\nConvergence rate: $O(1/k)$.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Lipschitz Constant\n\nIf $$\\lVert \\nabla f(x) - \\nabla f(y)\\rVert_2 \\leq L \\lVert x - y\\rVert_2$$ then\n$L$ is the Lipschitz constant of $\\nabla f$.\n\n:::\n\n::: {.column width=\"50%\"}\n\n<!-- ![](../images/lipschitz.gif){width=50%} -->\n\n:::\n\n::::\n\n### Example: OLS\n\n$$L = \\lVert X^TX \\rVert_2$$\n\n. . .\n\n### Line Searching\n\nIn practice, we may not know $L$ or it may be difficult to compute.\n\n. . .\n\n#### Exact Line Search\n\nIt may be tempting to try to solve $$t^* = \\arg_t\\min f(x - t\\nabla f(x))$$ but\nthis is typically too expensive.\n\n### Backtracking Line Search\n\nSet $\\beta \\in (0, 1)$ and $\\alpha \\in (0, 1/2]$. At each iteration, start with\n$t \\gets t_0$, and while\n$$f(x - t \\nabla f(x)) > f(x) - \\alpha t \\lVert \\nabla f(x)\\rVert_2^2$$ set\n$t = t \\beta.$\n\n![](../images/backtrack-line-search.png){width=65%}\n\n### Backtracking Line Search Step Size\n\n![](../images/gd-stepsize-linesearch.png){width=50%}\n\n### Stopping Criteria\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Common (ad-hoc) choices\n\n- Small gradient: $$\\lVert \\nabla f(x) \\rVert_r \\leq \\epsilon$$\n- Small relative change in $x$:\n  $$\\frac{\\lVert x^{(k+1)} - x^{(k)} \\rVert_r}{\\lVert x^{(k)} + 10^{-q}\\rVert_2} \\leq \\epsilon$$\n- Small relative change in $f$:\n  $$\\frac{f(x^{(k+1)}) - f(x^{(k)})}{f(x^{k})} \\leq \\epsilon$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### Duality Gap\n\nIf $f$ is **strongly** convex and $x$ optimal, then the duality gap is zero at\nthe optimum.\n\nMost principled stopping criterion, but not always available.\n\n:::\n\n::::\n\n### Problems with Gradient Descent\n\nSensitive to conditioning.\n\n![](../images/gd-conditioning.png){width=70%}\n\n## Newton's Method\n\nRecall second-order expansion of $f$ around $x$:\n$$f(y) \\approx \\hat{f}(y) = f(x) + \\nabla f(x)^T (y - x) + \\frac{1}{2}(y- x)^T \\nabla^2 f(x) (y - x).$$\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### Newton Update\n\nInstead of replacing $\\nabla^2 f(x)$ with $\\frac{1}{t} I$ (gradient descent\nupdate), minimize $\\hat{f}(y)$ directly:\n$$x^+ = x - \\big(\\nabla^2 f(x)\\big)^{-1} \\nabla f(x).$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](../images/newton-approx.png){width=52%}\n\n:::\n\n::::\n\n### Damped Newton\n\nIn practice this might not converge, so we use **damped** Newton:\n$$x^+ = x - t\\nabla^2 f(x)^{-1} \\nabla f(x),$$ with $t > 0$ chosen by\nbacktracking line search.\n\n. . .\n\nIn practice, this is often just called Newton's method.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Backtracking Line Search\n\nAt each iteration $t \\gets 1$ and while\n$$f(x + t v) > f(x) + \\alpha t \\nabla f(x)^T v$$ with\n$v = -(\\nabla^2 f(x)^{-1} \\nabla f(x))$ set $$t \\gets \\beta t.$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### Phases\n\n- As long as $t < 1$, we are in the **damped** phase.\n- When $t = 1$, we enter the **pure** phase and $t$ will remain $1$.\n- In pure phase, we have quadratic convergence.\n\n:::\n\n::::\n\n### Gradient Descent vs Newton\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Pros\n\n- Faster convergence: quadratic (eventually)\n- Independent of conditioning\n\n#### Cons\n\n- Higher memory cost\n- Higher computational cost $O(p^3)$ vs $O(p)$ for GD\n- Needs twice differentiable $f$.\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](../images/newton-gd-comp.png){width=100%}\n\n:::\n\n::::\n\n### Convergence\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](../images/newton-gd-convergence-iteration.png){width=100%}\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](../images/newton-gd-convergence-time.png){width=100%}\n\n:::\n\n::::\n\nIn practice, results depend on the problem.\n\n## Quasi-Newton Methods\n\nReplace $\\nabla^2 f(x)$ with an approximation $B_k$.\n\n. . .\n\n#### BFGS\n\nComplexity: $O(p^2)$, still same memory cost.\n\n. . .\n\n### L-BFGS\n\nSame complexity but lower memory cost.\n\n## Example: Poisson Regression\n\n## Poisson Regression\n\nConsider observations $y_i \\in \\mathbb{N}_0$, $x_i \\in \\mathbb{R}^p$ for\n$i = 1, \\ldots, n$, and Poisson point probabilities\n$$f_i(y_i) = e^{-\\mu(x_i)} \\frac{\\mu(x_i)^{y_i}}{y_i!}.$$\n\n. . .\n\nIf $\\log(\\mu(x_i)) = x_i^T \\beta$ we rewrite\n\n$$f_i(y_i) = e^{\\beta^T x_i y_i - \\exp( x_i^T \\beta)} \\frac{1}{y_i!}.$$\n\nThis is a _Poisson regression model_.\n\n### Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(vegetables)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      sale         normalSale         store     \n Min.   :  1.0   Min.   :  0.20   1      :   4  \n 1st Qu.: 12.0   1st Qu.:  4.20   102    :   4  \n Median : 21.0   Median :  7.25   106    :   4  \n Mean   : 40.3   Mean   : 11.72   107    :   4  \n 3rd Qu.: 40.0   3rd Qu.: 12.25   11     :   4  \n Max.   :571.0   Max.   :102.00   110    :   4  \n                                  (Other):1042  \n```\n\n\n:::\n\n```{.r .cell-code}\ndim(vegetables)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1066    3\n```\n\n\n:::\n:::\n\n\n### Model\n\n$$\\log(E(\\text{sale})) = \\beta_0 + \\beta_1 \\log(\\text{normalSale}) + \\beta_{\\text{store}}.$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Note, variable store is a factor with 352 levels!\npois_model <- glm(\n  sale ~ log(normalSale) + store,\n  data = vegetables,\n  family = poisson()\n)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pois_model) |>\n  coefficients() |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Estimate Std. Error z value Pr(>|z|)\n(Intercept)        2.718      0.128    21.3 9.4e-101\nlog(normalSale)    0.202      0.031     6.5  9.5e-11\nstore10            1.577      0.114    13.9  1.1e-43\nstore100           0.768      0.125     6.2  7.0e-10\nstore101           0.583      0.131     4.5  8.2e-06\nstore102          -0.029      0.149    -0.2  8.4e-01\n```\n\n\n:::\n:::\n\n\n### Exponential Families\n\nJoint density:\n$$f(\\mathbf{y} \\mid \\theta) = \\prod_{i=1}^n \\frac{1}{\\varphi_i(\\theta)} e^{\\theta^T t_i(y_i)} = e^{\\theta^T \\sum_{i=1}^n t_i(y_i) - \\sum_{i=1}^n \\kappa_i(\\theta)}$$\nwhere $\\varphi_i(\\theta) = \\int e^{\\theta^T t_i(u)} \\mu_i(\\mathrm{d}u)$ and\n$\\kappa_i(\\theta) = \\log(\\varphi_i(\\theta)).$ The log-likelihood is\n$$\\ell(\\theta) = \\theta^T t(\\mathbf{y}) - \\kappa(\\theta)$$ where\n\n$$\nt(\\mathbf{y}) = \\sum_{i=1}^m t_i(y_i) \\quad \\text{and} \\quad\n\\kappa(\\theta) = \\sum_{i=1}^m \\log \\varphi_i(\\theta).\n$$\n\nThe gradient is $$\\nabla \\ell(\\theta) = t(\\mathbf{y}) - \\nabla \\kappa(\\theta).$$\n\n### The Poisson Regression model\n\nFor the Poisson regression model we find that\n\n$$\nt(\\mathbf{y}) = \\sum_{i=1}^n x_i y_i = \\mathbf{X}^T \\mathbf{y} \\quad \\text{and} \\quad\n\\kappa(\\beta) = \\sum_{i=1}^n e^{x_i^T \\beta}.\n$$\n\nMoreover,\n$$\\nabla \\kappa(\\beta) = \\sum_{i=1}^n x_i e^{x_i^T \\beta} = \\mathbf{X}^T \\mathbf{w}(\\beta)$$\nwhere $$ \\mathbf{w}(\\beta) = \\exp(\\mathbf{X}^T \\beta)$$\nwith $\\exp$ applied\ncoordinatewisely.\n\n### Model Matrix and `glm.fit()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(\n  sale ~ log(normalSale) + store,\n  data = vegetables\n)\ny <- vegetables$sale\n\ndim(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1066  353\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::bench_time(\n  pois_fit <- glm.fit(X, y, family = poisson())\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nprocess    real \n  120ms   120ms \n```\n\n\n:::\n:::\n\n\n### `glm.fit()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit$iter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\npois_fit$converged\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n. . .\n\n#### Deviance\n\nTwo times (log-likelihood of null (intercept-only) model minus log-likelihood of\nthe full model)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit0 <- glm.fit(X[, 1], y, family = poisson())\npois_fit0$deviance - pois_fit$deviance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 42593\n```\n\n\n:::\n:::\n\n\n### `glm.fit()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_fit <- glm.fit(X, y, family = poisson(), control = list(trace = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDeviance = 9410 Iterations - 1\nDeviance = 8596 Iterations - 2\nDeviance = 8585 Iterations - 3\nDeviance = 8585 Iterations - 4\nDeviance = 8585 Iterations - 5\n```\n\n\n:::\n:::\n\n\n--\n\n### Stopping criterion\n\n$$|\\mathrm{deviance}_{n-1} - \\mathrm{deviance}_n| < \\varepsilon (\\mathrm{deviance}_n + 0.1)$$\nwith the default tolerance parameter $\\varepsilon = 10^{-8}$ and with a maximal\nnumber of iterations set to 25.\n\n### Implementation of Objective Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_map <- drop(crossprod(X, y))\n\nobjective <- function(beta) {\n  drop(sum(exp(X %*% beta)) - beta %*% t_map) / nrow(X)\n}\n\ngradient <- function(beta) {\n  (drop(crossprod(X, exp(X %*% beta))) - t_map) / nrow(X)\n}\n```\n:::\n\n\n. . .\n\nRecomputing the deviance difference,\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2 *\n  nrow(X) *\n  (objective(c(pois_fit0$coefficients, rep(0, 352))) -\n    objective(pois_fit$coefficients))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 42593\n```\n\n\n:::\n:::\n\n\n. . .\n\nand the value of the negative log-likelihood:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobjective(pois_fit$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -128.6\n```\n\n\n:::\n:::\n\n\n### Using `optim()` with Conjugate Gradient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::bench_time(\n  pois_CG <- optim(\n    rep(0, length = ncol(X)),\n    objective,\n    gradient,\n    method = \"CG\",\n    control = list(maxit = 10000)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nprocess    real \n  2.55s   2.56s \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_CG$value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -128.6\n```\n\n\n:::\n\n```{.r .cell-code}\npois_CG$counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction gradient \n   11522     4917 \n```\n\n\n:::\n:::\n\n\n### Using `optim()` with BFGS\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::bench_time(\n  pois_BFGS <- optim(\n    rep(0, length = ncol(X)),\n    objective,\n    gradient,\n    method = \"BFGS\",\n    control = list(maxit = 10000)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nprocess    real \n 64.5ms  64.9ms \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_BFGS$value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -128.6\n```\n\n\n:::\n\n```{.r .cell-code}\npois_BFGS$counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction gradient \n     126      114 \n```\n\n\n:::\n:::\n\n\n### Using Sparse Matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Matrix)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- Matrix(X)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::bench_time(\n  pois_BFGS_sparse <- optim(\n    rep(0, length = ncol(X)),\n    objective,\n    gradient,\n    method = \"BFGS\",\n    control = list(maxit = 10000)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nprocess    real \n 57.9ms  58.6ms \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(pois_BFGS$par - pois_BFGS_sparse$par)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -8.957e-13  1.245e-12\n```\n\n\n:::\n:::\n\n\n### Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"R/optim_alg.R\"))\n```\n:::\n\n\n[Source code](R/optim_alg.R)\n\n### Poisson Regression Problem\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_model <- function(form, data, response) {\n  X <- model.matrix(form, data)\n  y <- data[[response]]\n  t_map <- drop(crossprod(X, y))\n  n <- nrow(X)\n  p <- ncol(X)\n\n  objective <- function(beta) {\n    drop(sum(exp(X %*% beta)) - beta %*% t_map) / n\n  }\n\n  gradient <- function(beta) {\n    (drop(crossprod(X, exp(X %*% beta))) - t_map) / n\n  }\n\n  hessian <- function(beta) {\n    crossprod(X, drop(exp(X %*% beta)) * X) / n\n  }\n\n  list(par = rep(0, p), H = objective, grad_H = gradient, Hessian_H = hessian)\n}\n```\n:::\n\n\n### Test\n\nGradient descent is very slow for the full Poisson model, so we consider a much\nsmaller problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nveg_pois <- poisson_model(~ log(normalSale), vegetables, response = \"sale\")\npois_gd <- gradient_descent(veg_pois$par, veg_pois$H, veg_pois$grad_H)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_glm <- glm(sale ~ log(normalSale), data = vegetables, family = poisson())\nrbind(pois_glm = coefficients(pois_glm), pois_gd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept) log(normalSale)\npois_glm      1.4614         0.92157\npois_gd       1.4604         0.92194\n```\n\n\n:::\n:::\n\n\n### Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nveg_pois$H(coefficients(pois_glm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -124.406827879897\n```\n\n\n:::\n\n```{.r .cell-code}\nveg_pois$H(pois_gd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -124.406825325047\n```\n\n\n:::\n:::\n\n\n### Tracer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CSwR)\ngd_tracer <- tracer(c(\"value\", \"h_prime\", \"gamma\"), N = 50)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_gd <- gradient_descent(\n  veg_pois$par,\n  veg_pois$H,\n  veg_pois$grad_H,\n  cb = gd_tracer$tracer\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn = 1: value = 1; h_prime = 14269; gamma = NA; \nn = 2: value = -119.9; h_prime = 1631; gamma = 0.01; \nn = 3: value = -121.9; h_prime = 561.6; gamma = 0.004096; \nn = 4: value = -122.3; h_prime = 133.7; gamma = 0.003277; \nn = 5: value = -122.5; h_prime = 76.26; gamma = 0.004096; \nn = 6: value = -122.5; h_prime = 49.26; gamma = 0.004096; \nn = 7: value = -122.6; h_prime = 65.64; gamma = 0.00512; \nn = 8: value = -122.7; h_prime = 42.5; gamma = 0.004096; \nn = 9: value = -122.7; h_prime = 55.83; gamma = 0.00512; \nn = 10: value = -122.8; h_prime = 36.32; gamma = 0.004096; \nn = 11: value = -122.8; h_prime = 46.97; gamma = 0.00512; \nn = 12: value = -122.8; h_prime = 60.31; gamma = 0.00512; \nn = 13: value = -122.9; h_prime = 36.19; gamma = 0.004096; \nn = 14: value = -122.9; h_prime = 45.69; gamma = 0.00512; \nn = 15: value = -123; h_prime = 60.15; gamma = 0.00512; \nn = 16: value = -123; h_prime = 37.57; gamma = 0.004096; \nn = 17: value = -123.1; h_prime = 48.67; gamma = 0.00512; \nn = 18: value = -123.1; h_prime = 30.67; gamma = 0.004096; \nn = 19: value = -123.1; h_prime = 39.05; gamma = 0.00512; \nn = 20: value = -123.2; h_prime = 49.38; gamma = 0.00512; \nn = 21: value = -123.2; h_prime = 28.96; gamma = 0.004096; \nn = 22: value = -123.3; h_prime = 35.96; gamma = 0.00512; \nn = 23: value = -123.3; h_prime = 46.29; gamma = 0.00512; \nn = 24: value = -123.3; h_prime = 28.34; gamma = 0.004096; \nn = 25: value = -123.4; h_prime = 35.82; gamma = 0.00512; \nn = 26: value = -123.4; h_prime = 44.92; gamma = 0.00512; \nn = 27: value = -123.4; h_prime = 25.69; gamma = 0.004096; \nn = 28: value = -123.5; h_prime = 31.65; gamma = 0.00512; \nn = 29: value = -123.5; h_prime = 40.24; gamma = 0.00512; \nn = 30: value = -123.5; h_prime = 24.15; gamma = 0.004096; \nn = 31: value = -123.5; h_prime = 30.14; gamma = 0.00512; \nn = 32: value = -123.6; h_prime = 37.36; gamma = 0.00512; \nn = 33: value = -123.6; h_prime = 21.12; gamma = 0.004096; \nn = 34: value = -123.6; h_prime = 25.7; gamma = 0.00512; \nn = 35: value = -123.6; h_prime = 32.16; gamma = 0.00512; \nn = 36: value = -123.7; h_prime = 19.09; gamma = 0.004096; \nn = 37: value = -123.7; h_prime = 23.42; gamma = 0.00512; \nn = 38: value = -123.7; h_prime = 28.63; gamma = 0.00512; \nn = 39: value = -123.7; h_prime = 35.93; gamma = 0.00512; \nn = 40: value = -123.8; h_prime = 20.65; gamma = 0.004096; \nn = 41: value = -123.8; h_prime = 25.46; gamma = 0.00512; \nn = 42: value = -123.8; h_prime = 14.98; gamma = 0.004096; \nn = 43: value = -123.8; h_prime = 18.08; gamma = 0.00512; \nn = 44: value = -123.8; h_prime = 21.8; gamma = 0.00512; \nn = 45: value = -123.8; h_prime = 26.91; gamma = 0.00512; \nn = 46: value = -123.9; h_prime = 15.38; gamma = 0.004096; \nn = 47: value = -123.9; h_prime = 18.62; gamma = 0.00512; \nn = 48: value = -123.9; h_prime = 22.46; gamma = 0.00512; \nn = 49: value = -123.9; h_prime = 27.68; gamma = 0.00512; \nn = 50: value = -123.9; h_prime = 15.46; gamma = 0.004096; \nn = 51: value = -124; h_prime = 18.71; gamma = 0.00512; \nn = 52: value = -124; h_prime = 10.77; gamma = 0.004096; \nn = 53: value = -124; h_prime = 12.75; gamma = 0.00512; \nn = 54: value = -124; h_prime = 15.12; gamma = 0.00512; \nn = 55: value = -124; h_prime = 18.26; gamma = 0.00512; \nn = 56: value = -124; h_prime = 10.29; gamma = 0.004096; \nn = 57: value = -124; h_prime = 12.18; gamma = 0.00512; \nn = 58: value = -124; h_prime = 14.41; gamma = 0.00512; \nn = 59: value = -124.1; h_prime = 17.35; gamma = 0.00512; \nn = 60: value = -124.1; h_prime = 9.619; gamma = 0.004096; \nn = 61: value = -124.1; h_prime = 11.35; gamma = 0.00512; \nn = 62: value = -124.1; h_prime = 13.39; gamma = 0.00512; \nn = 63: value = -124.1; h_prime = 16.06; gamma = 0.00512; \nn = 64: value = -124.1; h_prime = 8.781; gamma = 0.004096; \nn = 65: value = -124.1; h_prime = 10.33; gamma = 0.00512; \nn = 66: value = -124.1; h_prime = 12.13; gamma = 0.00512; \nn = 67: value = -124.1; h_prime = 14.47; gamma = 0.00512; \nn = 68: value = -124.2; h_prime = 7.833; gamma = 0.004096; \nn = 69: value = -124.2; h_prime = 9.169; gamma = 0.00512; \nn = 70: value = -124.2; h_prime = 10.73; gamma = 0.00512; \nn = 71: value = -124.2; h_prime = 12.72; gamma = 0.00512; \nn = 72: value = -124.2; h_prime = 6.833; gamma = 0.004096; \nn = 73: value = -124.2; h_prime = 7.954; gamma = 0.00512; \nn = 74: value = -124.2; h_prime = 9.258; gamma = 0.00512; \nn = 75: value = -124.2; h_prime = 10.92; gamma = 0.00512; \nn = 76: value = -124.2; h_prime = 5.835; gamma = 0.004096; \nn = 77: value = -124.2; h_prime = 6.75; gamma = 0.00512; \nn = 78: value = -124.2; h_prime = 7.813; gamma = 0.00512; \nn = 79: value = -124.2; h_prime = 9.154; gamma = 0.00512; \nn = 80: value = -124.2; h_prime = 4.884; gamma = 0.004096; \nn = 81: value = -124.2; h_prime = 5.613; gamma = 0.00512; \nn = 82: value = -124.2; h_prime = 6.458; gamma = 0.00512; \nn = 83: value = -124.3; h_prime = 7.515; gamma = 0.00512; \nn = 84: value = -124.3; h_prime = 4.015; gamma = 0.004096; \nn = 85: value = -124.3; h_prime = 4.58; gamma = 0.00512; \nn = 86: value = -124.3; h_prime = 5.236; gamma = 0.00512; \nn = 87: value = -124.3; h_prime = 6.049; gamma = 0.00512; \nn = 88: value = -124.3; h_prime = 6.981; gamma = 0.00512; \nn = 89: value = -124.3; h_prime = 3.563; gamma = 0.004096; \nn = 90: value = -124.3; h_prime = 4.039; gamma = 0.00512; \nn = 91: value = -124.3; h_prime = 4.625; gamma = 0.00512; \nn = 92: value = -124.3; h_prime = 5.298; gamma = 0.00512; \nn = 93: value = -124.3; h_prime = 6.126; gamma = 0.00512; \nn = 94: value = -124.3; h_prime = 3.175; gamma = 0.004096; \nn = 95: value = -124.3; h_prime = 3.607; gamma = 0.00512; \nn = 96: value = -124.3; h_prime = 4.103; gamma = 0.00512; \nn = 97: value = -124.3; h_prime = 4.709; gamma = 0.00512; \nn = 98: value = -124.3; h_prime = 2.466; gamma = 0.004096; \nn = 99: value = -124.3; h_prime = 2.776; gamma = 0.00512; \nn = 100: value = -124.3; h_prime = 3.134; gamma = 0.00512; \nn = 101: value = -124.3; h_prime = 3.569; gamma = 0.00512; \nn = 102: value = -124.3; h_prime = 4.065; gamma = 0.00512; \nn = 103: value = -124.3; h_prime = 2.066; gamma = 0.004096; \nn = 104: value = -124.3; h_prime = 2.31; gamma = 0.00512; \nn = 105: value = -124.3; h_prime = 2.605; gamma = 0.00512; \nn = 106: value = -124.3; h_prime = 2.944; gamma = 0.00512; \nn = 107: value = -124.3; h_prime = 3.352; gamma = 0.00512; \nn = 108: value = -124.3; h_prime = 1.733; gamma = 0.004096; \nn = 109: value = -124.3; h_prime = 1.937; gamma = 0.00512; \nn = 110: value = -124.3; h_prime = 2.171; gamma = 0.00512; \nn = 111: value = -124.4; h_prime = 2.451; gamma = 0.00512; \nn = 112: value = -124.4; h_prime = 2.77; gamma = 0.00512; \nn = 113: value = -124.4; h_prime = 3.153; gamma = 0.00512; \nn = 114: value = -124.4; h_prime = 1.591; gamma = 0.004096; \nn = 115: value = -124.4; h_prime = 1.78; gamma = 0.00512; \nn = 116: value = -124.4; h_prime = 1.997; gamma = 0.00512; \nn = 117: value = -124.4; h_prime = 2.255; gamma = 0.00512; \nn = 118: value = -124.4; h_prime = 2.547; gamma = 0.00512; \nn = 119: value = -124.4; h_prime = 1.262; gamma = 0.004096; \nn = 120: value = -124.4; h_prime = 1.403; gamma = 0.00512; \nn = 121: value = -124.4; h_prime = 1.569; gamma = 0.00512; \nn = 122: value = -124.4; h_prime = 1.759; gamma = 0.00512; \nn = 123: value = -124.4; h_prime = 1.983; gamma = 0.00512; \nn = 124: value = -124.4; h_prime = 1.005; gamma = 0.004096; \nn = 125: value = -124.4; h_prime = 1.113; gamma = 0.00512; \nn = 126: value = -124.4; h_prime = 1.237; gamma = 0.00512; \nn = 127: value = -124.4; h_prime = 1.383; gamma = 0.00512; \nn = 128: value = -124.4; h_prime = 1.549; gamma = 0.00512; \nn = 129: value = -124.4; h_prime = 1.745; gamma = 0.00512; \nn = 130: value = -124.4; h_prime = 0.8704; gamma = 0.004096; \nn = 131: value = -124.4; h_prime = 0.9642; gamma = 0.00512; \nn = 132: value = -124.4; h_prime = 1.071; gamma = 0.00512; \nn = 133: value = -124.4; h_prime = 1.197; gamma = 0.00512; \nn = 134: value = -124.4; h_prime = 1.339; gamma = 0.00512; \nn = 135: value = -124.4; h_prime = 0.6643; gamma = 0.004096; \nn = 136: value = -124.4; h_prime = 0.7306; gamma = 0.00512; \nn = 137: value = -124.4; h_prime = 0.8085; gamma = 0.00512; \nn = 138: value = -124.4; h_prime = 0.897; gamma = 0.00512; \nn = 139: value = -124.4; h_prime = 1; gamma = 0.00512; \nn = 140: value = -124.4; h_prime = 1.117; gamma = 0.00512; \nn = 141: value = -124.4; h_prime = 0.5501; gamma = 0.004096; \nn = 142: value = -124.4; h_prime = 0.6043; gamma = 0.00512; \nn = 143: value = -124.4; h_prime = 0.6677; gamma = 0.00512; \nn = 144: value = -124.4; h_prime = 0.7397; gamma = 0.00512; \nn = 145: value = -124.4; h_prime = 0.8234; gamma = 0.00512; \nn = 146: value = -124.4; h_prime = 0.9179; gamma = 0.00512; \nn = 147: value = -124.4; h_prime = 0.4495; gamma = 0.004096; \nn = 148: value = -124.4; h_prime = 0.4931; gamma = 0.00512; \nn = 149: value = -124.4; h_prime = 0.5439; gamma = 0.00512; \nn = 150: value = -124.4; h_prime = 0.6014; gamma = 0.00512; \nn = 151: value = -124.4; h_prime = 0.6682; gamma = 0.00512; \nn = 152: value = -124.4; h_prime = 0.7435; gamma = 0.00512; \nn = 153: value = -124.4; h_prime = 0.3628; gamma = 0.004096; \nn = 154: value = -124.4; h_prime = 0.3974; gamma = 0.00512; \nn = 155: value = -124.4; h_prime = 0.4375; gamma = 0.00512; \nn = 156: value = -124.4; h_prime = 0.4829; gamma = 0.00512; \nn = 157: value = -124.4; h_prime = 0.5354; gamma = 0.00512; \nn = 158: value = -124.4; h_prime = 0.5946; gamma = 0.00512; \nn = 159: value = -124.4; h_prime = 0.2897; gamma = 0.004096; \nn = 160: value = -124.4; h_prime = 0.3168; gamma = 0.00512; \nn = 161: value = -124.4; h_prime = 0.348; gamma = 0.00512; \nn = 162: value = -124.4; h_prime = 0.3834; gamma = 0.00512; \nn = 163: value = -124.4; h_prime = 0.4242; gamma = 0.00512; \nn = 164: value = -124.4; h_prime = 0.4701; gamma = 0.00512; \nn = 165: value = -124.4; h_prime = 0.2291; gamma = 0.004096; \nn = 166: value = -124.4; h_prime = 0.25; gamma = 0.00512; \nn = 167: value = -124.4; h_prime = 0.2741; gamma = 0.00512; \nn = 168: value = -124.4; h_prime = 0.3013; gamma = 0.00512; \nn = 169: value = -124.4; h_prime = 0.3327; gamma = 0.00512; \nn = 170: value = -124.4; h_prime = 0.368; gamma = 0.00512; \nn = 171: value = -124.4; h_prime = 0.1797; gamma = 0.004096; \nn = 172: value = -124.4; h_prime = 0.1956; gamma = 0.00512; \nn = 173: value = -124.4; h_prime = 0.214; gamma = 0.00512; \nn = 174: value = -124.4; h_prime = 0.2347; gamma = 0.00512; \nn = 175: value = -124.4; h_prime = 0.2586; gamma = 0.00512; \nn = 176: value = -124.4; h_prime = 0.2854; gamma = 0.00512; \nn = 177: value = -124.4; h_prime = 0.1398; gamma = 0.004096; \nn = 178: value = -124.4; h_prime = 0.1519; gamma = 0.00512; \nn = 179: value = -124.4; h_prime = 0.1657; gamma = 0.00512; \nn = 180: value = -124.4; h_prime = 0.1814; gamma = 0.00512; \nn = 181: value = -124.4; h_prime = 0.1994; gamma = 0.00512; \nn = 182: value = -124.4; h_prime = 0.2197; gamma = 0.00512; \nn = 183: value = -124.4; h_prime = 0.2428; gamma = 0.00512; \nn = 184: value = -124.4; h_prime = 0.1178; gamma = 0.004096; \nn = 185: value = -124.4; h_prime = 0.1283; gamma = 0.00512; \nn = 186: value = -124.4; h_prime = 0.1402; gamma = 0.00512; \nn = 187: value = -124.4; h_prime = 0.1537; gamma = 0.00512; \nn = 188: value = -124.4; h_prime = 0.169; gamma = 0.00512; \nn = 189: value = -124.4; h_prime = 0.1864; gamma = 0.00512; \nn = 190: value = -124.4; h_prime = 0.0908; gamma = 0.004096; \nn = 191: value = -124.4; h_prime = 0.09864; gamma = 0.00512; \nn = 192: value = -124.4; h_prime = 0.1075; gamma = 0.00512; \nn = 193: value = -124.4; h_prime = 0.1177; gamma = 0.00512; \nn = 194: value = -124.4; h_prime = 0.1291; gamma = 0.00512; \nn = 195: value = -124.4; h_prime = 0.1422; gamma = 0.00512; \nn = 196: value = -124.4; h_prime = 0.06959; gamma = 0.004096; \nn = 197: value = -124.4; h_prime = 0.07541; gamma = 0.00512; \nn = 198: value = -124.4; h_prime = 0.08201; gamma = 0.00512; \nn = 199: value = -124.4; h_prime = 0.08955; gamma = 0.00512; \nn = 200: value = -124.4; h_prime = 0.09806; gamma = 0.00512; \nn = 201: value = -124.4; h_prime = 0.1077; gamma = 0.00512; \nn = 202: value = -124.4; h_prime = 0.1186; gamma = 0.00512; \nn = 203: value = -124.4; h_prime = 0.05712; gamma = 0.004096; \nn = 204: value = -124.4; h_prime = 0.06196; gamma = 0.00512; \nn = 205: value = -124.4; h_prime = 0.06749; gamma = 0.00512; \nn = 206: value = -124.4; h_prime = 0.07373; gamma = 0.00512; \nn = 207: value = -124.4; h_prime = 0.08083; gamma = 0.00512; \nn = 208: value = -124.4; h_prime = 0.08881; gamma = 0.00512; \nn = 209: value = -124.4; h_prime = 0.04312; gamma = 0.004096; \nn = 210: value = -124.4; h_prime = 0.04665; gamma = 0.00512; \nn = 211: value = -124.4; h_prime = 0.05068; gamma = 0.00512; \nn = 212: value = -124.4; h_prime = 0.05524; gamma = 0.00512; \nn = 213: value = -124.4; h_prime = 0.06041; gamma = 0.00512; \nn = 214: value = -124.4; h_prime = 0.06624; gamma = 0.00512; \nn = 215: value = -124.4; h_prime = 0.07284; gamma = 0.00512; \nn = 216: value = -124.4; h_prime = 0.03514; gamma = 0.004096; \nn = 217: value = -124.4; h_prime = 0.03808; gamma = 0.00512; \nn = 218: value = -124.4; h_prime = 0.0414; gamma = 0.00512; \nn = 219: value = -124.4; h_prime = 0.04518; gamma = 0.00512; \nn = 220: value = -124.4; h_prime = 0.04943; gamma = 0.00512; \nn = 221: value = -124.4; h_prime = 0.05425; gamma = 0.00512; \nn = 222: value = -124.4; h_prime = 0.02639; gamma = 0.004096; \nn = 223: value = -124.4; h_prime = 0.02852; gamma = 0.00512; \nn = 224: value = -124.4; h_prime = 0.03093; gamma = 0.00512; \nn = 225: value = -124.4; h_prime = 0.03367; gamma = 0.00512; \nn = 226: value = -124.4; h_prime = 0.03676; gamma = 0.00512; \nn = 227: value = -124.4; h_prime = 0.04026; gamma = 0.00512; \nn = 228: value = -124.4; h_prime = 0.04419; gamma = 0.00512; \nn = 229: value = -124.4; h_prime = 0.02126; gamma = 0.004096; \nn = 230: value = -124.4; h_prime = 0.023; gamma = 0.00512; \nn = 231: value = -124.4; h_prime = 0.02497; gamma = 0.00512; \nn = 232: value = -124.4; h_prime = 0.0272; gamma = 0.00512; \nn = 233: value = -124.4; h_prime = 0.02972; gamma = 0.00512; \nn = 234: value = -124.4; h_prime = 0.03255; gamma = 0.00512; \nn = 235: value = -124.4; h_prime = 0.03575; gamma = 0.00512; \nn = 236: value = -124.4; h_prime = 0.01711; gamma = 0.004096; \nn = 237: value = -124.4; h_prime = 0.01853; gamma = 0.00512; \nn = 238: value = -124.4; h_prime = 0.02014; gamma = 0.00512; \nn = 239: value = -124.4; h_prime = 0.02195; gamma = 0.00512; \nn = 240: value = -124.4; h_prime = 0.024; gamma = 0.00512; \nn = 241: value = -124.4; h_prime = 0.02631; gamma = 0.00512; \nn = 242: value = -124.4; h_prime = 0.01272; gamma = 0.004096; \nn = 243: value = -124.4; h_prime = 0.01374; gamma = 0.00512; \nn = 244: value = -124.4; h_prime = 0.01489; gamma = 0.00512; \nn = 245: value = -124.4; h_prime = 0.01619; gamma = 0.00512; \nn = 246: value = -124.4; h_prime = 0.01766; gamma = 0.00512; \nn = 247: value = -124.4; h_prime = 0.01932; gamma = 0.00512; \nn = 248: value = -124.4; h_prime = 0.02118; gamma = 0.00512; \nn = 249: value = -124.4; h_prime = 0.01015; gamma = 0.004096; \nn = 250: value = -124.4; h_prime = 0.01097; gamma = 0.00512; \nn = 251: value = -124.4; h_prime = 0.0119; gamma = 0.00512; \nn = 252: value = -124.4; h_prime = 0.01295; gamma = 0.00512; \nn = 253: value = -124.4; h_prime = 0.01414; gamma = 0.00512; \nn = 254: value = -124.4; h_prime = 0.01547; gamma = 0.00512; \nn = 255: value = -124.4; h_prime = 0.007506; gamma = 0.004096; \nn = 256: value = -124.4; h_prime = 0.008088; gamma = 0.00512; \nn = 257: value = -124.4; h_prime = 0.00875; gamma = 0.00512; \nn = 258: value = -124.4; h_prime = 0.009497; gamma = 0.00512; \nn = 259: value = -124.4; h_prime = 0.01034; gamma = 0.00512; \nn = 260: value = -124.4; h_prime = 0.01129; gamma = 0.00512; \nn = 261: value = -124.4; h_prime = 0.01236; gamma = 0.00512; \nn = 262: value = -124.4; h_prime = 0.00597; gamma = 0.004096; \nn = 263: value = -124.4; h_prime = 0.00644; gamma = 0.00512; \nn = 264: value = -124.4; h_prime = 0.006972; gamma = 0.00512; \nn = 265: value = -124.4; h_prime = 0.007573; gamma = 0.00512; \nn = 266: value = -124.4; h_prime = 0.008251; gamma = 0.00512; \nn = 267: value = -124.4; h_prime = 0.009015; gamma = 0.00512; \nn = 268: value = -124.4; h_prime = 0.009873; gamma = 0.00512; \nn = 269: value = -124.4; h_prime = 0.004735; gamma = 0.004096; \nn = 270: value = -124.4; h_prime = 0.005111; gamma = 0.00512; \nn = 271: value = -124.4; h_prime = 0.005538; gamma = 0.00512; \nn = 272: value = -124.4; h_prime = 0.006019; gamma = 0.00512; \nn = 273: value = -124.4; h_prime = 0.006561; gamma = 0.00512; \nn = 274: value = -124.4; h_prime = 0.007171; gamma = 0.00512; \nn = 275: value = -124.4; h_prime = 0.007858; gamma = 0.00512; \nn = 276: value = -124.4; h_prime = 0.003754; gamma = 0.004096; \nn = 277: value = -124.4; h_prime = 0.004056; gamma = 0.00512; \nn = 278: value = -124.4; h_prime = 0.004397; gamma = 0.00512; \nn = 279: value = -124.4; h_prime = 0.004782; gamma = 0.00512; \nn = 280: value = -124.4; h_prime = 0.005215; gamma = 0.00512; \nn = 281: value = -124.4; h_prime = 0.005704; gamma = 0.00512; \nn = 282: value = -124.4; h_prime = 0.002759; gamma = 0.004096; \nn = 283: value = -124.4; h_prime = 0.002972; gamma = 0.00512; \nn = 284: value = -124.4; h_prime = 0.003213; gamma = 0.00512; \nn = 285: value = -124.4; h_prime = 0.003486; gamma = 0.00512; \nn = 286: value = -124.4; h_prime = 0.003793; gamma = 0.00512; \nn = 287: value = -124.4; h_prime = 0.00414; gamma = 0.00512; \nn = 288: value = -124.4; h_prime = 0.004529; gamma = 0.00512; \nn = 289: value = -124.4; h_prime = 0.002178; gamma = 0.004096; \nn = 290: value = -124.4; h_prime = 0.002348; gamma = 0.00512; \nn = 291: value = -124.4; h_prime = 0.00254; gamma = 0.00512; \nn = 292: value = -124.4; h_prime = 0.002758; gamma = 0.00512; \nn = 293: value = -124.4; h_prime = 0.003003; gamma = 0.00512; \nn = 294: value = -124.4; h_prime = 0.003278; gamma = 0.00512; \nn = 295: value = -124.4; h_prime = 0.003587; gamma = 0.00512; \nn = 296: value = -124.4; h_prime = 0.001719; gamma = 0.004096; \nn = 297: value = -124.4; h_prime = 0.001855; gamma = 0.00512; \nn = 298: value = -124.4; h_prime = 0.002008; gamma = 0.00512; \nn = 299: value = -124.4; h_prime = 0.002181; gamma = 0.00512; \nn = 300: value = -124.4; h_prime = 0.002376; gamma = 0.00512; \nn = 301: value = -124.4; h_prime = 0.002595; gamma = 0.00512; \nn = 302: value = -124.4; h_prime = 0.002841; gamma = 0.00512; \nn = 303: value = -124.4; h_prime = 0.001355; gamma = 0.004096; \nn = 304: value = -124.4; h_prime = 0.001462; gamma = 0.00512; \nn = 305: value = -124.4; h_prime = 0.001584; gamma = 0.00512; \nn = 306: value = -124.4; h_prime = 0.001722; gamma = 0.00512; \nn = 307: value = -124.4; h_prime = 0.001877; gamma = 0.00512; \nn = 308: value = -124.4; h_prime = 0.002051; gamma = 0.00512; \nn = 309: value = -124.4; h_prime = 0.000991; gamma = 0.004096; \nn = 310: value = -124.4; h_prime = 0.001067; gamma = 0.00512; \nn = 311: value = -124.4; h_prime = 0.001152; gamma = 0.00512; \nn = 312: value = -124.4; h_prime = 0.001249; gamma = 0.00512; \nn = 313: value = -124.4; h_prime = 0.001358; gamma = 0.00512; \nn = 314: value = -124.4; h_prime = 0.001481; gamma = 0.00512; \nn = 315: value = -124.4; h_prime = 0.001619; gamma = 0.00512; \nn = 316: value = -124.4; h_prime = 0.0007797; gamma = 0.004096; \nn = 317: value = -124.4; h_prime = 0.0008399; gamma = 0.00512; \nn = 318: value = -124.4; h_prime = 0.000908; gamma = 0.00512; \nn = 319: value = -124.4; h_prime = 0.0009849; gamma = 0.00512; \nn = 320: value = -124.4; h_prime = 0.001071; gamma = 0.00512; \nn = 321: value = -124.4; h_prime = 0.001169; gamma = 0.00512; \nn = 322: value = -124.4; h_prime = 0.001278; gamma = 0.00512; \nn = 323: value = -124.4; h_prime = 0.0006128; gamma = 0.004096; \nn = 324: value = -124.4; h_prime = 0.0006606; gamma = 0.00512; \nn = 325: value = -124.4; h_prime = 0.0007146; gamma = 0.00512; \nn = 326: value = -124.4; h_prime = 0.0007755; gamma = 0.00512; \nn = 327: value = -124.4; h_prime = 0.0008442; gamma = 0.00512; \nn = 328: value = -124.4; h_prime = 0.0009213; gamma = 0.00512; \nn = 329: value = -124.4; h_prime = 0.001008; gamma = 0.00512; \nn = 330: value = -124.4; h_prime = 0.0004816; gamma = 0.004096; \nn = 331: value = -124.4; h_prime = 0.0005195; gamma = 0.00512; \nn = 332: value = -124.4; h_prime = 0.0005623; gamma = 0.00512; \nn = 333: value = -124.4; h_prime = 0.0006107; gamma = 0.00512; \nn = 334: value = -124.4; h_prime = 0.000665; gamma = 0.00512; \nn = 335: value = -124.4; h_prime = 0.0007261; gamma = 0.00512; \nn = 336: value = -124.4; h_prime = 0.0003518; gamma = 0.004096; \nn = 337: value = -124.4; h_prime = 0.0003783; gamma = 0.00512; \nn = 338: value = -124.4; h_prime = 0.0004084; gamma = 0.00512; \nn = 339: value = -124.4; h_prime = 0.0004423; gamma = 0.00512; \nn = 340: value = -124.4; h_prime = 0.0004806; gamma = 0.00512; \nn = 341: value = -124.4; h_prime = 0.0005236; gamma = 0.00512; \nn = 342: value = -124.4; h_prime = 0.000572; gamma = 0.00512; \nn = 343: value = -124.4; h_prime = 0.000276; gamma = 0.004096; \nn = 344: value = -124.4; h_prime = 0.000297; gamma = 0.00512; \nn = 345: value = -124.4; h_prime = 0.0003208; gamma = 0.00512; \nn = 346: value = -124.4; h_prime = 0.0003477; gamma = 0.00512; \nn = 347: value = -124.4; h_prime = 0.0003779; gamma = 0.00512; \nn = 348: value = -124.4; h_prime = 0.000412; gamma = 0.00512; \nn = 349: value = -124.4; h_prime = 0.0004502; gamma = 0.00512; \nn = 350: value = -124.4; h_prime = 0.0002165; gamma = 0.004096; \nn = 351: value = -124.4; h_prime = 0.0002332; gamma = 0.00512; \nn = 352: value = -124.4; h_prime = 0.000252; gamma = 0.00512; \nn = 353: value = -124.4; h_prime = 0.0002733; gamma = 0.00512; \nn = 354: value = -124.4; h_prime = 0.0002972; gamma = 0.00512; \nn = 355: value = -124.4; h_prime = 0.0003241; gamma = 0.00512; \nn = 356: value = -124.4; h_prime = 0.0003543; gamma = 0.00512; \nn = 357: value = -124.4; h_prime = 0.0001698; gamma = 0.004096; \nn = 358: value = -124.4; h_prime = 0.0001829; gamma = 0.00512; \nn = 359: value = -124.4; h_prime = 0.0001978; gamma = 0.00512; \nn = 360: value = -124.4; h_prime = 0.0002146; gamma = 0.00512; \nn = 361: value = -124.4; h_prime = 0.0002336; gamma = 0.00512; \nn = 362: value = -124.4; h_prime = 0.0002548; gamma = 0.00512; \nn = 363: value = -124.4; h_prime = 0.0002787; gamma = 0.00512; \nn = 364: value = -124.4; h_prime = 0.0001331; gamma = 0.004096; \nn = 365: value = -124.4; h_prime = 0.0001435; gamma = 0.00512; \nn = 366: value = -124.4; h_prime = 0.0001553; gamma = 0.00512; \nn = 367: value = -124.4; h_prime = 0.0001686; gamma = 0.00512; \nn = 368: value = -124.4; h_prime = 0.0001836; gamma = 0.00512; \nn = 369: value = -124.4; h_prime = 0.0002004; gamma = 0.00512; \nn = 370: value = -124.4; h_prime = 0.0002192; gamma = 0.00512; \nn = 371: value = -124.4; h_prime = 0.0001043; gamma = 0.004096; \nn = 372: value = -124.4; h_prime = 0.0001126; gamma = 0.00512; \nn = 373: value = -124.4; h_prime = 0.0001219; gamma = 0.00512; \nn = 374: value = -124.4; h_prime = 0.0001324; gamma = 0.00512; \nn = 375: value = -124.4; h_prime = 0.0001442; gamma = 0.00512; \nn = 376: value = -124.4; h_prime = 0.0001575; gamma = 0.00512; \nn = 377: value = -124.4; h_prime = 7.601e-05; gamma = 0.004096; \n```\n\n\n:::\n:::\n\n\n### Trace Information\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrace_sum_gd <- summary(gd_tracer)\nhead(trace_sum_gd, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   value h_prime    gamma     .time\n1    1.0 14268.6       NA 0.000e+00\n2 -119.9  1631.4 0.010000 3.314e-05\n3 -121.9   561.6 0.004096 9.274e-05\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntail(trace_sum_gd, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     value   h_prime    gamma   .time\n375 -124.4 1.442e-04 0.005120 0.01956\n376 -124.4 1.575e-04 0.005120 0.01962\n377 -124.4 7.601e-05 0.004096 0.01967\n```\n\n\n:::\n\n```{.r .cell-code}\nval_min <- veg_pois$H(coefficients(pois_glm))\n```\n:::\n\n\n### Trace Information\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trace_sum_gd, aes(.time, value - val_min)) +\n  geom_line() +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](lecture8_files/figure-beamer/trace-info-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Trace Information\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trace_sum_gd, aes(.time, h_prime)) +\n  geom_line() +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](lecture8_files/figure-beamer/trace-sum3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n### Newton Method\n\n\n::: {.cell}\n\n```{.r .cell-code}\nveg_pois <- poisson_model(\n  ~ store + log(normalSale) - 1,\n  vegetables,\n  response = \"sale\"\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time(\n  pois_newton <- newton(\n    veg_pois$par,\n    veg_pois$H,\n    veg_pois$grad_H,\n    veg_pois$Hessian_H\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.583   0.005   0.094 \n```\n\n\n:::\n:::\n\n\n### Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois_glm <- glm(\n  sale ~ store + log(normalSale) - 1,\n  data = vegetables,\n  family = poisson()\n)\nrange(pois_newton - pois_glm$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -4.980e-10  1.299e-06\n```\n\n\n:::\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nveg_pois$H(pois_newton)\nveg_pois$H(pois_glm$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -128.5894504744697997\n[1] -128.58945047447090815\n```\n\n\n:::\n:::\n\n\n. . .\n\n`glm()` (and the workhorse `glm.fit()`) uses a Newton-type algorithm.\n\n### Tracing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewton_tracer <- tracer(\n  c(\"value\", \"grad_norm_sq\"),\n  N = 0,\n  expr = expression(grad_norm_sq <- sum(grad^2))\n)\npois_newton <- newton(\n  veg_pois$par,\n  veg_pois$H,\n  veg_pois$grad_H,\n  veg_pois$Hessian_H,\n  epsilon = 8e-28,\n  cb = newton_tracer$tracer\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn = 1: value = 1; grad_norm_sq = 12737; \nn = 2: value = -14.83; grad_norm_sq = 44750; \nn = 3: value = -64.82; grad_norm_sq = 84762; \nn = 4: value = -111.3; grad_norm_sq = 11438; \nn = 5: value = -124.2; grad_norm_sq = 972.3; \nn = 6: value = -127.7; grad_norm_sq = 80.48; \nn = 7: value = -128.5; grad_norm_sq = 5.133; \nn = 8: value = -128.6; grad_norm_sq = 0.09585; \nn = 9: value = -128.6; grad_norm_sq = 7.168e-05; \nn = 10: value = -128.6; grad_norm_sq = 4.645e-11; \nn = 11: value = -128.6; grad_norm_sq = 1.951e-23; \nn = 12: value = -128.6; grad_norm_sq = 1.148e-30; \n```\n\n\n:::\n\n```{.r .cell-code}\ngd_tracer <- tracer(c(\"value\"), N = 0)\npois_gd <- gradient_descent(\n  veg_pois$par,\n  veg_pois$H,\n  veg_pois$grad_H\n)\n```\n:::\n\n\n### Tracing\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrace_sum_newton <- summary(newton_tracer)\nval_min <- veg_pois$H(pois_newton)\ntrace_sum_newton\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     value grad_norm_sq   .time\n1     1.00    1.274e+04 0.00000\n2   -14.83    4.475e+04 0.01091\n3   -64.82    8.476e+04 0.01683\n4  -111.34    1.144e+04 0.02306\n5  -124.25    9.723e+02 0.03033\n6  -127.71    8.048e+01 0.03743\n7  -128.50    5.133e+00 0.04537\n8  -128.59    9.585e-02 0.05515\n9  -128.59    7.168e-05 0.06430\n10 -128.59    4.645e-11 0.07178\n11 -128.59    1.951e-23 0.07854\n12 -128.59    1.148e-30 0.08631\n```\n\n\n:::\n:::\n\n\n### Newton Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture8_files/figure-beamer/trace-sum-Newton-1.pdf)\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Exercise\n\n### Step 1\n\nImplement the gradient descent algorithm to solve ridge regression:\n$$\\operatorname{minmize}_{\\beta \\in \\mathbb{R}^p}\\left( \\frac{1}{2} \\lVert X\\beta - y\\rVert_2^2 + \\frac{\\lambda \\lVert \\beta \\rVert_2^2}{2} \\right)$$\n\n. . .\n\n### Step 2\n\nImplement a method for solving ridge regression directly.\n\nExplicitly factorize (i.e. don't use `solve()`)\n\n## Summary\n\n### Today\n\n- Optimization problems\n- Algorithms to solve optimization problems\n  - Gradient descent\n  - The Newton method\n\n. . .\n\n### Things We Did Not Cover\n\n- Accelerated gradient methods\n- Conjugate gradient\n- Constrained optimization\n- Nonsmooth objectives\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}