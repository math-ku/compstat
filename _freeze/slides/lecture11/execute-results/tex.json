{
  "hash": "6c377e470b2c0903b31be3acd9898324",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gaussian Mixtures and Mixed Models with the EM Algorithm\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Today\n\nContinue with the EM algorithm. Two examples.\n\n### Gaussian Mixtures\n\nHands-on example with Gaussian mixtures, computing the empirical Fisher\ninformation in multiple ways.\n\n### Mixed Models\n\nFitting mixed models using the EM algorithm\n\n# Gaussian Mixtures\n\n## Gaussian Mixtures\n\nThe marginal density is\n\n$$\nf(x) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(x - \\mu_1)^2}{2 \\sigma_1^2}} +\n(1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(x - \\mu_2)^2}{2 \\sigma_2^2}}.\n$$\n\nand we will regard $\\theta = (p, \\mu_1, \\mu_2)$ as the unknown parameters, while\n$\\sigma_1$ and $\\sigma_2$ are fixed.\n\n. . .\n\n$$Q(\\theta \\mid \\theta')  = \\sum_{i=1}^n \\hat{p}_{i} \\left(\\log(p) - \\frac{(x_i - \\mu_1)^2}{2 \\sigma_1^2} \\right) + (1 - \\hat{p}_{i})\\left( \\log(1-p) - \\frac{(x_i - \\mu_2)^2}{2 \\sigma_2^2} \\right)$$\n\nwhere $\\hat{p}_i = P_{\\theta'}(Z_i = 1 \\mid X_i = x_i)$,\n\n-- which attains its maximum in\n\n$$\n\\theta = \\left(\\frac{1}{n} \\sum_{i} \\hat{p}_i,\\quad \\frac{1}{\\sum_{i} \\hat{p}_i} \\sum_{i} \\hat{p}_i x_i,\\quad\n\\frac{1}{\\sum_{i} (1 - \\hat{p}_i)} \\sum_{i} (1 - \\hat{p}_i) x_i \\right).\n$$\n\n### Simulation\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma1 <- 1.5\nsigma2 <- 1.5 # Same variances\n\np <- 0.5\nmu1 <- -0.5\nmu2 <- 4\n\nn <- 5000\nset.seed(321)\nz <- sample(\n  c(TRUE, FALSE),\n  n,\n  replace = TRUE,\n  prob = c(p, 1 - p)\n)\n\nx <- numeric(n)\nn1 <- sum(z)\nx[z] <- rnorm(n1, mu1, sigma1)\nx[!z] <- rnorm(n - n1, mu2, sigma2)\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture11_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n### The E-Step\n\n$$\n\\hat{p}_i = P_{\\theta'} (Z_i = 1 \\mid X = x_i) =\n\\frac{ p'_1 e^{-\\frac{(x_i - \\mu_1')^2}{2 \\sigma_1^2}}}{\n\\left( p'_1 e^{-\\frac{(x_i - \\mu_1')^2}{2 \\sigma_1^2}} +\n\\frac{\\sigma_1 (1 - p'_1)}{\\sigma_2} e^{-\\frac{(x_i - \\mu_2')^2}{2 \\sigma_2^2}}\\right) }\n$$\n\n### EM Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"R/em_gauss_mix_exercise.R\"))\nEM <- EM_gauss_mix(x)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM(c(0.4, -0.2, 4.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.4958753 -0.5151810  3.9439551\n```\n\n\n:::\n\n```{.r .cell-code}\nEM(c(0.9, 1, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.4958752 -0.5151815  3.9439546\n```\n\n\n:::\n:::\n\n\n. . .\n\nWhat happens when evaluating the following, and why?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM(c(0.6, 3, 1))\n```\n:::\n\n\n## Gradients and numDeriv\n\nRecall that\n$$\\nabla_{\\theta} \\ell(\\hat{\\theta}) = \\nabla_{\\theta} Q(\\hat{\\theta}, \\hat{\\theta}).$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ <- function(par, par_prime, EStep) {\n  phat <- EStep(par_prime)\n  p <- par[1]\n  mu1 <- par[2]\n  mu2 <- par[3]\n  sum(\n    phat *\n      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +\n      (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))\n  )\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(numDeriv)\ngrad1 <- function(par) grad(function(par) -neg_loglik(par, x), par)\ngrad2 <- function(par) {\n  grad(Q, par, par_prime = par, EStep = environment(EM)$EStep)\n}\n```\n:::\n\n\n## Checking the Gradient Identity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad1(c(0.5, 0, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  512.79296 -417.12831   51.21327\n```\n\n\n:::\n\n```{.r .cell-code}\ngrad2(c(0.5, 0, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  512.79296 -417.12831   51.21327\n```\n\n\n:::\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar_hat <- EM(c(0.5, 0, 4))\ngrad1(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.005302220 -0.001308433 -0.001318446\n```\n\n\n:::\n\n```{.r .cell-code}\ngrad2(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.005302236 -0.001308449 -0.001318448\n```\n\n\n:::\n:::\n\n\n## Fisher Information\n\nLet $\\hat{i}_X = - D^2_{\\theta} \\ell(\\hat{\\theta})$ denote the observed Fisher\ninformation.\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nThen \\begin{align} \\hat{i}_X &= - D_{\\overline{\\theta}} \\left( \\nabla*{\\theta}\nQ(\\overline{\\theta} \\mid \\overline{\\theta})\\right) |*{\\overline{\\theta} =\n\\hat{\\theta}} \\\\ & = - D^2*{\\theta} Q(\\hat{\\theta} \\mid \\hat{\\theta}) -\nD*{\\theta'} \\nabla*{\\theta} Q(\\hat{\\theta} \\mid \\hat{\\theta})\\\\ & = - \\left(I -\nD*{\\theta} \\Phi(\\hat{\\theta})^T\\right) D^2*{\\theta} Q(\\hat{\\theta} \\mid\n\\hat{\\theta}) \\end{align} where $$\\Phi(\\theta') = \\textrm{arg max}*{\\theta} \\\nQ(\\theta \\mid \\theta')$$ is the EM-map.\n\n:::\n\n::: {.column width=\"47%\"}\n\n### Exercise\n\n- Download the [source code here](R/em_gauss_mix_exercise.R)\n- Implement these three methods for computing the Fisher information in the\n  Gaussian mixture problem.\n- Test that they work.\n- Benchmark them against one another.\n\n:::\n\n::::\n\n# Mixed Models\n\n## Mixed Model (General Form)\n\nGeneral form is $$y = X\\beta + Zu + \\varepsilon$$ where\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n- $y$ is the length $n$ response vector,\n- $X$ is the $n \\times p$ fixed effects design matrix,\n- $\\beta$ is the length $p$ fixed effects parameter vector,\n- $Z$ is the $n \\times m$ random effects design matrix, and\n- $u$ is the length $m$ random effects vector.\n\n:::\n\n::: {.column width=\"47%\"}\n\n### Example\n\n$$Z = \\begin{bmatrix} 1 & 0 & 0 & x_{1j} \\\\ 0 & 1 & 0 & x_{2j} \\\\ 0 & 0 & 1 & x_{3j} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 0 & 1 & 0 & x_{nj} \\end{bmatrix}$$\n\n:::\n\n::::\n\n![](../images/mixed-model-effects.jpg){width=85%}\n\n### Hierarchical Data\n\n![](../images/hierarchical-data.png){width=90%}\n\n### Simpson's Paradox\n\n![](../images/simpsons-paradox.png){width=90%}\n\n## Estimation\n\nTypical assumptions are\n\n- $u \\sim \\mathcal{N}(0, \\Sigma_u)$\n- $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$\n- $\\operatorname{cov}(u, \\varepsilon) = 0$\n\n. . .\n\n### Fits the EM Framework\n\n- $u, \\varepsilon$ are latent (unobserved)\n- $y$ is observed\n\n. . .\n\n### Distribution of $(u, \\varepsilon)$\n\n$$\\begin{bmatrix}u \\\\ \\varepsilon\\end{bmatrix} \\sim \\mathcal{N}\\left(0,\\begin{bmatrix}\\Sigma_u & 0 \\\\ 0 & I\\sigma^2\\end{bmatrix}\\right)$$\n\n## Marginal Likelihood\n\n$$y = X\\beta + Zu + \\varepsilon = \\begin{bmatrix}Z & I_n\\end{bmatrix}\\begin{bmatrix}u \\\\ \\varepsilon\\end{bmatrix} + X\\beta$$\nis a linear combination, so we have\n$$y \\sim \\mathcal{N}(\\mu_y = X\\beta,\\Sigma_y =  Z\\Sigma_u Z^T + \\sigma^2 I_n).$$\n\n. . .\n\nSo, we can write the log-likelihood as\n$$\\ell(\\theta) = -\\frac{1}{2} \\left( (y - X\\beta)^T \\Sigma_y^{-1} (y - X\\beta) + \\log\\lvert \\Sigma_y \\rvert \\right).$$\n\n. . .\n\nCan actually solve this problem directly!\n\n. . .\n\n**But** requires solving a large linear system and computing the log-determinant\nof the $n \\times n$ matrix $Z \\Sigma_u Z^T + \\sigma^2 I_n$.\n\n## Joint Distribution\n\nThe conditional distribution of $y$ given $u$ is\n$$(y \\mid u) \\sim \\mathcal{N}(X\\beta + Zu, \\sigma^2 I).$$\n\n. . .\n\nAnd since $f(y, u) = f(y \\mid u) f(u)$, we have\n$$f(y, u) \\propto \\frac{1}{\\sqrt{\\lvert\\Sigma_u\\rvert \\sigma^{2n}}}  \\exp\\left( -\\frac{1}{2\\sigma^2} \\lVert y - X\\beta - Zu \\rVert_2^2 - \\frac{1}{2} u^T \\Sigma_u^{-1} u\\right).$$\n\n. . .\n\nThe log-likelihood is then\n$$\\ell(\\theta) = -\\frac{1}{2\\sigma^2} \\lVert y - X\\beta - Zu\\rVert^2_2  - \\frac{1}{2}u^T \\Sigma_u^{-1} u  - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2}\\log\\lvert\\Sigma_u\\rvert.$$\n\n## Distribution of $(u \\mid y)$\n\nWe want the $Q$-function, but first need the distribution of $(u \\mid y$).\n\n. . .\n\nBy Bayes' theorem, we have\n\n$$\n\\begin{aligned}f(u \\mid y) = \\frac{f(y \\mid u) f(u)}{f(y)} &\\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\lVert y - X\\beta - Z u\\rVert_2^2 - \\frac{1}{2} u^T \\Sigma_u u\\right) \\\\\n&\\propto \\exp\\left(-\\frac{1}{2}(u - \\eta)^T G^{-1} (u - \\eta)\\right)\\end{aligned}\n$$\n\nwith\n$$G = \\left(\\frac{Z^T Z}{\\sigma^2} + \\Sigma_u^{-1}\\right)^{-1}\\qquad \\text{and} \\qquad \\eta = \\frac{G Z^T (y - X \\beta)}{\\sigma^2}.$$\n\n. . .\n\nSo $(u \\mid y) \\sim \\mathcal{N}(\\eta,  G)$.\n\n### The $Q$ Function\n\nNow we can compute the $Q$ function:\n\n$$\n\\begin{aligned}\\operatorname{E}_{u \\sim p(\\cdot \\mid y, \\theta')} \\ell(\\theta) &= \\frac{1}{2\\sigma^2}\\left(r^Tr - 2r^T Z \\eta + \\operatorname{tr}(Z^T Z G) + \\eta^T Z^T Z \\eta \\right) \\\\\n&\\phantom{={}}- \\frac{1}{2}\\left(\\operatorname{tr}(\\Sigma_u^{-1} G) + \\eta^T \\Sigma_u^{-1} \\eta\\right) \\\\\n&\\phantom{={}} - \\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2} \\log | \\Sigma_u |.\\end{aligned}\n$$\n\n. . .\n\nMinimizing this with respect to $\\theta$, we find\n\n$$\n\\begin{aligned}\\beta^* &= (X^T X)^{-1} X^T (y - Z \\eta) \\\\\n                  (\\sigma^2)^* &= \\frac{1}{n}\\left( \\lVert y - X\\beta\\rVert_2^2 - 2 (y - X\\beta)^T Z \\eta + \\operatorname{tr}(Z^T Z G)\\right)\\\\\n                \\Sigma_u^* &= \\frac{1}{m}\\left(G + \\eta \\eta^T\\right)\\end{aligned}\n$$\n\n### Simplification\n\nTo test out how this works, we introduce two simplifications:\n\n- $\\Sigma_u(\\nu) = \\nu I_m$\n- $X = \\mathbf{1}$ (intercept-only fixed effects)\n- $z_{ij} \\in \\{0, 1\\}$ (random intercept, fixed slope)\n\n. . .\n\n#### Easier Update\n\n$$\\nu^* = \\operatorname{tr}(G + \\eta \\eta^T) / m$$\n\n## Simple Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta0 <- 1\nbeta <- cbind(beta0)\nnu <- 2\nsigma <- 3\n\nm <- 20\nN <- 10\nni <- rep(N, m)\nn <- sum(ni)\n\nX <- cbind(rep(1, n))\nZ <- Matrix::bdiag(lapply(ni, function(d) matrix(1, nrow = d, ncol = 1)))\n\nu <- rnorm(m, 0, sqrt(nu))\nmu <- X %*% beta + Z %*% u\ny <- rnorm(n, as.vector(mu), sigma)\n```\n:::\n\n\n### Visualizing $Z$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture11_files/figure-beamer/unnamed-chunk-6-1.pdf)\n:::\n:::\n\n\n### Define the Log-Likelihood\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik <- function(y, X, Z, beta, sigma, nu) {\n  n <- length(y)\n  m <- ncol(Z)\n\n  r <- y - X %*% beta\n  U <- Z %*% tcrossprod(nu * diag(m), Z) + sigma^2 * diag(n)\n  inv_U_r <- solve(U, r)\n  log_det_U <- log(det(U))\n\n  -0.5 * (crossprod(r, inv_U_r) + log_det_U + n * log(2 * pi))\n}\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar0 <- c(0.2, 2.1, 0.9)\nres_em <- mixed_model_em(y, X, Z, par0[1], par0[2], par0[3])\nres_em$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     beta        nu    sigma2 \n0.6996877 1.7700719 7.9182372 \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n\nmixed_data <- data.frame(y = y, Subject = factor(rep(1:m, times = ni)))\nmixed_lmer <- lmer(y ~ (1 | Subject), data = mixed_data, REML = FALSE)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(mixed_lmer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.7054258 \n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(VarCorr(mixed_lmer))[, c(1, 4)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       grp     vcov\n1  Subject 1.769295\n2 Residual 7.918388\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n### Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture11_files/figure-beamer/plot-convergence-1.pdf)\n:::\n:::\n\n\n## Exercise: The Marginal Model\n\nRecall that\n$$y \\sim \\mathcal{N}(\\mu_y = X\\beta,\\Sigma_y =  Z\\Sigma_u Z^T + \\sigma^2 I_n).$$\n\n. . .\n\nSo, we can write the log-likelihood (of the simplified model) as\n$$\\ell(\\theta) = -\\frac{1}{2} \\left( (y - \\beta_0)^T (\\nu ZZ^T + \\sigma^2 I_n)^{-1} (y - \\beta_0) + \\log\\big\\lvert \\nu Z Z^T + \\sigma^2 I_n \\big\\rvert \\right).$$\n\nOptimize this log-likelihood directly using `optim()`. Use a numerical gradient.\n\n#### Hints\n\n- Make sure to load the `Matrix` package or use the namespace directly when\n  working with sparse matrices. --\n\n- How should you deal with the inverse?\n- How should you parameterize the problem?\n\n\n\n### Benchmark\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture11_files/figure-beamer/benchmark-1.pdf)\n:::\n:::\n\n\n. . .\n\nBut this is admittedly a flawed comparison. Why?\n\n- No standard convergence criterion\n- Numerical gradients (and Hessian) used in `optim()`\n\n## Summary\n\n### Gaussian Mixtures\n\n- We profiled implementations of our EM algorithm for Gaussian mixtures\n- We experimented with various implementations of the empirical Fisher\n  information.\n\n. . .\n\n### Mixed Models\n\n- We fit mixed models using the EM algoroithm.\n- We compared this approach to directly optimizing the marginal likelihood.\n\n. . .\n\n## Next Time\n\nStochastic gradient descent\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}