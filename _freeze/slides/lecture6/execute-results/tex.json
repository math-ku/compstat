{
  "hash": "415b702dd098a97495ce7b62ce43109b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Monte Carlo Methods\"\n---\n\n\n\n## Rejection Sampling Recap\n\nSuppose that $f(y) \\propto q(y)$ is the **target** density, known up to\nnormalization.\n\n. . .\n\nSuppose that $g(y) \\propto p(y)$ is another density, known up to normalization,\nthat we can simulate from, and suppose\n\n$$\\alpha' q \\leq p$$\n\nfor some $\\alpha' > 0$.\n\n. . .\n\nIf $Y$ has density $g$ and $U$ is uniform on $(0,1)$ then the conditional\ndistribution $$Y \\mid U \\leq \\alpha' q(Y) / p(Y)$$ has density $f$.\n\n### Basic Algorithm\n\nTo simulate one variable with density $f$, run the following code until it\nreturns a value:\n\n. . .\n\n- Simulate $y$ from $g$\n- Simulate $u$ from $\\operatorname{Uniform}(0,1)$\n- If $u > \\alpha' q(y) / p(y)$ return to step one\n- Else return $y$\n\n. . .\n\n### Two Challenges\n\n- Finding an envelope $p/\\alpha'$, of $q$\n- Finding a **tight-enough** envelope\n\n## Today\n\n### Monte Carlo Methods\n\nGeneral class of methods for estimating integrals and means\n\nRejection sampling is a type of Monte Carlo method.\n\n### Importance Sampling\n\nFocus on integrating functions and estimating means\n\nUseful when we cannot analytically compute the integral\n\nVariance reduction technique\n\n## Monte Carlo Methods\n\n- Basic idea: use random sampling to obtain numerical results\n- Useful when analytical solutions are infeasible\n\n. . .\n\n### Monte Carlo Integration\n\nWith $X_1, \\ldots, X_n$ i.i.d. with density $f$\n$$\\hat{\\mu}_{\\textrm{MC}} := \\frac{1}{n} \\sum_{i=1}^n h(X_i) \\rightarrow \\mu := \\mathbb{E} h(X_1) = \\int h(x) f(x) \\ dx$$\nfor $n \\to \\infty$ by the law of large numbers (LLN).\n\n## Monte Carlo Integration\n\nThe CLT gives that\n\n$$\n\\frac{1}{n} \\sum_{i=1}^n h(X_i) \\overset{\\textrm{approx}} \\sim\n\\mathcal{N}(\\mu, \\sigma^2_{\\textrm{MC}} / n)\n$$\n\nwhere\n$$\\sigma^2_{\\textrm{MC}} = \\mathbb{V} h(X_1) = \\int (h(x) - \\mu)^2 f(x) \\ dx.$$\n\n## Monte Carlo Integration\n\nWe can estimate $\\sigma^2_{\\textrm{MC}}$ using the empirical variance\n$$\\hat{\\sigma}^2_{\\textrm{MC}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i) - \\hat{\\mu}_{\\textrm{MC}})^2,$$\n\nthen the variance of $\\hat{\\mu}_{\\textrm{MC}}$ is estimated as\n$\\hat{\\sigma}^2_{\\textrm{MC}} / n$ and a standard 95% confidence interval for\n$\\mu$ is\n$$\\hat{\\mu}_{\\textrm{MC}} \\pm 1.96 \\frac{\\hat{\\sigma}_{\\textrm{MC}}}{\\sqrt{n}}.$$\n\n## Example: Gamma Distribution\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1000\nx <- rgamma(B, 8)\n\nmu_hat <- (cumsum(x) / (1:B))\nsigma_hat <- sd(x)\nmu_hat[B] # Theoretical value: 8\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.096\n```\n\n\n:::\n\n```{.r .cell-code}\nsigma_hat # Theoretical value: âˆš8\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.862\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-fig-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n### Convergence\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- ggplot(tibble(x = 1:B, y = mu_hat), aes(x, y)) +\n  geom_hline(yintercept = 8, linetype = \"dashed\", col = \"steelblue4\") +\n  geom_line() +\n  geom_point() +\n  coord_cartesian(ylim = c(6, 10))\np\n```\n\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-fig1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Confidence Bands\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_ribbon(\n    aes(\n      ymin = mu_hat - 1.96 * sigma_hat / sqrt(1:B),\n      ymax = mu_hat + 1.96 * sigma_hat / sqrt(1:B)\n    ),\n    fill = \"dark orange\",\n    alpha = 0.4\n  ) +\n  geom_line() +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-fig2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## When to Stop?\n\n- Can use concentration inequalities to get a stopping rule\n- But need variance estimate for Chebychev-based bound and moment generating\n  functions for Chernoff-based bounds. --\n\n- If you overshoot then you waste computer resources, and if you undershoot then\n  you get a bad estimate.\n\n. . .\n\n### Exercise\n\nImplement a heuristic stopping rule that stops when the confidence interval is\n0.1 units wide.\n\n\n\n# Importance Sampling\n\n## Importance Sampling\n\nWhen we are only interested in Monte Carlo integration, we do not need to sample\nfrom the target distribution.\n\n. . .\n\n### Basic Idea\n\n- Usually, some regions of the target distribution are more important\n- So let's oversample these regions\n- But we need to correct for this oversampling\n\n. . .\n\nWe want to find $\\mu = \\operatorname{E} h(X)$. Observe that\n$$\\mu = \\int h(x) f(x) \\ dx = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\ dx = \\int h(x) w^*(x) g(x) \\ dx$$\nwhenever $g$ is a density fulfilling that $$g(x) = 0 \\Rightarrow f(x) = 0.$$\n\n### Weights\n\nWith $X_1, \\ldots, X_n$ i.i.d. with density $g$ define the _weights_\n$$w^*(X_i) = \\frac{f(X_i)}{g(X_i)}.$$\n\n. . .\n\n- $f/g$ is called the _likelihood ratio_.\n- $f$ is the _nominal_ density\n- $g$ is the _importance_ density\n\n. . .\n\n### Estimator\n\n$$\\hat{\\mu}_{\\textrm{IS}}^* := \\frac{1}{n} \\sum_{i=1}^n h(X_i)w^*(X_i).$$\n\n. . .\n\nIt has mean $\\mu$.\n\n## Gamma Importance Sampling\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1000\nx <- rnorm(B, 10, 3)\nw_star <-\n  dgamma(x, 8) / dnorm(x, 10, 3) #<<\nmu_hat_IS <-\n  cumsum(x * w_star) / (1:B)\nmu_hat_IS[B] # Theoretical value 8\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.995\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gammasim-fig-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n### Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-IS-fig1-1.pdf)\n:::\n:::\n\n\n## Importance Sampling Variance\n\nAgain by the LLN\n$$\\hat{\\mu}_{\\textrm{IS}}^* \\rightarrow \\mathbb{E} h(X_1) w^*(X_1) = \\mu,$$ and\nby the CLT\n\n$$\n\\hat{\\mu}_{\\textrm{IS}}^* \\overset{\\textrm{approx}} \\sim\n\\mathcal{N}(\\mu, \\sigma^{*2}_{\\textrm{IS}} / n)\n$$\n\nwhere\n$$\\sigma^{*2}_{\\textrm{IS}} = \\mathbb{V} h(X_1)w^*(X_1) = \\int (h(x) w^*(x) - \\mu)^2 g(x) \\ dx.$$\n\n## Importance Sampling Variance\n\nThe importance sampling variance can be estimated just as the MC variance\n$$\\hat{\\sigma}^{*2}_{\\textrm{IS}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i)w^*(X_i) - \\hat{\\mu}_{\\textrm{IS}}^*)^2,$$\n\n. . .\n\nAnd a 95% standard confidence interval is\n$$\\hat{\\mu}^*_{\\textrm{IS}} \\pm 1.96 \\frac{\\hat{\\sigma}^*_{\\textrm{IS}}}{\\sqrt{n}}.$$\n\n. . .\n\nWe may have $\\sigma^{*2}_{\\textrm{IS}} > \\sigma^2_{\\textrm{MC}}$ or\n$\\sigma^{*2}_{\\textrm{IS}} < \\sigma^2_{\\textrm{MC}}$ depending on $h$ and $g$.\n\n. . .\n\n### Goal\n\nChoose $g$ so that $h(x) w^*(x)$ becomes as constant as possible.\n\n## Gamma Importance Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_hat_IS <- sd(x * w_star)\nsigma_hat_IS # Theoretical value?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.5\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-IS-fig2-1.pdf)\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Standardized Weights\n\nIf $f = c^{-1} q$ with $c$ unknown then\n$$c = \\int q(x) dx = \\int \\frac{q(x)}{g(x)} g(x) d x.$$\n\n. . .\n\nAnd $$\\mu = \\frac{\\int h(x) w^*(x) g(x) \\ d x}{\\int w^*(x) g(x) \\ d x},$$ where\n$w^*(x) = q(x) / g(x).$\n\n## IS with Standardized Weights\n\nAn importance sampling estimate of $\\mu$ is thus\n\n$$\n\\begin{aligned} \\hat{\\mu}_{\\textrm{IS}} & = \\frac{\\sum_{i=1}^n h(X*i) w^*(X_i)}{\\sum*{i=1}^n w^*(X*i)} \\\\\n                                        & = \\sum*{i=1}^n h(X_i) w(X_i),\n\\end{aligned}\n$$\n\nwhere $w^*(X_i) = q(X_i) / g(X_i)$ and\n$$w(X_i) = \\frac{w^*(X_i)}{\\sum_{i=1}^n w^*(X_i)}$$ are the _standardized\nweights_. This works irrespectively of the value of the normalizing constant\n$c$, and also if an unnormalized $g$ is used.\n\n## Gamma Standardized Weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1000\nx <- rnorm(B, 10, 3)\nw_star <- numeric(B)\nx_pos <- x[x > 0]\nw_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))\n\nmu_hat_IS <- cumsum(x * w_star) / cumsum(w_star)\nmu_hat_IS[B] # Theoretical value 8\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.102\n```\n\n\n:::\n:::\n\n\n### Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-ISw-fig1-1.pdf)\n:::\n:::\n\n\n## Importance Sampling Variance\n\nThe variance of the IS estimator with standardized weights is a little more\ncomplicated, because the estimator is a ratio of random variables.\n\n. . .\n\nFrom the multivariate CLT, we have\n\n$$\n\\frac{1}{n} \\sum_{i=1}^n \\left(\\begin{array}{c} h(X_i) w^*(X_i) \\\\ w^*(X_i) \\end{array}\\right) \\overset{\\textrm{approx}}{\\sim}\n\\mathcal{N}\\left( c \\left(\\begin{array}{c} \\mu  \\\\   {1} \\end{array}\\right),\n\\frac{1}{n} \\left(\\begin{array}{cc} \\sigma^{*2}_{\\textrm{IS}} & \\gamma \\\\ \\gamma & \\sigma^2_{w^*}\n\\end{array} \\right)\\right),\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\sigma^2_{\\textrm{IS}} & = \\mathbb{V}(h(X_1) w^*(X_1)) \\\\\n\\gamma & = \\mathrm{cov}(h(X_1) w^*(X_1), w^*(X_1)) \\\\\n\\sigma^2_{w^*} & = \\mathbb{V}(w^*(X_1))\n\\end{aligned}\n$$\n\n## Importance Sampling Variance\n\nWe can then apply the $\\Delta$-method with $t(x, y) = x / y$.\n\n. . .\n\nObserve that $Dt(x, y) = (1 / y, - x / y^2)$, whence\n\n$$\nDt(c\\mu, c)   \\left(\\begin{array}{cc} \\hat{\\sigma}^{*2}_{\\textrm{IS}} & \\gamma \\\\ \\gamma & \\sigma^2_{w^*}\n\\end{array} \\right) Dt(c\\mu, c)^T = c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma).\n$$\n\n. . .\n\nBy the $\\Delta$-method\n\n$$\n\\hat{\\mu}_{\\textrm{IS}} \\overset{\\textrm{approx}}{\\sim}\n\\mathcal{N}(\\mu, c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma) / n).\n$$\n\n. . .\n\nFor $c = 1$, the asymptotic variance can be estimated by plugging in empirical\nestimates. For $c \\neq 1$ it is necessary to estimate $c$ as\n$\\hat{c} = \\frac{1}{n} \\sum_{i=1}^n w^*(X_i)$.\n\n## Gamma Standardized Weights, Variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc_hat <- mean(w_star)\nsigma_hat_IS <- sd(x * w_star)\nsigma_hat_w_star <- sd(w_star)\ngamma_hat <- cov(x * w_star, w_star)\nsigma_hat_IS_w <- sqrt(\n  sigma_hat_IS^2 +\n    mu_hat_IS[B]^2 * sigma_hat_w_star^2 -\n    2 * mu_hat_IS[B] * gamma_hat\n) /\n  c_hat\nsigma_hat_IS_w\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.198\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Gamma Importance Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\np +\n  geom_ribbon(\n    aes(\n      ymin = mu_hat_IS - 1.96 * sigma_hat_IS_w / sqrt(1:B),\n      ymax = mu_hat_IS + 1.96 * sigma_hat_IS_w / sqrt(1:B)\n    ),\n    fill = \"gray\"\n  ) +\n  geom_line() +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/gamma-ISw-fig2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n# Examples\n\n## Network Failure\n\n## ![](../images/networkfig.png)\n\nThe different edges \"fail\" independently with probability $p$.\n\n. . .\n\nWhat is the probability that nodes 1 and 10 are disconnected?\n\n## Representing Graphs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA # Graph adjacency matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    1    0    1    1    0    0    0    0     0\n [2,]    1    0    1    0    0    1    0    0    0     0\n [3,]    0    1    0    0    0    1    1    1    0     1\n [4,]    1    0    0    0    1    0    0    1    0     0\n [5,]    1    0    0    1    0    0    0    1    1     0\n [6,]    0    1    1    0    0    0    1    0    0     1\n [7,]    0    0    1    0    0    1    0    0    0     1\n [8,]    0    0    1    1    1    0    0    0    1     0\n [9,]    0    0    0    0    1    0    0    1    0     1\n[10,]    0    0    1    0    0    1    1    0    1     0\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAup <- A\nAup[lower.tri(Aup)] <- 0\n```\n:::\n\n\n## Check Connectivity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscon <- function(Aup) {\n  A <- Matrix::forceSymmetric(Aup, \"U\")\n  i <- 3\n  Apow <- A %*% A %*% A # A^3\n\n  while (Apow[1, 10] == 0 && i < 9) {\n    Apow <- Apow %*% A\n    i <- i + 1\n  }\n\n  Apow[1, 10] == 0 # TRUE if nodes 1 and 10 are not connected\n}\n```\n:::\n\n\n## Sampling a Graph\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_net <- function(Aup, p) {\n  ones <- which(Aup == 1)\n  Aup[ones] <- sample(\n    c(0, 1),\n    length(ones),\n    replace = TRUE,\n    prob = c(p, 1 - p)\n  )\n  Aup\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::bench_time(replicate(1e5, sim_net(Aup, 0.5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nprocess    real \n   1.1s    1.1s \n```\n\n\n:::\n:::\n\n\n### Estimating Probability of Nodes 1 and 10 Disconnected\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(27092016)\nn <- 1e5\ntmp <- replicate(n, discon(sim_net(Aup, 0.05)))\n\nmu_hat <- mean(tmp)\nmu_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00034\n```\n\n\n:::\n:::\n\n\n. . .\n\nEstimate with confidence interval using $\\sigma^2 = \\mu (1 - \\mu)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002257 0.0003400 0.0004543\n```\n\n\n:::\n:::\n\n\n## Importance Sampling\n\nWe will simulate with failure probability $p_0$ and compute the importance\nweights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweights <- function(Aup, Aup0, p0, p) {\n  w <- discon(Aup0)\n\n  if (w) {\n    s <- sum(Aup0)\n    w <- (p / p0)^18 * (p0 * (1 - p) / (p * (1 - p0)))^s\n  }\n\n  as.numeric(w)\n}\n```\n:::\n\n\n. . .\n\nFor the IS estimator the weights will be multiplied by the indicator that 1 and\n10 are disconnected.\n\n### Estimating Probability of Nodes 1 and 10 Disconnected\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- replicate(n, weights(Aup, sim_net(Aup, 0.2), 0.2, 0.05))\nmu_hat_IS <- mean(tmp)\nmu_hat_IS\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002819\n```\n\n\n:::\n:::\n\n\n. . .\n\nConfidence interval using empirical variance estimate $\\hat{\\sigma}^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002493 0.0002819 0.0003144\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(sd(tmp), sqrt(mu_hat * (1 - mu_hat))) # The estimated standard deviations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.005257 0.018436\n```\n\n\n:::\n:::\n\n\n## Comparison\n\nThe ratio of variances is estimated as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_hat * (1 - mu_hat) / var(tmp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.3\n```\n\n\n:::\n:::\n\n\n- Need about 12 times more naive\n  samples compared to importance sampling for same precision.\n\n. . .\n\n- Benchmarking would should that the extra computing time for importance\n  sampling is small compared to the reduction of variance.\n\n. . .\n\nIt is worth the coding effort if used repeatedly, but not if it is a one-off\ncomputation.\n\n## Enumeration\n\nThere are $2^{18} = 262,144$ different networks with any number of the edges\nfailing, so complete enumeration is possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nones <- which(Aup == 1)\nAtmp <- Aup\np <- 0.05\nprob <- numeric(2^18)\n\nfor (i in 0:(2^18 - 1)) {\n  on <- as.numeric(intToBits(i)[1:18])\n  Atmp[ones] <- on\n\n  if (discon(Atmp)) {\n    s <- sum(on)\n    prob[i + 1] <- p^(18 - s) * (1 - p)^s\n  }\n}\n```\n:::\n\n\n## Probability of 1 and 10 Being Disconnected\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(prob) # True value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002883\n```\n\n\n:::\n\n```{.r .cell-code}\n# MC estimate and interval\nmu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002257 0.0003400 0.0004543\n```\n\n\n:::\n\n```{.r .cell-code}\n# IS estimate and interval\nmu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002493 0.0002819 0.0003144\n```\n\n\n:::\n:::\n\n\n## A High-Dimensional Integral\n\n$$\\mu := (2 \\pi)^{-p/2} \\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{i=2}^p (x_i - \\alpha x_{i-1})^2\\right)}  \\mathrm{d} x.$$\n\n. . .\n\n\\begin{align*} \\mu & = (2 \\pi)^{-p/2} \\int e^{- \\frac{1}{2} \\sum*{i = 2}^p\n\\alpha^2 x*{i-1}^2 - 2\\alpha x*i x*{i-1}} e^{-\\frac{||x||*2^2}{2}} \\mathrm{d} x\n\\\\ & = \\int \\underbrace{e^{- \\frac{1}{2} \\sum*{i = 2}^p \\alpha^2 x*{i-1}^2 -\n2\\alpha x_i x*{i-1}}}\\_{h(x)} f(x) \\mathrm{d} x \\end{align*} where $f$ is the\ndensity for the $\\mathcal{N}(0, I_p)$ distribution.\n\n. . .\n\nIn fact, $$\\mu = 1.$$\n\n## A High-Dimensional Integral\n\nFirst we define the function we want to integrate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- function(x, alpha = 0.1) {\n  p <- length(x)\n  tmp <- alpha * x[1L:(p - 1L)]\n  exp(-sum((tmp / 2.0 - x[2L:p]) * tmp))\n}\n```\n:::\n\n\n## A High-Dimensional Integral\n\nThen we specify various parameters and implement the simulations as a simple\nloop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10000 # The number of random variables to generate\np <- 100 # The dimension of each random variable\nalpha <- 0.2\nevaluations <- numeric(n)\nset.seed(123)\n\nfor (i in 1:n) {\n  x <- rnorm(p)\n  evaluations[i] <- h(x, alpha = alpha)\n}\n```\n:::\n\n\n### Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/MC_CLT-1.pdf)\n:::\n:::\n\n\n## Different Target\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.4 #<<\nevaluations <- numeric(n)\nset.seed(123)\n\nfor (i in 1:n) {\n  x <- rnorm(p)\n  evaluations[i] <- h(x, alpha = alpha)\n}\n```\n:::\n\n\n### Convergence\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture6_files/figure-beamer/MC4_CLT-1.pdf)\n:::\n:::\n\n\n### Caution\n\nWith $\\alpha = 0.4$, proposal and target are too different\n\n\n::: {.cell}\n\n:::\n\n\n## Exercise: Von Mises Importance Sampling\n\nImplement an importance sampling algorithm for computing the mean of the von\nMises distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndvmises <- function(x, mu = 0, kappa = 2) {\n  exp(kappa * cos(x - mu)) / (2 * pi * besselI(kappa, 0))\n}\n```\n:::\n\n\nTry various proposals.\n\nIf you have time, create a function that takes parameters of the von Mises\ndistribution and adjusts the proposal distribution based on these.\n\nPlot the result and check convergence.\n\n\n::: {.cell}\n\n:::\n\n\n## Summary\n\n- Monte Carlo methods are useful for estimating integrals and means\n- Importance sampling can reduce variance considerably\n- But the proposal distribution must be chosen carefully\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}