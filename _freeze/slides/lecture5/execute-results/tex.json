{
  "hash": "8b7cf645f5c21554e57d70dad0f0f0e8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Random Number Generation and Rejection Sampling\"\n---\n\n\n\n## Questions from Last Time\n\n### Rule of Thumb Bandwidth for Non-Gaussian Kernels\n\n- Depends on kernel $K$ and reference distribution $\\tilde{f}$ (plugin for $f$):\n  $$\\mathrm{AMISE}(h) = \\frac{\\|K\\|_2^2}{nh} + \\frac{h^4 \\sigma^4_K \\|f_0''\\|_2^2}{4}$$\n- R's `density()` uses Silverman's rule of thumb and adapts to kernel but\n  **not** reference distribution.\n\n. . .\n\n### Alternative Loss Functions for Smoothing Splines\n\n- Definitely several alternatives\n- L1 loss for data-fitting term exists.\n- Not sure about L1 loss on penalty term. Should work but not sure what the\n  optimal spline would look like.\n\n## Today\n\n### Pseudo-Random Numbers\n\n- How do you simulate random numbers in R?\n\n. . .\n\n### Rejection Sampling\n\n- General and useful method for sampling from a target distribution\n\n## Pseudo-Random Numbers\n\n- Computers usually generate pseudo-random numbers\n- Not really random (are any numbers?), but have properties that make them\n  appear to be so\n- A research field in itself, see `?RNG` in R for available algorithms\n\n. . .\n\n### Mersenne Twister\n\n- Default in R\n- Generates integers in the range $$\\{0, 1, \\ldots, 2^{32} -1\\}.$$\n- Long period; all combinations of consecutive integers up to dimension 623\n  occur equally often in a period.\n\n. . .\n\nPseudo-random numbers in $(0, 1)$ are returned by `runif()` by division with\n$2^{32}$ and a fix to prevent the algorithm from returning 0.\n\n## Transformation Methods\n\nIf $T : \\mathcal{Z} \\to \\mathbb{R}$ is a map and $Z \\in \\mathcal{Z}$ is a random\nvariable we can sample, then we can sample $X = T(Z).$ --\n\n. . .\n\n### Inversion Sampling\n\nIf $F^{-1} : (0,1) \\mapsto \\mathbb{R}$ is the generalized inverse of a\ndistribution function and $U$ is uniformly distributed on $(0, 1)$ then\n$$F^{-1}(U)$$ has distribution function $F$.\n\n. . .\n\nComputing quantile function easy for discrete distriutions but hard for\ncontinuous ones.\n\n## Gaussian Random Variables\n\n### [Boxâ€“Muller](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)\n\nA transformation of two independent uniforms into two independent Gaussian\nrandom variables (polar coordinates).\n\n. . .\n\n### [Inversion](https://en.wikipedia.org/wiki/Inverse_transform_sampling)\n\n$X = \\Phi^{-1}(U)$ where $\\Phi$ is the distribution function for the Gaussian\ndistribution.\n\n. . .\n\n### [Rejection Sampling](https://cswr.nrhstat.org/4-3-reject-samp.html)\n\nSee\n[Exercise 5.1 in CSwR](https://cswr.nrhstat.org/random-number-generation#univariate:ex)\nor the [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm).\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRNGkind()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"       \n```\n\n\n:::\n:::\n\n\n### Computing $\\Phi^{-1}$\n\nRecall that\n$$\\Phi(x) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^x e^{-z^2/2} \\mathrm{d} z$$\n\n. . .\n\n[This](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/snorm.c#L265),\ntogether with\n[this technical approximation](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/qnorm.c#L52)\nof $\\Phi^{-1}$. is how R generates samples from $\\mathcal{N}(0,1)$.\n\n. . .\n\nThe technical approximation is a rational function.\n\n## Sampling from a $t$-Distribution\n\nLet $Z = (Y, W) \\in \\mathbb{R} \\times (0, \\infty)$ with\n$Z \\sim \\mathcal{N}(0, 1)$ and $W \\sim \\chi^2_k$ independent.\n\n. . .\n\nDefine $T : \\mathbb{R} \\times (0, \\infty) \\to \\mathbb{R}$ by\n$$T(z,w) = \\frac{z}{\\sqrt{w/k}},$$ then\n$$X = T(Z, W) = \\frac{Z}{\\sqrt{W/k}} \\sim t_k.$$ --\n\nThis is how R simulates from a $t$-distribution with $W$ generated from a Gamma\ndistribution with shape parameter $k / 2$ and scale parameter $2$.\n\n## Exercise\n\n### Step 1\n\nUse that $W = X_1^2 + \\ldots + X_k^2 \\sim  \\chi^2_k$ for $X_1, \\ldots, X_k$\ni.i.d. $\\mathcal{N}(0, 1)$ to implement a function, `my_rchisq()`, such that\n`my_rchisq(n, k)` returns a sample of $n$ i.i.d. observations from $\\chi^2_k$.\n\n\n\n\n\n. . .\n\n### Step 2\n\nMake another function `my_other_rchisq()` that uses inverse sampling.\n\n\n\n. . .\n\n### Step 3\n\nBenchmark your implementations against one another and `rchisq()`.\n\n\n\n## Von Mises Distribution\n\nThe density on $(-\\pi, \\pi]$ is\n$$f(x) = \\frac{1}{2 \\pi I_0(\\kappa)} \\ e^{\\kappa \\cos(x - \\mu)}$$ for\n$\\kappa > 0$ and $\\mu \\in (-\\pi, \\pi]$ parameters and $I_0$ is a modified Bessel\nfunction.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(movMF)\nxy <- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5)))\n\n# rmovMF represents samples as elements on the unit circle\nx <- acos(xy[, 1]) * sign(xy[, 2])\n```\n:::\n\n\n. . .\n\nHard to compute quantile function!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Histogram of samples from rmovMF()](lecture5_files/figure-beamer/vMhist-1.pdf)\n:::\n:::\n\n\n# Rejection Sampling\n\n## Rejection Sampling\n\nWe want to sample from a target distribution $f(x)$ but have no readily\navailable transform $T$ for our proposal $g(x)$.\n\n. . .\n\nLet $Y_1, Y_2, \\ldots$ be i.i.d. with density $g$ on $\\mathbb{R}$ and\n$U_1, U_2, \\ldots$ be i.i.d. uniform and independent of $Y_i$ for all $i$.\n\nDefine $$\\sigma = \\inf\\{n \\geq 1 \\mid U_n \\leq \\alpha f(Y_n) / g(Y_n)\\},$$ for\n$\\alpha \\in (0, 1]$, where $f$ is a density.\n\n. . .\n\n### Theorem\n\nIf $\\alpha f(y) \\leq g(y)$ for all $y \\in \\mathbb{R}$ then the distribution of\n$Y_{\\sigma}$ has density $f$.\n\n. . .\n\n$\\alpha$ is the **acceptance probability** and $g(y)/\\alpha$ the **envelope** of\n$f$.\n\n## Normalizing Constants\n\nIf $f(y) = c q(y)$ and $g(y) = d p(y)$ for (unknown) normalizing constants\n$c, d > 0$ and $\\alpha' q \\leq p$ then <br> <br>\n$$\\underbrace{\\left(\\frac{\\alpha' d}{c}\\right)}_{= \\alpha} \\ f \\leq g.$$\n\n. . .\n\nMoreover,\n\n$$\nu > \\frac{\\alpha f(y)}{g(y)} \\Leftrightarrow u > \\frac{\\alpha'\nq(y)}{p(y)},\n$$\n\nand rejection sampling can be implemented without computing $c$ or $d$.\n\n## Von Mises Rejection Sampling\n\nRejection sampling using the uniform proposal, $g(y) \\propto 1$\n\n. . .\n\nSince $$e^{\\kappa(\\cos(y) - 1)} = \\alpha' e^{\\kappa \\cos(y)} \\leq 1,$$ where\n$\\alpha' = \\exp(-\\kappa)$ we reject if $$U > e^{\\kappa(\\cos(Y) - 1)}.$$\n\n## Von Mises Rejection Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_vonmises_slow <- function(n, kappa) {\n  y <- numeric(n)\n\n  for (i in seq_len(n)) {\n    reject <- TRUE\n\n    while (reject) {\n      y0 <- runif(1, -pi, pi)\n      u <- runif(1)\n      reject <- u > exp(kappa * (cos(y0) - 1))\n    }\n\n    y[i] <- y0\n  }\n\n  y\n}\n```\n:::\n\n\n## Von Mises Rejection Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0))\nx1 <- sample_vonmises_slow(100000, 0.5)\nx2 <- sample_vonmises_slow(100000, 2)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/vMsim2-figs-1.pdf)\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"R/vMSim.R\"), keep.source = TRUE)\nprofvis(sample_vonmises_slow(10000, 5))\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/profile-vsim-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Summary\n\n- Calling random number generators in R sequentially is relatively slow.\n- It is faster to generate all random numbers once and store them in a vector.\n- How to do that for rejection sampling with an unknown number of rejections?\n\n## Von Mises Rejection Sampling\n\n- $Y \\sim \\mathrm{unif}(-\\pi, \\pi)$ (proposal)\n- $U \\sim \\mathrm{unif}(0, 1)$\n- Accept if $U \\leq e^{\\kappa(\\cos(Y) - 1)}$\n\n## Vectorized von Mises Simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_vonmises_sampler <- function(m, kappa) {\n  y <- runif(m, -pi, pi)\n  u <- runif(m)\n  accept <- u <= exp(kappa * (cos(y) - 1))\n  y[accept]\n}\n```\n:::\n\n\n. . .\n\n- Simple and performant expression\n- But returns **random** number of samples\n- We need a function that returns exactly $n$ samples and\n- Problem is we don't know $\\alpha$ in advanced\n\n### Function Factory for Rejection Sampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_rejection_sampler <- function(generator) {\n  function(n, ...) {\n    alpha <- 1\n    y <- numeric(0)\n    n_accepted <- 0\n    while (n_accepted < n) {\n      m <- ceiling((n - n_accepted) / alpha)\n      y_new <- generator(m, ...) #<<\n      n_accepted <- n_accepted + length(y_new)\n      if (length(y) == 0) {\n        alpha <- (n_accepted + 1) / (m + 1) # Estimate of alpha #<<\n      }\n      y <- c(y, y_new)\n    }\n    list(x = y[seq_len(n)], alpha = alpha)\n  }\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_vonmises <- new_rejection_sampler(fixed_vonmises_sampler)\n```\n:::\n\n\n### Benchmarking\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  sample_vonmises(1e5, 5),\n  sample_vonmises_slow(1e5, 5),\n  check = FALSE\n) |>\n  plot()\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/vmises-bench-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Gamma Distribution\n\nWe want to sample from\n$$f_r(x) = \\frac{1}{\\Gamma(r)} x^{r - 1} e^{-x}, \\qquad x > 0.$$\n\nCan we find a suitable envelope?\n\n. . .\n\nPerhaps, but here rejection sampling of a non-standard distribution will be used\nin combination with a simple transformation.\n\n. . .\n\nLet $t(y) = a(1 + by)^3$ for $y \\in (-b^{-1}, \\infty)$, then\n$t(Y) \\sim \\Gamma(r,1)$ if $Y$ has density\n$$f(y) \\propto t(y)^{r-1}t'(y) e^{-t(y)} = e^{(r-1)\\log t(y) + \\log t'(y) - t(y)}.$$\n\n. . .\n\nThe density $f$ will be the _target density_ for a rejection sampler.\n\n## Gamma Distribution\n\nWith $$f(y) \\propto e^{(r-1)\\log t(y) + \\log t'(y) - t(y)},$$ $a = r - 1/3$ and\n$b = 1/(3 \\sqrt{a})$\n$$f(y) \\propto e^{a \\log t(y)/a - t(y) + a \\log a} \\propto \\underbrace{e^{a \\log t(y)/a - t(y) + a}}_{q(y)}.$$\n--\n\nAn analysis of $w(y) := - y^2/2 - \\log q(y)$ shows that it is convex on\n$(-b^{-1}, \\infty)$ and it attains its minimum in 0 with $w(0) = 0$, whence\n$$q(y) \\leq e^{-y^2/2}.$$\n\n### Gamma Rejection Sampling\n\n- $Y \\sim \\mathcal{N}(0,1)$ (proposal)\n- $U \\sim \\mathrm{unif}(0, 1)$\n- Accept if $U \\leq q(Y) e^{Y^2/2}$\n- If accept, return $t(Y)$\n\n### Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntfun <- function(y, a) {\n  b <- 1 / (3 * sqrt(a))\n  (y > -1 / b) * a * (1 + b * y)^3 # 0 when y <= -1/b\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqfun <- function(y, r) {\n  a <- r - 1 / 3\n  tval <- tfun(y, a)\n  exp(a * log(tval / a) - tval + a)\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_gamma_sampler <- function(m, r) {\n  y <- rnorm(m)\n  u <- runif(m)\n  accept <- u <= qfun(y, r) * exp(y^2 / 2)\n  tfun(y[accept], r - 1 / 3)\n}\n```\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_gamma <- new_rejection_sampler(fixed_gamma_sampler)\n```\n:::\n\n\n### Gamma Rejection Sampler Tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- sample_gamma(10000, 8)\nhist(tmp$x, freq = FALSE)\ncurve(dgamma(x, 8), col = \"blue\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Gamma Rejection Sampler Tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- sample_gamma(10000, 3)\nhist(tmp$x, freq = FALSE, ylim = c(0, 0.3))\ncurve(dgamma(x, 3), col = \"blue\", lwd = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Rejection Probabilities\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- sample_gamma(1e5, 16)\n1 - tmp$alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001889981\n```\n\n\n:::\n\n```{.r .cell-code}\ntmp <- sample_gamma(1e5, 8)\n1 - tmp$alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.003609964\n```\n\n\n:::\n\n```{.r .cell-code}\ntmp <- sample_gamma(1e5, 4)\n1 - tmp$alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.007499925\n```\n\n\n:::\n\n```{.r .cell-code}\ntmp <- sample_gamma(1e5, 1)\n1 - tmp$alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0496095\n```\n\n\n:::\n:::\n\n\n## Adaptive Envelopes\n\nWhen $f$ is _log-concave_ on $I$ we can construct bounds of the form\n$$f(x) \\leq e^{V(x)}$$ --\n\nwhere $$V(x) = \\sum_{i=1}^m  (a_i x + b_i) \\mathbf{1}_{I_i}(x)$$ for intervals\n$I_i$ forming a partition of $I$.\n\n. . .\n\nTypically, $a_i x + b_i$ is tangent to the graph of $\\log(f)$ at\n$x_i \\in I_i = (z_{i-1}, z_i]$ for\n$$z_0 < x_1 < z_1 < x_2 < \\ldots < z_{m-1} < x_m < z_m.$$\n\n### Beta Distribution\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/Beta-fig-1.pdf)\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/beta-envelopes-1.pdf)\n:::\n:::\n\n\n### Von Mises Adaptive Envelope\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/vonmises-adaptive-plot-1.pdf)\n:::\n:::\n\n\n### Benchmark\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/benchmark-vonmises-adaptive-1.pdf)\n:::\n:::\n\n\n. . .\n\nDifferences due to R implementation.\n\n### Notes about Adaptive Envelopes\n\n- General and powerful technique\n- But tricky to implement\n- Efficiency depends on number of rejections and also quality of implementation.\n- A good implementation should make use of new points to update envelope.\n\n## Exercise: Boxâ€“Muller Transform\n\nWrite your own version of the `rnorm()` function using the Boxâ€“Muller transform.\n\n> Suppose $U_1$ and $U_2$ are independent samples chosen from the uniform\n> distribution on the unit interval $(0,1)$. Let\n> $$Z_0 = R \\cos(\\Theta) = \\sqrt{-2 \\log U_1} \\cos (2\\pi U_2)$$ and\n> $$Z_1 = R \\sin(\\Theta) = \\sqrt{-2 \\log U_1} \\sin(2 \\pi U_2).$$ Then $Z_0$ and\n> $Z_1$ are i.i.d. $\\mathcal{N}(0,1)$.\n\n. . .\n\n- Test your method\n- Benchmark it against `rnorm()`\n\n### Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxmuller_rnorm <- function(n) {\n  m <- ceiling(n / 2)\n\n  u1 <- runif(m)\n  u2 <- runif(m)\n\n  z0 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)\n  z1 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)\n\n  c(z0, z1)[seq_len(n)]\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n\ntest_that(\"Box-Muller transform generates normal values\", {\n  set.seed(633)\n  x <- boxmuller_rnorm(1e5)\n\n  # Check moments\n  expect_equal(mean(x), 0, tolerance = 1e-2)\n  expect_equal(sd(x), 1, tolerance = 1e-2)\n\n  # Check ECDF against CDF\n  emp <- ecdf(x)\n  z <- seq(-3, 3, length.out = 100)\n  ref <- pnorm(z)\n\n  expect_equal(emp(z), ref, tolerance = 1e-3)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  boxmuller_rnorm(1e6),\n  rnorm(1e6),\n  check = FALSE\n) |>\n  plot()\n```\n\n::: {.cell-output-display}\n![](lecture5_files/figure-beamer/boxmuller-bench-1.pdf){fig-pos='H'}\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}