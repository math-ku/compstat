{
  "hash": "5dd3d8ab69b4dd771002ece285088f2f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The EM-Algorithm\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Today\n\n### The EM Algorithm\n\nUseful method for likelihood optimization in the presence of missing data or\nlatet variables.\n\n. . .\n\n### Fisher Information\n\nWhen the likelihood is optimized, the Fisher information gives the variance of\nthe MLE.\n\n. . .\n\n### Gaussian Mixtures\n\nExample of a finite mixture model where the EM-algorithm is useful.\n\n\n::: {.cell}\n\n:::\n\n\n# The EM Algorithm\n\n## Incomplete Data Likelihood\n\nSuppose that $Y$ is a random variable and $X = M(Y)$. Suppose that $Y$ has\ndensity $f(\\cdot \\mid \\theta)$ and that $X$ has marginal density\n$g(x \\mid \\theta)$.\n\n. . .\n\nThe marginal density is typically of the form\n$$g(x \\mid \\theta) = \\int_{\\{y: M(y) = x\\}} f(y \\mid \\theta) \\ \\mu_x(\\mathrm{d} y)$$\nfor a suitable measure $\\mu_x$ depending on $M$ and $x$ but not $\\theta$.\n\n. . .\n\nIf $Y = (X, Z)$, then\n$$g(x \\mid \\theta) = \\int f(x, z \\mid \\theta) \\, \\mathrm{d} z = \\int f(x \\mid z, \\theta) h(z \\mid \\theta) \\, \\mathrm{d}z.$$\n\n. . .\n\n### Problem\n\n$g(x \\mid \\theta)$ typically impossible (or difficult) to compute.\n\n::: {.notes}\n\nThe general argument for the marginal density relies on the coarea formula.\n\n:::\n\n## Conditional Expectation\n\nThe complete log-likelihood, $\\log f(y \\mid \\theta)$, **is** often easy to\ncompute, but we don't know $Y$, only that $M(Y) = x$.\n\n. . .\n\nIn some cases it is possible to compute\n\n$$Q(\\theta \\mid \\theta') := \\operatorname{E}_{\\theta'}(\\log f(Y \\mid \\theta) \\mid X = x),$$\n\nwhich is the _conditional expectation of the complete log-likelihood given the\nobserved data and under the probability measure given by $\\theta'$._\n\n## The EM Algorithm\n\n### Idea\n\nWith an initial guess of $\\theta' = \\theta^{(n)}$, iteratively compute\n$$\\theta^{(n + 1)} = \\textrm{arg max} \\ Q(\\theta \\mid \\theta^{(n)})$$ for\n$n = 0, 1, 2, \\ldots$.\n\n. . .\n\n### Steps\n\n- **E-step**: Compute the conditional expectation $Q(\\theta \\mid \\theta^{(n)})$.\n- **M-step**: Maximize $Q(\\theta \\mid \\theta^{(n)})$ in $\\theta$.\n\n## Conditional Distributions\n\nGenerally, conditional distribution of $Y$ given $X = x$ has density\n$$h(y \\mid x, \\theta) = \\frac{f(y \\mid \\theta)}{g(x \\mid \\theta)}$$ w.r.t. a\nsuitable measure $\\mu_x$ that does not depend upon $\\theta$.\n\n. . .\n\n$Y = (Z, X)$ with joint density w.r.t. a product measure $\\mu \\otimes \\nu$ that\ndoes not depend upon $\\theta$.\n\n. . .\n\nIn the latter case, $f(y \\mid \\theta) = f(z, x \\mid \\theta)$ and\n$$g(x \\mid \\theta) = \\int f(z, x \\mid \\theta) \\ \\mathrm{d} z.$$\n\n## The Central Identity\n\nIn general\n$$\\ell(\\theta) = \\log g(x \\mid \\theta) = \\log f(y \\mid \\theta) - \\log h(y \\mid x, \\theta),$$\nand under some integrability conditions, this decomposition is used to show that\nthe EM-algorithm increases the log-likelihood, $\\ell(\\theta)$, in each\niteration.\n\n## An Ascent Method\n\nIt can be shown that\n$$\\log g(x \\mid \\theta) - \\log g(x \\mid \\theta') \\geq Q(\\theta | \\theta') - Q(\\theta' \\mid \\theta)$$\n\n. . .\n\nIn other words, marginal likelihood **weakly** increases in each iteration.\n\n. . .\n\nIf $Q(\\theta^{(n+1)} \\mid \\theta) > Q(\\theta^{(n)} \\mid \\theta)$ then the\nalgorithm **strictly** increases the likelihood.\n\n![](../images/em-iterations.png){width=87%}\n\n## Multinomial Complete Data Likelihood\n\nIf $Y \\sim \\textrm{Mult}(p, n)$ the complete data log-likelihood is\n$$\\ell_{\\textrm{complete}}(p) = \\sum_{i=1}^K Y_i \\log(p_i).$$\n\n. . .\n\nThus\n$$Q(p \\mid p') = \\operatorname{E}_{p'}( \\ell_{\\textrm{complete}}(p) \\mid X = x) = \\sum_{i=1}^K \\operatorname{E}_{p'}( Y_i \\mid X = x) \\log(p_i)$$\nfor any $X = M(Y)$.\n\n## E-step for Multinomial Model\n\nFor the multinomial model with $M : \\mathbb{N}_0^K \\to \\mathbb{N}_0^{K_0}$ the\ncell collapsing map corresponding to the partition\n$A_1 \\cup \\ldots \\cup A_{K_0} = \\{1, \\ldots, K \\}$,\n$$\\operatorname{E}_p (Y_k \\mid X = x) = \\frac{x_j p_k}{M(p)_j}.$$ for\n$k \\in A_j$.\n\n. . .\n\n### Abstract E-Step Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEStep_mult <- function(p, x, group) {\n  x[group] * p / M(p, group)[group]\n}\n```\n:::\n\n\n## Multinomial MLE for Moths\n\nWith $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete\nobservation, it can be shown that the MLE is \\begin{align} \\hat{p}_C & =\n(n_{CC} + (n*{CI} + n*{CT}) / 2) / n \\\\ \\hat{p}_I & = ((n_{CI} + n*{IT}) / 2 +\nn*{II}) / n \\end{align} where\n$n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.\n\n. . .\n\nFor us $\\hat{p} = \\frac{1}{n} \\mathbf{X} y$.\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(\n  c(\n    2,\n    1,\n    1,\n    0,\n    0,\n    0,\n    0,\n    1,\n    0,\n    2,\n    1,\n    0\n  ) /\n    2,\n  2,\n  6,\n  byrow = TRUE\n)\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1  0.5  0.5    0  0.0    0\n[2,]    0  0.5  0.0    1  0.5    0\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## Abstract M-Step\n\nMLE of complete log-likelihood is linear estimator in $y / n$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMStep_mult <- function(n, X) {\n  as.vector(X %*% n / (sum(n)))\n}\n```\n:::\n\n\n--\n\n`EStep_mult()` and `MStep_mult()` are abstract implementations. Requires\narguments `group` and `X`.\n\n. . .\n\nM-step only implemented when complete-data MLE is _linear estimator_.\n\n### EM Factory for Multinomial Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM_multinomial <- function(x, group, prob, X) {\n  force(x)\n  force(group)\n  force(prob)\n  force(X)\n\n  EStep <- function(p) EStep_mult(prob(p), x, group)\n  MStep <- function(n) MStep_mult(n, X)\n\n  function(par, epsilon = 1e-6, maxit = 20, cb = NULL) {\n    for (i in seq_len(maxit)) {\n      par0 <- par\n      par <- MStep(EStep(par))\n      if (!is.null(cb)) {\n        cb()\n      }\n      if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {\n        break\n      }\n    }\n    par\n  }\n}\n```\n:::\n\n\n## Peppered Moths EM Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM <- EM_multinomial(\n  x = c(85, 196, 341),\n  group = c(1, 1, 1, 2, 2, 3),\n  prob = prob,\n  X = matrix(\n    c(\n      2,\n      1,\n      1,\n      0,\n      0,\n      0,\n      0,\n      1,\n      0,\n      2,\n      1,\n      0\n    ) /\n      2,\n    2,\n    6,\n    byrow = TRUE\n  )\n)\n\nEM(c(0.3, 0.3))\nmoth_optim$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18877\n[1] 0.07085 0.18872\n```\n\n\n:::\n:::\n\n\n## Inside the EM Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CSwR)\nEM_tracer <- tracer(\"par\")\nEM(c(0.3, 0.3), cb = EM_tracer$tracer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn = 1: par = 0.08039, 0.22464; \nn = 2: par = 0.07119, 0.19547; \nn = 3: par = 0.07085, 0.18993; \nn = 4: par = 0.07084, 0.18895; \nn = 5: par = 0.07084, 0.18877; \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18877\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(EM_tracer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    par.1  par.2     .time\n1 0.08039 0.2246 0.000e+00\n2 0.07119 0.1955 7.391e-05\n3 0.07085 0.1899 1.221e-04\n4 0.07084 0.1889 1.676e-04\n5 0.07084 0.1888 2.131e-04\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Tracing the EM Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM_tracer <- tracer(c(\"par0\", \"par\"), Delta = 0)\nphat <- EM(c(0.3, 0.3), epsilon = 0, cb = EM_tracer$tracer)\nphat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18874\n```\n\n\n:::\n\n```{.r .cell-code}\nEM_trace <- summary(EM_tracer)\ntail(EM_trace)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    par0.1 par0.2   par.1  par.2     .time\n15 0.07084 0.1887 0.07084 0.1887 0.0005455\n16 0.07084 0.1887 0.07084 0.1887 0.0005810\n17 0.07084 0.1887 0.07084 0.1887 0.0006170\n18 0.07084 0.1887 0.07084 0.1887 0.0006564\n19 0.07084 0.1887 0.07084 0.1887 0.0006967\n20 0.07084 0.1887 0.07084 0.1887 0.0007381\n```\n\n\n:::\n:::\n\n\n## Adding Computed Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))\nEM_trace <- transform(\n  EM_trace,\n  n = seq_len(nrow(EM_trace)),\n  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),\n  loglik = loglik_pep(par.1, par.2)\n)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture10_files/figure-beamer/plot-curves-1.pdf)\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture10_files/figure-beamer/tracer-plot1-1.pdf)\n:::\n:::\n\n\nNote the log-axis. The EM-algorithm converges linearly.\n\n## Linear Convergence\n\nThe log-rate of the convergence can be estimated by least-squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_lm <- lm(log(par_norm_diff) ~ n, data = EM_trace)\nexp(coefficients(log_lm)[\"n\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     n \n0.1755 \n```\n\n\n:::\n:::\n\n\n. . .\n\nIt is very small in this case implying fast convergence.\n\n. . .\n\nThis is not always the case. If the log-likelihood is flat, the EM-algorithm can\nbecome quite slow with a rate close to 1.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Log-likelihood convergence](lecture10_files/figure-beamer/loglikelihood-convergence-1.pdf)\n:::\n:::\n\n\n## Optimization and Statistics\n\nThe EM-algorithm is a general algorithm for numerical optimization of a\nlog-likelihood function. It works by iteratively optimizing\n\n$$Q(\\theta \\mid \\theta^{(n)}) = E_{\\theta^{(n)}}(\\log f(Y \\mid \\theta) \\mid X = x).$$\n\n. . .\n\nFor numerical optimization of $Q$ or variants of EM (generalized EM algorithms)\nthe gradient and Hessian of $Q$ can be useful.\n\n. . .\n\nFor statistics we need the observed Fisher information (Hessian of the negative\nlog-likelihood for the observed data).\n\n## Local Maxima\n\n- $Q$ is not typically convex, so no guarantee that the EM-algorithm converges\n  to the global maximum. --\n\n- How to deal with this?\n- Random starting values.\n- Global optimization (e.g. simulated annealing, genetic algorithms).\n\n# Fisher Information\n\n## Gradient of $Q$ for the Multinomial Model\n\nNote that with $p = p(\\theta)$ in some parametrization of the cell\nprobabilities,\n$$Q(\\theta \\mid \\theta') = \\sum_{i} \\frac{x_{j(i)} p_i(\\theta')}{M(p(\\theta'))_{j(i)}} \\log p_i(\\theta),$$\nwhere $j(i)$ is defined by $i \\in A_{j(i)}$.\n\n. . .\n\nThe gradient of $Q$ w.r.t. $\\theta$\n$$\\nabla_{\\theta} Q(\\theta \\mid \\theta') = \\sum_{i} \\frac{x_{j(i)}p_i(\\theta')}{M(p(\\theta'))_{j(i)}p_i(\\theta)} \\nabla p_i(\\theta).$$\n\n. . .\n\nWhen evaluated in $\\theta'$ it is\n\n$$\\nabla_{\\theta} Q(\\theta' \\mid \\theta') = \\sum_{i} \\frac{x_{j(i)}}{M(p(\\theta'))_{j(i)}} \\nabla p_i(\\theta').$$\n\n### Gradient Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDprob <- function(p) {\n  matrix(\n    c(\n      2 * p[1],\n      0,\n      2 * p[2],\n      2 * p[1],\n      2 * p[3] - 2 * p[1],\n      -2 * p[1],\n      0,\n      2 * p[2],\n      -2 * p[2],\n      2 * p[3] - 2 * p[2],\n      -2 * p[3],\n      -2 * p[3]\n    ),\n    ncol = 2,\n    nrow = 6,\n    byrow = TRUE\n  )\n}\n\ngrad_Q <- function(p, x = c(85, 196, 341)) {\n  p[3] <- 1 - p[1] - p[2]\n  group <- c(1, 1, 1, 2, 2, 3)\n  (x[group] / M(prob(p), group)[group]) %*% Dprob(p)\n}\n```\n:::\n\n\n### Gradient Identity\n\nThough computed as the gradient of $Q$,\n$$\\nabla_{\\theta} Q(\\theta' \\mid \\theta') = \\nabla_{\\theta} \\ell(\\theta')$$ from\nthe fact that\n$$\\theta' = \\arg\\max_{\\theta} \\left(Q(\\theta \\mid \\theta') -  \\ell(\\theta)\\right).$$\n\n. . .\n\nCan also be verified by direct computation.\n\n. . .\n\nThe gradient is effectively zero in the limit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_hat <- EM(c(0.3, 0.3), epsilon = 1e-20)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18874\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad_Q(p_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,] -1.456e-09 -7.168e-09\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## Empirical Fisher Information\n\nRecall that a multinomial observation with size parameter $n$ can be regarded as\n$n$ i.i.d. observations.\n\n. . .\n\nFor i.i.d. observations the Fisher information (for one sample) can be estimated\nas the empirical variance of the gradient of the log-likelihood. By the identity\n$$\\nabla_{\\theta} Q_i(\\theta' \\mid \\theta') = \\nabla_{\\theta} \\ell_i(\\theta')$$\nholding for each observation, we can compute the empirical variance.\n\n### Moths Empirical Fisher\n\nCan think of the moths as i.i.d. observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemp_Fisher <- function(p, x = c(85, 196, 341)) {\n  grad1 <- grad_Q(p, c(1, 0, 0))\n  grad2 <- grad_Q(p, c(0, 1, 0))\n  grad3 <- grad_Q(p, c(0, 0, 1))\n  x[1] *\n    t(grad1) %*% grad1 +\n    x[2] * t(grad2) %*% grad2 +\n    x[3] * t(grad3) %*% grad3\n}\n```\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemp_Fisher(p_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18488 1385\n[2,]  1385 6817\n```\n\n\n:::\n:::\n\n\n### Numerical Fisher information\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n#### `stats::optimHess()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-optimHess(p_hat, loglik, grad_Q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18491 1385\n[2,]  1385 6817\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n#### `numDeriv::jacobian()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(numDeriv)\nihat <- -jacobian(grad_Q, p_hat)\nihat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18488 1385\n[2,]  1385 6817\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n- Different estimates of same quantity\n- Should be close but not identical\n\n### Measuring Variance\n\n- Fisher information can be used to compute standard errors of MLEs\n- Empirical Fisher information can be computed for i.i.d. observations using the\n  gradients $\\nabla_{\\theta} Q_i(\\bar{\\theta} \\mid \\hat{\\theta})$ evaluated in\n  $\\bar{\\theta} = \\hat{\\theta}$. --\n\n- Observed Fisher information can be computed by numerical differentiation of\n  the gradient $-\\nabla_{\\theta} Q(\\bar{\\theta} \\mid \\bar{\\theta})$ evaluated in\n  $\\bar{\\theta} = \\hat{\\theta}$.\n\n. . .\n\n#### Bootstrap\n\n- Another alternative is to use the bootstrap to compute standard errors.\n- Can be done nonparametrically\n- Or parametrically using the model assumptions.\n- Computationally more expensive\n\n## Second Way: The Information Identity\n\nFrom $$\\ell(\\theta) = Q(\\theta \\mid \\theta') + H(\\theta \\mid \\theta')$$ it\nfollows that the observed Fisher information equals\n\n$$\n\\hat{i}_X := - D^2_{\\theta} \\ell(\\hat{\\theta}) =\n\\underbrace{-D^2_{\\theta} Q(\\hat{\\theta} \\mid \\theta')}_{\\hat{i}_Y(\\theta')} -\n\\underbrace{D^2_{\\theta} H(\\hat{\\theta} \\mid \\theta')}_{\\hat{i}_{Y \\mid X}(\\theta')}.\n$$\n\n. . .\n\nIntroducing $\\hat{i}_Y := \\hat{i}_Y(\\hat{\\theta})$ and\n$\\hat{i}_{Y \\mid X} = \\hat{i}_{Y \\mid X}(\\hat{\\theta})$. We have the\n_information identity_ $$\\hat{i}_X = \\hat{i}_Y - \\hat{i}_{Y \\mid X}.$$\n\n. . .\n\n### Interpretation\n\n- $\\hat{i}_Y$ is the Fisher information for complete $Y$.\n- $\\hat{i}_{Y \\mid X}$ is the information \"lost\" from not observing full $X$.\n\n### More Identities\n\nHow to compute the information loss?\n\n. . .\n\nThe second **Bartlett identity** can be reformulated as\n\n$$\n\\partial_{\\theta_i} \\partial_{\\theta_j} H(\\bar{\\theta} \\mid \\bar{\\theta})\n= - \\partial_{\\theta'_i} \\partial_{\\theta_j} H(\\bar{\\theta} \\mid \\bar{\\theta})\n$$\n\nwhich follows from differentiation under the integral of\n$\\int h(y \\mid x, \\theta) \\mu_x(d y) = 1.$\n\n. . .\n\nAnd since we also have\n\n$$Q(\\theta \\mid \\theta') = \\ell(\\theta) - H(\\theta \\mid \\theta')$$\n\nwe find that\n\n$$\n\\partial_{\\theta'_i} \\partial_{\\theta_j} Q(\\bar{\\theta} \\mid \\bar{\\theta}) =  -\n\\partial_{\\theta'_i} \\partial_{\\theta_j} H(\\bar{\\theta} \\mid \\bar{\\theta}) =\n\\partial_{\\theta_i} \\partial_{\\theta_j} H(\\bar{\\theta} \\mid \\bar{\\theta}).\n$$\n\n--\n\nThus\n\n$$\\hat{i}_{Y \\mid X} = D_{\\theta'} \\nabla_{\\theta} Q(\\hat{\\theta} \\mid \\hat{\\theta}).$$\n\n### New Implementations\n\nFirst we implement the map $Q$ as an R function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQ <- function(p, pp, x = c(85, 196, 341)) {\n  p[3] <- 1 - p[1] - p[2]\n  pp[3] <- 1 - pp[1] - pp[2]\n  group <- c(1, 1, 1, 2, 2, 3)\n  (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p))\n}\n```\n:::\n\n\n--\n\nand a modified `grad_Q()` of two arguments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad_Q <- function(p, pp, x = c(85, 196, 341)) {\n  p[3] <- 1 - p[1] - p[2]\n  group <- c(1, 1, 1, 2, 2, 3)\n  (x[group] * prob(pp) / (M(prob(pp), group)[group] * prob(p))) %*% Dprob(p)\n}\n```\n:::\n\n\n### Numerical Differentiation of $Q$\n\nWe use numDeriv functions `jacobian()` and `hessian()` to differentiate\nnumerically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niY <- -hessian(Q, p_hat, pp = p_hat)\niY\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 19242 1680\n[2,]  1680 8271\n```\n\n\n:::\n:::\n\n\n--\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-jacobian(grad_Q, p_hat, pp = p_hat) # This should be the same as iY\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 19242 1680\n[2,]  1680 8271\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\niYX <- jacobian(function(pp) grad_Q(p_hat, pp), p_hat)\niY - iYX # same as `emp_Fisher(p_hat)`\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18488 1385\n[2,]  1385 6817\n```\n\n\n:::\n:::\n\n\n## Third Way: The EM-Mapping\n\nDefine $\\Phi : \\Theta \\mapsto \\Theta$ by\n$$\\Phi(\\theta') = \\textrm{arg max}_{\\theta} \\ Q(\\theta \\mid \\theta').$$\n\n. . .\n\nA global maximum of the likelihood is a fixed point of $\\Phi$,\n$\\Phi(\\theta) = \\theta.$\n\n. . .\n\nUsing that the limit of the EM algorithm, $\\hat{\\theta}$, is a fixed point and\nother identities above, it can be shown that\n$$D_{\\theta} \\Phi(\\hat{\\theta})^T = \\hat{i}_{Y\\mid X} \\left(\\hat{i}_Y\\right)^{-1}.$$\n\n. . .\n\nHence\n\n$$\n\\begin{aligned} \\hat{i}_X & = \\left(I - \\hat{i}_{Y\\mid X}\n\\left(\\hat{i}_Y\\right)^{-1}\\right) \\hat{i}\\_Y = \\left(I - D_{\\theta}\n\\Phi(\\hat{\\theta})^T\\right) \\hat{i}\\_Y. \\end{aligned}\n$$\n\n. . .\n\n$D_{\\theta} \\Phi(\\hat{\\theta})$ can be computed via numerical differentiation.\n\n### EM Step Implementation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM_multinomial_step <- function(x, group, prob, X) {\n  force(x)\n  force(group)\n  force(prob)\n  force(X)\n\n  EStep <- function(p) EStep_mult(prob(p), x, group)\n\n  MStep <- function(n) MStep_mult(n, X)\n\n  function(par) MStep(EStep(par))\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPhi <- EM_multinomial_step(c(85, 196, 341), c(1, 1, 1, 2, 2, 3), prob, X)\np_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18874\n```\n\n\n:::\n\n```{.r .cell-code}\nPhi(p_hat) # The limit is a fixed point\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07084 0.18874\n```\n\n\n:::\n:::\n\n\n### Differentiating the EM Map\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDPhi <- jacobian(Phi, p_hat) # Using numDeriv function 'jacobian()'\niX <- (diag(1, 2) - t(DPhi)) %*% iY\niX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18488 1385\n[2,]  1385 6817\n```\n\n\n:::\n\n```{.r .cell-code}\nihat # Computed using numerical differentiation of grad_Q\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2]\n[1,] 18488 1385\n[2,]  1385 6817\n```\n\n\n:::\n:::\n\n\n# Gaussian Mixtures\n\n## Finite Mixtures\n\nLet $Z \\in \\{1, \\ldots, K\\}$ with $P(Z = k) = p_k$, and the conditional\ndistribution of $X$ given $Z = k$ has density $f_k( \\cdot \\mid \\psi_k)$.\n\n. . .\n\nThe joint density is $$(x, k) \\mapsto f_k(x \\mid \\psi_k) p_k$$\n\n. . .\n\nand the marginal density for the distribution of $X$ is\n$$f(x \\mid \\theta) =  \\sum_{k=1}^K f_k(x \\mid \\psi_k) p_k.$$\n\n## Gaussian Mixtures $(K = 2)$\n\nThe two Gaussian distributions are parametrized by five parameters\n$\\mu_1, \\mu_2 \\in \\mathbb{R}$ and $\\sigma_1, \\sigma_2 > 0$, and\n$p = P(Z = 1) = 1 - P(Z = 2)$.\n\n. . .\n\nThe conditional distribution of $X$ given $Z = k$ is\n$$f_k(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} e^{-\\frac{(x - \\mu_k)^2}{2 \\sigma_k^2}}.$$\n\n. . .\n\nThe marginal density is\n\n$$\nf(x) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(x - \\mu_1)^2}{2 \\sigma_1^2}} +\n(1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(x - \\mu_2)^2}{2 \\sigma_2^2}}.\n$$\n\n### Simulation\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma1 <- 1\nsigma2 <- 2\nmu1 <- -0.5\nmu2 <- 4\np <- 0.5\n\nn <- 5000\nz <- sample(\n  c(TRUE, FALSE),\n  n,\n  replace = TRUE,\n  prob = c(p, 1 - p)\n)\n\n# Conditional simulation\n# from mixture components\nx <- numeric(n)\nn1 <- sum(z)\nx[z] <- rnorm(n1, mu1, sigma1)\nx[!z] <- rnorm(n - n1, mu2, sigma2)\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture10_files/figure-beamer/unnamed-chunk-13-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Log-likelihood\n\nWe assume $\\sigma_1$ and $\\sigma_2$ known.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik <- function(par, x) {\n  p <- par[1]\n\n  if (p < 0 || p > 1) {\n    return(Inf)\n  }\n\n  mu1 <- par[2]\n  mu2 <- par[3]\n\n  -sum(log(\n    p *\n      exp(-(x - mu1)^2 / (2 * sigma1^2)) /\n      sigma1 +\n      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2\n  ))\n}\n```\n:::\n\n\n## Optimizing\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]  0.5131 -0.4985  4.0441\n\n$value\n[1] 6865\n```\n\n\n:::\n:::\n\n\n. . .\n\nAgain, however, initialization matters!\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim(c(0.9, 3, 1), loglik, x = x)[c(1, 2)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1] 0.2180 5.6499 0.6148\n\n$value\n[1] 7414\n```\n\n\n:::\n:::\n\n\n## The $Q$-Function\n\nThe complete data log-likelihood is\n$$\\sum_{i=1}^n 1(z_i = 1) \\left(\\log(p) - \\frac{(x_i - \\mu_1)^2}{2 \\sigma_1^2} \\right) + 1(z_i = 2)\\left( \\log(1-p) - \\frac{(x_i - \\mu_2)^2}{2 \\sigma_2^2} \\right) $$\n\n. . .\n\nand\n\n$$Q(\\theta \\mid \\theta')  = \\sum_{i=1}^n \\hat{p}_{i} \\left(\\log(p) - \\frac{(x_i - \\mu_1)^2}{2 \\sigma_1^2} \\right) + (1 - \\hat{p}_{i})\\left( \\log(1-p) - \\frac{(x_i - \\mu_2)^2}{2 \\sigma_2^2} \\right)$$\n\nwhere $\\hat{p}_i = P_{\\theta'}(Z_i = 1 \\mid X_i = x_i)$.\n\n. . .\n\nThe maximum is attained at\n\n$$\n\\theta = \\left(\\frac{1}{n} \\sum_{i} \\hat{p}_i, \\frac{1}{\\sum_{i} \\hat{p}_i} \\sum_{i} \\hat{p}_i x_i,\n\\frac{1}{\\sum_{i} (1 - \\hat{p}_i)} \\sum_{i} (1 - \\hat{p}_i) x_i \\right).\n$$\n\n### The E-Step\n\nThe conditional probability in a mixture model is generally\n\n$$P(Z = z \\mid X = x) = \\frac{p_z f_z(x \\mid \\psi_z)}{\\sum_{k = 1}^K p_k f_k(x \\mid \\psi_k)}$$\n\n. . .\n\nwhich for the $K = 2$ Gaussian case gives\n\n$$\n\\hat{p}_i = P_{\\theta'} (Z_i = 1 \\mid X = x_i) =\n\\frac{ p'_1 e^{-\\frac{(x_i - \\mu_1')^2}{2 \\sigma_1^2}}}{\n\\left( p'_1 e^{-\\frac{(x_i - \\mu_1')^2}{2 \\sigma_1^2}} +\n\\frac{\\sigma_1 (1 - p'_1)}{\\sigma_2} e^{-\\frac{(x_i - \\mu_2')^2}{2 \\sigma_2^2}}\\right) }.\n$$\n\n. . .\n\n### Implementation\n\nSee [`em_gauss_mix.R`](R/em_gauss_mix.R) for the source file of the\nimplementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"R/em_gauss_mix.R\"))\n```\n:::\n\n\n## EM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM <- EM_gauss_mix(x)\n```\n:::\n\n\n-- and testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM(c(0.5, -0.5, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.5131 -0.4986  4.0438\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]  0.5131 -0.4985  4.0441\n\n$value\n[1] 6865\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEM(c(0.9, 3, 1)) # Starting value still matters\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2181 5.6502 0.6147\n```\n\n\n:::\n:::\n\n\n## Gradients and Numerical Differentiation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(numDeriv)\ngrad1 <- function(par) grad(function(par) loglik(par, x), par)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEStep <- environment(EM)$EStep\nQ <- function(par, par_prime) {\n  phat <- EStep(par_prime)\n  p <- par[1]\n  mu1 <- par[2]\n  mu2 <- par[3]\n  sum(\n    phat *\n      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +\n      (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))\n  )\n}\ngrad2 <- function(par) -grad(Q, par, par_prime = par)\n```\n:::\n\n\n## Gradient Identity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad1(c(0.5, 0, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -640.52 1045.89  -62.78\n```\n\n\n:::\n\n```{.r .cell-code}\ngrad2(c(0.5, 0, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -640.52 1045.89  -62.78\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar_hat <- EM(c(0.5, 0, 4))\ngrad1(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.005611 0.001738 0.001157\n```\n\n\n:::\n\n```{.r .cell-code}\ngrad2(par_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.005611 0.001738 0.001157\n```\n\n\n:::\n:::\n\n\n### Convergence\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(CSwR)\n\nEM_tracer <- tracer(\n  c(\"par0\", \"par\", \"loglik\", \"h_prime\"),\n  Delta = 0,\n  expr = quote({\n    loglik <- loglik(par, x)\n    h_prime <- sum(grad2(par)^2)\n  })\n)\n\npar_hat <- EM(c(0.2, 2, 2), cb = EM_tracer$tracer)\n\nEM_trace <- summary(EM_tracer)\ntail(EM_trace, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   par0.1  par0.2 par0.3  par.1   par.2 par.3 loglik   h_prime    .time\n33 0.5131 -0.4986  4.044 0.5131 -0.4986 4.044   6865 5.043e-04 0.003190\n34 0.5131 -0.4986  4.044 0.5131 -0.4986 4.044   6865 1.709e-04 0.003266\n35 0.5131 -0.4986  4.044 0.5131 -0.4986 4.044   6865 5.790e-05 0.003339\n36 0.5131 -0.4986  4.044 0.5131 -0.4986 4.044   6865 1.962e-05 0.003414\n```\n\n\n:::\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(\n  EM_trace,\n  y = loglik - min(loglik)\n)\n```\n\n::: {.cell-output-display}\n![](lecture10_files/figure-beamer/convergence-plots-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(\n  EM_trace,\n  y = h_prime\n)\n```\n\n::: {.cell-output-display}\n![](lecture10_files/figure-beamer/convergence-plots-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n::::\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=latex}\n\\animategraphics[,controls,loop]{1}{lecture10_files/figure-beamer/mixture-convergence-}{1}{20}\n```\n:::\n:::\n\n\n## Summary\n\n- We introduced the EM-algorithm, using the pepper moth example throughout\n- We showed three ways to compute the Fisher information\n- We covered Gaussian mixtures and how to optimize over their parameters using\n  the EM algorithm\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{animate}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}