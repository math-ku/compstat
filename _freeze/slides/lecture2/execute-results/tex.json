{
  "hash": "34dceb7e9e0f50a49880ff36def1c8f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Object-Oriented Programming and Density Estimation\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Today's Agenda\n\n### Object-Oriented Programming in R\n\nHow to create and work with S3 classes\n\n. . .\n\n### Kernel Density Estimation\n\nHow to create non-parametric density estimates using kernel density estimation\n\n# Object-Oriented Programming in R\n\n## Many Systems\n\n### In Base R\n\n- S3\n- S4\n- Reference Classes\n\n. . .\n\n### Through Packages\n\n- [R6](https://cran.r-project.org/package=R6)\n- [S7](https://cran.r-project.org/package=S7)\n\n### This Course\n\nWe will focus **entirely** on S3: a _very_ informal OOP system.\n\n## Example: Integration\n\nThe function `integrate()` takes a function as argument and returns the value of\nnumerically integrating the function. --\n\n## It is an example of a _functional_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintegral <- integrate(sin, 0, 1)\nintegral\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.4596977 with absolute error < 5.1e-15\n```\n\n\n:::\n:::\n\n\nThe numerical value of the integral $$\\int_0^1 \\sin(x) \\mathrm{d}x$$ is printed\nnicely aboveâ€”including an indication of the numerical error.\n\n## Return Values\n\nIn fact, `integrate()` returns a class object: a list with a _class label_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(integral)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 5\n $ value       : num 0.46\n $ abs.error   : num 5.1e-15\n $ subdivisions: int 1\n $ message     : chr \"OK\"\n $ call        : language integrate(f = sin, lower = 0, upper = 1)\n - attr(*, \"class\")= chr \"integrate\"\n```\n\n\n:::\n:::\n\n\n### What is the Point of This Class Label?\n\nIt allows us to write functions that work differently depending on the class of\nthe argument.\n\nIf `x` is an object of class `numeric`, then do A. If `x` is an object of class\n`integrate`, then do B.\n\n## The Return Value of `integrate()`\n\nThe class label can be extracted directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(integral)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"integrate\"\n```\n\n\n:::\n:::\n\n\n--\n\n- The printed result of `integrate()` is not the same as the object itself.\n- What you see is the result of the **method** `print.integrate()`.\n\n## Printing Objects of Class Integrate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats:::print.integrate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (x, digits = getOption(\"digits\"), ...) \n{\n    if (x$message == \"OK\") \n        cat(format(x$value, digits = digits), \" with absolute error < \", \n            format(x$abs.error, digits = 2L), \"\\n\", sep = \"\")\n    else cat(\"failed with message \", sQuote(x$message), \"\\n\", \n        sep = \"\")\n    invisible(x)\n}\n<bytecode: 0x16d5d090>\n<environment: namespace:stats>\n```\n\n\n:::\n:::\n\n\n(The `print.integrate()` function is not exported from the stats package. It is\nin the namespace of the **stats** package, and to access it directly we use\n`stats:::`.)\n\n## Histogram Objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphi_hist <- hist(phipsi$phi, main = NULL)\n```\n\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/temp-hist-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Histogram Objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(phi_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"histogram\"\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(phi_hist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ breaks  : num [1:14] -3.5 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 ...\n $ counts  : int [1:13] 5 19 62 105 138 56 1 1 6 15 ...\n $ density : num [1:13] 0.0239 0.0907 0.2959 0.5012 0.6587 ...\n $ mids    : num [1:13] -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 ...\n $ xname   : chr \"phipsi$phi\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n```\n\n\n:::\n:::\n\n\n## Histogram Objects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphi_hist[1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$breaks\n [1] -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5  3.0\n\n$counts\n [1]   5  19  62 105 138  56   1   1   6  15   9   1   1\n\n$density\n [1] 0.02386635 0.09069212 0.29594272 0.50119332 0.65871122 0.26730310\n [7] 0.00477327 0.00477327 0.02863962 0.07159905 0.04295943 0.00477327\n[13] 0.00477327\n\n$mids\n [1] -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25  0.25  0.75  1.25  1.75  2.25\n[13]  2.75\n```\n\n\n:::\n:::\n\n\n## Getting Help for Objects\n\nYou can find documentation for `plot()` using e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?plot\n```\n:::\n\n\nHowever, this will be uninformative on how an object of class histogram is\nplotted. Try instead\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?plot.histogram\n```\n:::\n\n\nThis will give the documentation for the plot method for objects of class\nhistogram.\n\n## S3 Overview\n\n- S3 classes are standard data structures (typically lists) with _class labels_.\n- It is an informal system. No checks of object content.\n- One implements a _generic_ function via `UseMethod()`. E.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (x, y, ...)  UseMethod(\"plot\")\n```\n\n\n:::\n:::\n\n\n. . .\n\n- **Methods** for specific classes are implemented as standard R functions with\n  the naming convention `f.classname()` for a method for class `classname` of\n  the function `f()`. --\n\n- The system is widely used to write methods for the generic functions\n  `print()`, `plot()` and `summary()`.\n\n## Constructing a New Class\n\n\n::: {.cell}\n\n:::\n\n\n## Recall the function `count_zeros_vec()` that counts the number of zeros in a vector.\n\nIf we need this number many times it is beneficial to compute it once and then\nextract it whenever needed.\n\n. . .\n\nWe first write a _constructor function_ that returns a list with a class label.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_object <- function(x) {\n  structure(\n    list(\n      x = x,\n      n = count_zeros_vec(x)\n    ),\n    class = \"count_object\"\n  )\n}\n```\n:::\n\n\n## A Data Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ncount_data <- count_object(rpois(10, 2))\ncount_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n [1] 0 2 2 2 4 2 0 1 2 2\n\n$n\n[1] 2\n\nattr(,\"class\")\n[1] \"count_object\"\n```\n\n\n:::\n:::\n\n\n## The Generic Function\n\nTo activate looking up a method for a specific class, one needs to tell R that\nthe function `count_zeros()` is a _generic function_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros <- function(x) {\n  UseMethod(\"count_zeros\")\n}\n```\n:::\n\n\n--\n\nWe can let the default method be the vectorized version.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros.default <- function(x) {\n  count_zeros_vec(x)\n}\n```\n:::\n\n\n. . .\n\nThen we can implement a class-specific version for the class `count_object`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros.count_object <- function(x) {\n  x[[\"n\"]]\n}\n```\n:::\n\n\n## A Print Method\n\nAnd we can also implement a print method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint.count_object <- function(x) {\n  cat(\"Values:\", x$x, \"\\nNumber of zeros:\", x$n, \"\\n\")\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_data # Invokes print.count_object()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nValues: 0 2 2 2 4 2 0 1 2 2 \nNumber of zeros: 2 \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros(count_data) # Invokes count_zeros.count_object()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n## Exercise 1\n\n### Exercise 1\n\nCreate a constructor function called `summarize_vector()` that that takes a\nnumeric vector as input and returns an object of class `vector_summary`\ncontaining the mean, median, and standard deviation of the input vector.\n\n\n\n. . .\n\n### Exercise 2\n\nWrite a `print()` method for the class `vector_summary` that prints the mean,\nmedian, and standard deviation in a neatly formatted way.\n\n\n\n# Density Estimation\n\n## Density Estimation\n\nLet $f_0$ denote the unknown desnity we want to estimate.\n\n- If we fit a parameterize statistical model $(f_\\theta)_\\theta$ to data using\n  the estimator $\\hat{\\theta}$, then $f_{\\hat{\\theta}}$ is an estimate of $f_0$.\n  --\n\n- The histogram is a nonparameteric density estimator, $\\hat{f}$.\n- We are interested in nonparametric estimators because\n  - we want to compare data with the parametric estimate,\n  - we don't know a suitable parametric model, and\n  - we want aid in visualization.\n\n## Density Estimation\n\nDensity estimation relies on the approximation\n$$P(X \\in (x-h, x+h)) = \\int_{x-h}^{x+h} f_0(z) \\ dz \\simeq f_0(x) 2h.$$ --\n\nRearranging and using LLN gives\n\n\\begin{align*} f*0(x) & \\simeq \\frac{1}{2h}P(X \\in (x-h, x+h)) \\\\ & \\simeq\n\\frac{1}{2h} \\frac{1}{n} \\sum*{i=1}^n 1*{(x-h, x+h)}(x_i) \\\\ & = \\frac{1}{2hn}\n\\sum*{i=1}^n 1\\_{(-h, h)}(x - x_i) = \\hat{f}\\_h(x) \\end{align*}\n\n## Kernels\n\nWe will consider _kernel estimators_\n$$\\hat{f}_h(x) = \\frac{1}{hn} \\sum_{i=1}^n K\\left(\\frac{x - x_i}{h}\\right).$$\n\n. . .\n\nThe _uniform_ or _rectangular kernel_ is $$K(x) = \\frac{1}{2} 1_{(-1,1)}(x),$$\nwhich leads to the expression on the last slide.\n\n. . .\n\nThe _Gaussian kernel_ is $$K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}.$$\n\n## Implementation with the Gaussian Kernel\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_dens <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m)\n\n  for (i in seq_along(xx)) {\n    for (j in seq_along(x)) {\n      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))\n    }\n  }\n\n  y <- y / (sqrt(2 * pi) * h * length(x))\n\n  list(x = xx, y = y)\n}\n```\n:::\n\n\n### An Illustration\n\n\n::: {.cell}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nLet's say we have a data set\n\n$$\n\\boldsymbol{x} = (-1, 3, 5, 6, 9).\n$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-22-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n### An Illustration: Gaussian Kernel\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nWe add a Gaussian density kernel with bandwidth 1 for each point.\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-23-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n### An Illustration: Gaussian Kernel\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nFinally, we average the kernels.\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-24-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Angle Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(phipsi$psi, prob = TRUE, xlab = expression(psi), main = NULL)\nrug(phipsi$psi)\n```\n\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-25-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## A First Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf_hat <- kern_dens(phipsi$psi, 0.2)\nf_hat_dens <- density(phipsi$psi, 0.2)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(\n  f_hat,\n  type = \"l\",\n  lwd = 4,\n  xlab = \"x\",\n  ylab = \"Density\"\n)\n\nlines(\n  f_hat_dens,\n  col = \"red\",\n  lwd = 2\n)\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/test-dens-output-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## A First Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(\n  f_hat$x,\n  f_hat$y - f_hat_dens$y,\n  type = \"l\",\n  lwd = 2,\n  xlab = \"x\",\n  ylab = \"Difference\"\n)\n```\n\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Testing with **testthat**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n\ntest_that(\"Our density implementation corresponds to density()\", {\n  expect_equal(kern_dens(phipsi$psi, 0.2)$y, density(phipsi$psi, 0.2)$y)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-- Failure: Our density implementation corresponds to density() ----------------\nkern_dens(phipsi$psi, 0.2)$y not equal to density(phipsi$psi, 0.2)$y.\n511/512 mismatches (average diff: 4e-05)\n[1] 0.000136 - 0.000138 == -1.69e-06\n[2] 0.000170 - 0.000171 == -1.29e-06\n[3] 0.000211 - 0.000213 == -1.81e-06\n[4] 0.000261 - 0.000264 == -2.89e-06\n[5] 0.000322 - 0.000325 == -3.76e-06\n[6] 0.000394 - 0.000398 == -4.16e-06\n[7] 0.000480 - 0.000484 == -3.79e-06\n[8] 0.000581 - 0.000584 == -2.44e-06\n[9] 0.000701 - 0.000706 == -4.84e-06\n...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError:\n! Test failed\n```\n\n\n:::\n:::\n\n\n## Tolerance\n\nIt is often necessary to allow for small differences in floating point numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\ntest_that(\"Our density implementation corresponds to density()\", {\n  expect_equal(\n    kern_dens(phipsi$psi, 0.2)$y,\n    density(phipsi$psi, 0.2)$y,\n    tolerance = 1e-3 #<<\n  )\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest passed \n```\n\n\n:::\n:::\n\n\n. . .\n\n### Setting Tolerance Level\n\n- You need to decide on the tolerance level on a case-by-case basis.\n- Uses `all.equal()` internally (but depends on testthat edition!). Usually\n  tests **relative** difference (but not always!)\n\n## Density Estimation\n\nFor a parametric family we can use the MLE\n$$\\hat{\\theta} = \\text{arg max}_{\\theta} \\sum_{j=1}^n \\log f_{\\theta}(x_j).$$\n\nFor nonparametric estimation we can still introduce the log-likelihood:\n\n$$\n\\ell(f) = \\sum_{j=1}^n \\log f(x_j)\n$$\n\nLet's see what happens for the Gaussian kernel density estimate\n\n$$\nf(x) = f_h(x) = \\frac{1}{nh \\sqrt{2 \\pi}} \\sum_{j=1}^n e^{- \\frac{(x - x_j)^2}{2 h^2} }.\n$$\n\n## Bandwidth Selection\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_h <- function(x, h) mean(dnorm(x, phipsi$psi, h))\nf_h <- Vectorize(f_h)\nhist(phipsi$psi, prob = TRUE)\nrug(phipsi$psi)\ncurve(f_h(x, 1), add = TRUE, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-30-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n## $h = 1$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-31-1.pdf)\n:::\n:::\n\n\n## $h = 0.25$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-32-1.pdf)\n:::\n:::\n\n\n## $h = 0.01$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-33-1.pdf)\n:::\n:::\n\n\n## $h = 0.025$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-34-1.pdf)\n:::\n:::\n\n\n## $h = 0.01$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-35-1.pdf)\n:::\n:::\n\n\n## $h \\to 0$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-36-1.pdf)\n:::\n:::\n\n\n## Log-Likelihood\n\nIf $x_i \\neq x_j$ when $i \\neq j$\n\n$$\n\\begin{aligned}\n  \\ell(f*h) & = \\sum*{i} \\log\\left(1 + \\sum\\_{j \\neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \\right) - n \\log(nh\\sqrt{2 \\pi}) \\\\\n            & \\sim - n \\log(nh\\sqrt{2 \\pi})\n\\end{aligned}\n$$\n\n## for $h \\to 0$.\n\nHence, $\\ell(f_h) \\to \\infty$ for $h \\to 0$ and there is **no MLE** in the set\nof distributions with densities.\n\n## ISE, MISE, and MSE\n\nQuality of $\\hat{f}_h$ can be quantified by the _integrated squared error_,\n$$\\operatorname{ISE}(\\hat{f}_h) = \\int (\\hat{f}_h(x) - f_0(x))^2 \\ dx = ||\\hat{f}_h - f_0||_2^2.$$\n\n. . .\n\nQuality of the estimation procedure can be quantified by the mean ISE,\n$$\\operatorname{MISE}(h) = \\operatorname{E}(\\mathrm{ISE}(\\hat{f}_h)),$$ where\nthe expectation integral is over the data.\n\n. . .\n\n$$\\operatorname{MISE}(h) = \\int \\operatorname{MSE}_h(x) \\ dx$$ where\n$\\operatorname{MSE}_h(x) = \\operatorname{var}(\\hat{f}_h(x)) + \\mathrm{bias}(\\hat{f}_h(x))^2$.\n\n. . .\n\nLet's derive the case of the uniform kernel!\n\n## AMISE\n\nIf $K$ is a square-integrable probability density with mean 0,\n$$\\mathrm{MISE}(h) = \\mathrm{AMISE}(h) + o((nh)^{-1} + h^4)$$ where the\n_asymptotic mean integrated squared error_ is\n$$\\mathrm{AMISE}(h) = \\frac{\\|K\\|_2^2}{nh} + \\frac{h^4 \\sigma^4_K \\|f_0''\\|_2^2}{4}$$\nwith $\\sigma_K^2 = \\int t^2 K(t) \\ dt.$\n\n. . .\n\nUsing various plug-in estimates of $\\|f_0''\\|_2^2$, AMISE can be used to\nestimate the _asymptotically optimal bandwidth_ in a mean integrated squared\nerror sense.\n\n## Amino Acid Angles (Silverman, the default)\n\n$$h = 0.9 \\min \\left(\\hat{\\sigma},\\frac{\\text{IQR}}{1.34}\\right) n^{-1/5}$$\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$phi)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-37-1.pdf)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$psi)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-38-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Amino Acid Angles (Scott, using 1.06)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$phi, bw = \"nrd\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-39-1.pdf)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$psi, bw = \"nrd\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-40-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Amino Acid Angles (Sheather & Jones)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$phi, bw = \"SJ\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-41-1.pdf)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity(phipsi$psi, bw = \"SJ\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture2_files/figure-beamer/unnamed-chunk-42-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Exercises\n\nTake the `kern_dens()` function we started out with and implement Silverman's\nrule of thumb for choosing the bandwidth. Test against the default behavior of\n`density()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_dens_silverman <- function(x, h = NULL, m = 512) {\n  # If h is NULL, compute h using Silverman's rule #<<\n\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m)\n\n  for (i in seq_along(xx)) {\n    for (j in seq_along(x)) {\n      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))\n    }\n  }\n\n  y <- y / (sqrt(2 * pi) * h * length(x))\n\n  list(x = xx, y = y)\n}\n```\n:::\n\n\n\n\n## Exercises\n\n### Multiple Kernels\n\nLet the user choose between the Gaussian and the uniform kernel.\n\n. . .\n\n### Object\n\nReturn an S3 object from your density function and write a plot method for it,\nusing either ggplot2 or base graphics.\n\n. . .\n\n### Generic\n\nWrite a new generic called `my_density()` and provide different methods\ndepending on whether you provide a vector or a matrix, storing density estimates\nfor each column if it is a matrix.\n\nModify the `plot()` method to handle the new object.\n",
    "supporting": [
      "lecture2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}