{
  "hash": "5d4c34fbd0d898896d44d85bba9eabf2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## What is Computational Statistics?\n\nIt is a broad field, where meaning depends on context.\n\n. . .\n\nOne definition is that it is **the use of computational methods to solve\nstatistical problems**, for instance\n\n. . .\n\n- simulation,\n- optimization,\n- numerical integration,\n- data analysis, and\n- visualization.\n\n## A Running Example\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nLet's try to get a bit of flavor of what we will be doing in the course.\n\n\\bigskip\n\n. . .\n\nThroughout the course we will use a data set of amino acid angles, $\\Phi$ and\n$\\Psi$, from protein structures.\n\n:::\n\n::: {.column width=\"47%\"}\n\n![Amino Acid Angles](../images/PhiPsi_creative.jpg){width=100%}\n\n:::\n\n::::\n\n## Histograms\n\nA simple way to analyze the distributions of the angles $\\Phi$ and $\\Psi$ is the\n**histogram**.\n\n\\bigskip\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(phipsi, aes(x = phi)) +\n  geom_histogram() +\n  geom_rug(alpha = 0.5) +\n  labs(\n    x = expression(Phi),\n    y = \"Density\"\n  )\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/hist1-show-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Density Estimation\n\nHistograms are not very smooth. If we allow ourselves to make stronger\nassumptions, we can get a smoother estimate of the distribution, using **kernel\ndensity estimation** (KDE).\n\n\\bigskip\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(phipsi, aes(x = phi)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) +\n  labs(\n    x = expression(Phi),\n    y = \"Density\"\n  )\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n::::\n\n. . .\n\nBut how is this KDE actually computed? Doing this efficiently is a\n**computational statistics** problem.\n\n## Statistical Topics of the Course\n\nThe course can be broken down into a number of **statistical** and **computational**\ntopics.\n\n. . .\n\nThere are three statistical topics in the course:\n\n\\pause \n\n### Smoothing\n\nWe will learn how to compute efficient kernel density estimates and\nscatterplot smoothers.\n\n. . .\n\n### Simulation\n\nWe will learn to efficiently simulate from probability distributions \nusing inversion, rejection, and importance sampling.\n\n. . .\n\n### Optimization\n\nWe will learn to solve optimization problems that arise in statistics, for\ninstance in maximum likelihood estimation (MLE), using the EM algorithm and\ngradient-based optimization.\n\n## Computational Topics of the Course\n\n### Implementation\n\nWe will learn how to implement statistical methods in R, using\nobject-oriented programming and functional programming.\n\n. . .\n\n### Correctness\n\nWe will learn how to ensure that our code is correct, using\ntesting and debugging.\n\n. . .\n\n### Efficiency\n\nWe will learn how measure performance and find bottlenecks in our code using\nprofiling and benchmarking, and how to optimize it.\n\n## Teaching Staff\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Instructor\n\nJohan Larsson, postdoctoral researcher\n\n![Johan](../images/johan.jpg){width=50%}\n\n#### Contact\n\nUse Absalon for course-related questions and email (see Absalon) for personal\nmatters.\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Teaching Assistant\n\nJinyang Liu, PhD student in machine learning\n\n![Jin](../images/jinyang.jpg){width=50%}\n\n:::\n\n::::\n\n## Assignments\n\nFour assignments make up the bulk of the course work.\n\n. . .\n\n\\bigskip\n\nFor each assignment, there are two alternatives (A and B). You will pick one.\n\n. . .\n\n\\bigskip\n\nEach assignment is tied to a particular **topic**:\n\n1. Smoothing\n2. Univariate simulation\n3. The EM algorithm\n4. Stochastic optimization\n\n## Presentations\n\nThere will be four presentation sessions (week 3, 4, 6, 7)^[Not counting the\npotato harvesting week when you are off.]\n\n. . .\n\n\\bigskip\n\nYou will divide into groups of 2-3 students and present \nyour solution to one of the assignments during one of the sessions.\n\n. . .\n\n\\bigskip\n\nYou will register for groups and assignments in [Absalon](https://absalon.ku/dk).\n\n. . .\n\n\\bigskip\n\nPresentation is compulsory but not graded. We expect solutions to be\nwork in progress.\n\n\n. . .\n\n## Oral Examination\n\nThe main examination is an oral exam based on your assignments.\n\n\\bigskip \n\n. . .\n\nYou will prepare four presentations, one for each assignment you picked.\n\n\\bigskip\n\n. . .\n\nAt the exam, you will present one of these at random.\n\n## Schedule\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Lectures\n\n- Tuesdays and Thursdays, 10:15–12:00 (Johan)\n\n. . .\n\n### Exercise Sessions\n\n- Thursdays, 08:15–10:00 (Jinyang)\n\n. . .\n\n### Presentations\n\n- Thursdays, 13:15–15:00 (Johan)\n- Only weeks 3, 4, 6, and 7\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Examination\n\n- November 6-8 (8.15-17.30, tentative)\n- Rooms to be announced\n\n:::\n\n::::\n\n## Course Literature\n\n### Computational Statistics with R\n\nMain textbook for the course, written by Niels Richard Hansen.\n\n- Available online at <https://cswr.nrhstat.org/>\n- Not yet complete, but we only use parts that are.\n- [Companion package](https://github.com/nielsrhansen/CSwR/tree/master/CSwR_package):\n  install with `pak::pak(\"github::nielsrhansen/CSwR/CSwR_package\")`.\n\n. . .\n\n### Advanced R\n\nAuxiliary textbook, written by Hadley Wickham.\n\n- Available online at <https://adv-r.hadley.nz/>\n- Covers more advanced R programming topics.\n- We will use selected chapters.\n\n## Online Resources\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n### Absalon\n\nMain source for information and communication about the course.\nAccessed at [absalon.ku.dk](https://absalon.ku.dk/).\n\n. . .\n\n### CompStat Web Page\n\nCourse content will be uploaded to [github.io/math-ku/compstat](https://math-ku.github.io/compstat/).\n\n\\medskip\n\nThis is where you will a detailed schedule of the course, slides, and\nthe assignments.\n\n:::\n\n::: {.column width=\"25%\"}\n\n![Absalon](../images/absalon.jpg){width=100%}\n\n:::\n\n::::\n\n\n## Generative AI\n\nGenerative AI (e.g. ChatGPT, Copilot, Bard, etc.) are powerful tools.\n\n\\bigskip\n\n. . .\n\nYou are allowed to use them in this course, but with some caveats:\n\n- You must understand the results.\n- You must acknowledge their use in your assignments and how you used them.\n\n. . .\n\n\\bigskip\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Copilot\nAccess to GitHub Copilot is available for free to students, via \n[GitHub Education](https://education.github.com).\n\n:::\n\n::: {.column width=\"47%\"}\n\n![](../assets/images/github-education.png){width=100%}\n\n:::\n\n::::\n\n\n# Programming in R\n\n## Prerequisite R Knowledge\n\nWe expect knowledge of\n\n- data structures (vectors, lists, data frames),\n- control structures (loops, if-then-else),\n- function calling,\n- interactive and script usage (`source`) of R.\n\n. . .\n\nAll of this is covered in chapters 1-5 of\n[Advanced R](https://adv-r.hadley.nz/).\n\n\\medskip\n\nBut you **do not** need to be an experienced programmer.\n\n# Functions\n\n## Functions in R\n\n- Everything that happens in R is the result of a function call. Even `+`, `[`\n  and `<-` are functions.\n- An R function takes a number of _arguments_, and when a function call is\n  evaluated it computes a _return value_.\n- Functions can return any R object, including functions!\n- Implementations of R functions are collected into source files, which can be\n  organized into packages.\n- The order of your functions in the script does not matter.\n\n## Components of a Function\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Arguments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, y) {\n  x + y\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformals(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n\n\n$y\n```\n\n\n:::\n:::\n\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Body\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbody(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n    x + y\n}\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Environment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenvironment(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<environment: R_GlobalEnv>\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## Naming Functions\n\n> There are only two hard things in Computer Science: cache invalidation and\n> naming things.\n>\n> \\medskip\n> \n> \\hfill_--Phil Karlton_\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Favor Descriptive Names\n\nBegin to see if you can use a _verb_. Better long and descriptive than short and\ncryptic.\n\n. . .\n\n### Honor Common Conventions\n\nAvoid `.` in names; it is used for **methods** (upcoming).\n\n. . .\n\n:::\n\n::: {.column width=\"47%\"}\n\n### Use a Consistent Style\n\n- `lowercase`\n- `snake_case` (tidyverse)\n- `camelCase`\n- `UpperCamelCase`\n\n. . .\n\n### Namespace Clashes\n\n- Avoid names of existing functions.\n\n:::\n\n::::\n\n## Example: Counting Zeros\n\nCount data is often modeled using a Poisson distribution. R can simulate count\ndata using the function `rpois()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpois(10, 2) # n = 10 variables from a Poisson(2) distribution\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 2 2 2 4 2 0 1 2 2\n```\n\n\n:::\n:::\n\n\n. . .\n\nThere are two zeros in this sequence.\n\n. . .\n\nLet's write a function that counts the number of zeros: checks for zero\ninflation.\n\n## A First Attempt\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros <- function(x) {\n  n_zeros <- 0\n  for (i in 1:length(x)) {\n    if (x[i] == 0) {\n      n_zeros <- n_zeros + 1\n    }\n  }\n  n_zeros\n}\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros(c(3, 2, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros(c(0, 0, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n## Testing\n\nIt is critical to ensure that your code does what you think it does,\nand that it continues to do so as you modify it.\n\n. . .\n\n\\bigskip\n\nTesting is the process of writing code that checks that your code is\ncorrect.\n\n. . .\n\n\\bigskip\n\nSome people even think that the **first thing** you should do is\nto write a test.\n\n. . .\n\n\\bigskip\n\nAs you modify your code, your tests will catch these **regressions** for you.\n\n## testthat\n\n[testthat](https://testthat.r-lib.org/) is the most popular testing framework for R.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# In file tests/test_count_zeros.R\ntest_that(\"count_zeros work on various input\", {\n  expect_equal(count_zeros(c(0, 0, 1e-9, 25)), 2)\n  expect_equal(count_zeros(c(-0, 1.1, -2)), 1)\n  expect_equal(count_zeros(c()), 0)\n})\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestthat::test_dir(\"tests\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv | F W  S  OK | Context\n\n/ |          0 | count_zeros                      \nx | 1        2 | count_zeros\n--------------------------------------------------\nError ('test_count_zeros.R:3:1'): (code run outside of `test_that()`)\nError in `if (x[i] == 0) {\n    n_zeros <- n_zeros + 1\n}`: argument is of length zero\nBacktrace:\n    x\n 1. +-testthat::expect_equal(count_zeros(c()), 0) at test_count_zeros.R:3:1\n 2. | \\-testthat::quasi_label(enquo(object), label, arg = \"object\")\n 3. |   \\-rlang::eval_bare(expr, quo_get_env(quo))\n 4. \\-global count_zeros(c())\n--------------------------------------------------\n\n== Results =======================================\n-- Failed tests ----------------------------------\nError ('test_count_zeros.R:3:1'): (code run outside of `test_that()`)\nError in `if (x[i] == 0) {\n    n_zeros <- n_zeros + 1\n}`: argument is of length zero\nBacktrace:\n    x\n 1. +-testthat::expect_equal(count_zeros(c()), 0) at test_count_zeros.R:3:1\n 2. | \\-testthat::quasi_label(enquo(object), label, arg = \"object\")\n 3. |   \\-rlang::eval_bare(expr, quo_get_env(quo))\n 4. \\-global count_zeros(c())\n\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 2 ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError: Test failures\n```\n\n\n:::\n:::\n\n\n## A Second Attempt\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros <- function(x) {\n  n_zeros <- 0\n  for (i in seq_along(x)) {\n    if (x[i] == 0) {\n      n_zeros <- n_zeros + 1\n    }\n  }\n  n_zeros\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestthat::test_dir(\"tests\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv | F W  S  OK | Context\n\n/ |          0 | count_zeros                      \nv |          3 | count_zeros\n\n== Results =======================================\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n```\n\n\n:::\n:::\n\n\n## Debugging\n\n- Sometimes hard to identify the offending piece of code.\n- Helpful to use a debugging tool. R studio comes with a helpful interface for\n  this.\n- We will talk more about debugging in week 5.\n\n## Functional Programming\n\nIn functional programming, functions are **first-class citizens**: they can be passed as\narguments to other functions, returned as values from functions, and assigned to\nvariables.\n\n\\bigskip\n\n. . .\n\nThis allows for a high degree of abstraction and code reuse, for instance\nthrough the use of the `apply` family of functions.\n\n. . .\n\nLet's write our own apply function.\n\n\\bigskip\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply <- function(x, fun) {\n  val <- numeric(length(x))\n  for (i in seq_along(x)) {\n    val[i] <- fun(x[[i]])\n  }\n  val\n}\n```\n:::\n\n\n## Testing Our Apply Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(1:10, exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]     2.718282     7.389056    20.085537\n [4]    54.598150   148.413159   403.428793\n [7]  1096.633158  2980.957987  8103.083928\n[10] 22026.465795\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]     2.718282     7.389056    20.085537\n [4]    54.598150   148.413159   403.428793\n [7]  1096.633158  2980.957987  8103.083928\n[10] 22026.465795\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Assumptions\n\n`x` is a list, `fun()` takes a single argument, and `fun()` returns a numeric.\n\n. . .\n\n## What if `fun()` Needs Additional Arguments?\n\nThen we get an error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, rpois)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in fun(x[[i]]): argument \"lambda\" is missing, with no default\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Anonymous Functions\n\nWe can use an anonymous function to pass additional arguments to `fun()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(\n  1:10,\n  function(lambda) rpois(1, lambda = 0.9)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 1 2 0 2 3 1 1 0\n```\n\n\n:::\n:::\n\n\n## `...`\n\nMore general functionality can be achieved wit the  `...` (ellipsis) argument,\nwhich passes arguments forward.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply <- function(x, fun, ...) { # <1>\n  val <- numeric(length(x))\n  for (i in seq_along(x)) {\n    val[i] <- fun(x[[i]], ...) # <2>\n  }\n  val\n}\n```\n:::\n\n\n1. `...` in the argument list of `our_apply()` collects additional arguments.\n2. `...` in the call to `fun()` passes these additional arguments to `fun()`.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, rpois, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0  1  4  3  7  6  8 16  8 12\n```\n\n\n:::\n:::\n\n\n# Benchmarking\n\n## R Is Slow ...\n\n... when used like a low-level language.\n\n- R is an **interpreted** (as opposed to _compiled_) language.\n- It was written mainly for specifying statistical models (not for developing\n  new numerical methods).\n- It is suitable for high-level programming where most low-level computations\n  are implemented in a compiled language (e.g. `lm()` and `qr()`.)\n- It is also quite old.\n\n## R Is Fast ...\n\n... when most computations are carried out by calls to compiled code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1e4)\nbench_res <- bench::mark(\n  loop = {\n    y <- numeric(length(x))\n    for (i in seq_along(x)) {\n      y[i] <- 10 * x[i]\n    }\n    y\n  },\n  vectorized = 10 * x\n)\n```\n:::\n\n\n## Plot Benchmark Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bench_res)\n```\n\n::: {.cell-output-display}\n![Benchmark results for a loop vs. a vectorized computation](lecture1_files/figure-beamer/bench-plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Vectorization\n\nVectorization is the process of rewriting code to use vectorized operations,\nwhich operate on entire vectors at once, instead of using loops to operate on\nindividual elements.\n\n. . .\n\nThe term is somewhat misleading, since it does not necessarily involve\nvector processors.\n\n. . .\n\n### Example: Counting Zeros, Vectorized\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_zeros_vec <- function(x) {\n  sum(x == 0)\n}\n```\n:::\n\n\n. . .\n\n- `x == 0` checks if each entry of `x` is 0 and returns a vector of logicals.\n- `sum()` computes and returns the sum of all elements in a vector. Logicals are\n  coerced to integers.\n- In this case the vectorized implementation is cohesive and clear.\n- The vectorized computations are performed by compiled code (C/C++/Fortran),\n  which run faster than pure R code.\n- Writing vectorized code requires a larger knowledge of R functions.\n\n## Beware of Loops in Disguise\n\nJust because you ran a function, it does not mean that it is vectorized.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  sapply(x, function(x_i) 10 * x_i),\n  10 * x\n) |>\n  plot()\n```\n\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Development Cycle Sketch\n\n- Is there a good-enough existing implemention for your problem? If yes, then\n  you are done.\n- If not, implement a solution and test it. Does it solve your problem\n  sufficiently well? If yes, then you're done.\n- If not, then profile (next week!), benchmark, and debug (week 5). Then\n  refactor and optimize.\n\n. . .\n\n### The Root of All Evil\n\n\\medskip\n\n> We _should_ forget about small efficiencies, say about 97% of the time:\n> premature optimization is the root of all evil. Yet we should not pass up our\n> opportunities in that critical 3%.\n>\n> _—Donald Knuth_\n\n## Example: Density Estimation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_dens <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m)\n\n  for (i in seq_along(xx)) {\n    for (j in seq_along(x)) {\n      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))\n    }\n  }\n\n  y <- y / (sqrt(2 * pi) * h * length(x))\n\n  list(x = xx, y = y)\n}\n```\n:::\n\n\n## Vectorizing Our Density Estimator\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_dens_vec <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m)\n  const <- (sqrt(2 * pi) * h * length(x))\n\n  for (i in seq_along(xx)) {\n    y[i] <- sum(exp(-(xx[i] - x)^2 / (2 * h^2))) / const\n  }\n\n  list(x = xx, y = y)\n}\n```\n:::\n\n\n## Benchmarking\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_bench <- bench::mark(\n  kern_dens(phipsi$psi, 0.2),\n  kern_dens_vec(phipsi$psi, 0.2)\n)\n```\n:::\n\n\n## Plot Benchmark Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(kern_bench)\n```\n\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/kern-bench-autoplot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Parameterized Benchmarking\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkern_benchmarks <- bench::press(\n  n = 2^(6:9),\n  m = 2^(5:11),\n  {\n    bench::mark(\n      loop = kern_dens(x[1:n], h = 0.2, m = m),\n      vec = kern_dens_vec(x[1:n], h = 0.2, m = m)\n    )\n  }\n)\n```\n:::\n\n\n## Plotting Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmutate(kern_benchmarks, expression = as.character(expression)) |>\n  ggplot(aes(m, median, color = expression)) +\n  geom_point() +\n  geom_line() +\n  facet_grid(cols = vars(n))\n```\n\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/kern-bench-fig-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Getting Help with R\n\n### Google It\n\nEspecially good for error messages.\n\n. . .\n\n### Generative AI\n\n- Also great for error messages and debugging\n- _Caution_: You need to understand the results, especially when you ask it to\n  create something for you.\n\n. . .\n\n### Absalon Discussion Forum\n\nUse the fact that there are twenty other people in the course with exactly the\nsame problem.\n\n# Exercises\n\n### Exercise 1\n\nCan you list three ways to access element `a` in this list?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl <- list(a = 1, b = 2)\n```\n:::\n\n\n\n\n. . .\n\n### Exercise 2\n\nWrite a for loop that prints \"even\" if the loop variable is even, \"odd\" if the\nloop variable is odd, and exits if is larger than 10.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}