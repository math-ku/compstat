{
  "hash": "53a5c910da839be06cb80ce58d302159",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## What is Computational Statistics?\n\nIt is a broad field, where meaning depends on context.\n\n\\bigskip\n\n. . .\n\nOne definition is that it is **the use of computational methods to solve\nstatistical problems**, for instance\n\n. . .\n\n- simulation,\n- optimization,\n- numerical integration,\n- data analysis, and\n- visualization.\n\n## A Running Example\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nLet's try to get a bit of flavor of what we will be doing in the course.\n\n\\bigskip\n\n. . .\n\nThroughout the course we will use a data set of amino acid angles, $\\Phi$ and\n$\\Psi$, from protein structures.\n\n:::\n\n::: {.column width=\"47%\"}\n\n![Amino Acid Angles](../images/PhiPsi_creative.jpg){width=90%}\n\n:::\n\n::::\n\n## Histograms\n\nA simple way to analyze the distributions of the angles $\\Phi$ and $\\Psi$ is the\n**histogram**.\n\n\\bigskip\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(phipsi, aes(x = phi)) +\n  geom_histogram() +\n  geom_rug(alpha = 0.5) +\n  labs(\n    x = expression(Phi),\n    y = \"Density\"\n  )\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/hist1-show-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n## Density Estimation\n\nHistograms are not very smooth. If we allow ourselves to make stronger\nassumptions, we can get a smoother estimate of the distribution, using **kernel\ndensity estimation** (KDE).\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(phipsi, aes(x = phi)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) +\n  labs(\n    x = expression(Phi),\n    y = \"Density\"\n  )\n```\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture1_files/figure-beamer/dens-fig-1.pdf)\n:::\n:::\n\n\n:::\n\n::::\n\n. . .\n\n\\bigskip\n\nBut how is this KDE actually computed? Doing this efficiently is a\n**computational statistics** problem.\n\n## Statistical Topics of the Course\n\nThe course can be broken down into a number of **statistical** and **computational**\ntopics.\n\n\\bigskip\n\n. . .\n\nThere are three statistical topics in the course:\n\n\\pause \n\n### Smoothing\n\nWe will learn how to compute efficient kernel density estimates and\nscatterplot smoothers.\n\n. . .\n\n### Simulation\n\nWe will learn to efficiently simulate from probability distributions \nusing inversion, rejection, and importance sampling.\n\n. . .\n\n### Optimization\n\nWe will learn to solve optimization problems that arise in statistics, for\ninstance in maximum likelihood estimation (MLE), using the EM algorithm and\ngradient-based optimization.\n\n## Computational Topics of the Course\n\n### Implementation\n\nWe will learn how to implement statistical methods in R, using\nobject-oriented programming and functional programming.\n\n. . .\n\n### Correctness\n\nWe will learn how to ensure that our code is correct, using\ntesting and debugging.\n\n. . .\n\n### Efficiency\n\nWe will learn how measure performance and find bottlenecks in our code using\nprofiling and benchmarking, and how to optimize it.\n\n## Teaching Staff\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Instructor\n\nJohan Larsson, postdoctoral researcher\n\n![Johan](../images/johan.jpg){width=50%}\n\n#### Contact\n\nUse Absalon for course-related questions and email (see Absalon) for personal\nmatters.\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Teaching Assistant\n\nJinyang Liu, PhD student in machine learning\n\n![Jin](../images/jinyang.jpg){width=50%}\n\n:::\n\n::::\n\n## Assignments\n\nFour assignments make up the bulk of the course work.\n\n. . .\n\n\\bigskip\n\nFor each assignment, there are two alternatives (A and B). You will pick one.\n\n. . .\n\n\\bigskip\n\nEach assignment is tied to a particular **topic**:\n\n1. Smoothing\n2. Univariate simulation\n3. The EM algorithm\n4. Stochastic optimization\n\n## Presentations\n\nThere will be four presentation sessions (week 3, 4, 6, 7)^[Not counting the\npotato harvesting week when you are off.]\n\n. . .\n\n\\bigskip\n\nYou will divide into groups of 2-3 students and present \nyour solution to one of the assignments during one of the sessions.\n\n. . .\n\n\\bigskip\n\nYou will register for groups and assignments in [Absalon](https://absalon.ku/dk).\n\n. . .\n\n\\bigskip\n\nPresentation is compulsory but not graded. We expect solutions to be\nwork in progress.\n\n\n. . .\n\n## Oral Examination\n\nThe main examination is an oral exam based on your assignments.\n\n\\bigskip \n\n. . .\n\nYou will prepare four presentations, one for each assignment you picked.\n\n\\bigskip\n\n. . .\n\nAt the exam, you will present one of these at random.\n\n## Schedule\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Lectures\n\n- Tuesdays and Thursdays, 10:15–12:00 (Johan)\n\n. . .\n\n### Exercise Sessions\n\n- Thursdays, 08:15–10:00 (Jinyang)\n\n. . .\n\n### Presentations\n\n- Thursdays, 13:15–15:00 (Johan)\n- Only weeks 3, 4, 6, and 7\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Examination\n\n- November 6-8 (8.15-17.30, tentative)\n- Rooms to be announced\n\n:::\n\n::::\n\n## Course Literature\n\n### Computational Statistics with R\n\nMain textbook for the course, written by Niels Richard Hansen.\n\n- Available online at <https://cswr.nrhstat.org/>\n- Not yet complete, but we only use parts that are.\n- [Companion package](https://github.com/nielsrhansen/CSwR/tree/master/CSwR_package):\n  install with `pak::pak(\"github::nielsrhansen/CSwR/CSwR_package\")`.\n\n. . .\n\n### Advanced R\n\nAuxiliary textbook, written by Hadley Wickham.\n\n- Available online at <https://adv-r.hadley.nz/>\n- Covers more advanced R programming topics.\n- We will use selected chapters.\n\n## Online Resources\n\n:::: {.columns}\n\n::: {.column width=\"70%\"}\n\n### Absalon\n\nMain source for information and communication about the course.\nAccessed at [absalon.ku.dk](https://absalon.ku.dk/).\n\n. . .\n\n### CompStat Web Page\n\nCourse content will be uploaded to [github.io/math-ku/compstat](https://math-ku.github.io/compstat/).\n\n\\medskip\n\nThis is where you will a detailed schedule of the course, slides, and\nthe assignments.\n\n:::\n\n::: {.column width=\"25%\"}\n\n![Absalon](../images/absalon.jpg){width=100%}\n\n:::\n\n::::\n\n\n## Generative AI\n\nGenerative AI (e.g. ChatGPT, Copilot, Bard, etc.) are powerful tools.\n\n\\bigskip\n\n. . .\n\nYou are allowed to use them in this course, but with some caveats:\n\n- You must understand the results.\n- You must acknowledge their use in your assignments and how you used them.\n\n. . .\n\n\\bigskip\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Copilot\nAccess to GitHub Copilot is available for free to students, via \n[GitHub Education](https://education.github.com).\n\n:::\n\n::: {.column width=\"47%\"}\n\n![](../assets/images/github-education.png){width=100%}\n\n:::\n\n::::\n\n\n# Programming in R\n\n## What is R?\n\nR is a programming language and application (command-line interface) for\nstatistical computing and graphics.\n\n. . .\n\n\\bigskip \n\nIt is widely used among statisticians and data miners for developing statistical\nsoftware and data analysis.\n\n. . .\n\n### Why R?\n\nIt is free, open source, and cross-platform.\n\n. . .\n\n### Why not Python/Julia?\n\nR has a large number of packages for statistical computing and graphics.\n\n. . .\n\n\\bigskip\n\nMy personal opinion is that:\n\n- R is better suited for visualization and exploratory data analysis, while \n- Python is better suited for general-purpose\n  programming and machine learning, and\n- Julia is a best for numerical computing.\n\n## Prerequisite R Knowledge\n\nWe expect knowledge of\n\n- data structures (vectors, lists, data frames),\n- control structures (loops, if-then-else),\n- function calling,\n- interactive and script usage (`source`) of R.\n\n. . .\n\nAll of this is covered in chapters 1-5 of\n[Advanced R](https://adv-r.hadley.nz/).\n\n. . .\n\n\\bigskip\n\nWe do not expect that you are an expert in R.\n\n## Getting Help with R\n\n### Google It\n\nEspecially good for error messages.\n\n. . .\n\n### Generative AI\n\n- Also great for error messages and debugging\n- _Caution_: You need to understand the results, especially when you ask it to\n  create something for you.\n\n. . .\n\n### Absalon Discussion Forum\n\nUse the fact that there are twenty other people in the course with exactly the\nsame problem.\n\n# Functions\n\n## Functions in R\n\nEverything that happens in R is the result of a function call. Even `+`, `[`\nand `<-` are functions.\n\n. . .\n\n\\medskip\n\nAn R function takes a number of _arguments_, and when a function call is\nevaluated it computes a _return value_.\n\n. . .\n\n\\medskip\n\nFunctions can return any R object, including functions!\n\n. . .\n\n\\medskip\n\nImplementations of R functions are collected into source files, which can be\norganized into packages.\n\n## Why Functions?\n\nTechnically, you could write all your code in a single script. So why use\nfunctions?\n\n. . .\n\n\\bigskip\n\nFunctions help you structure your code, make it reusable, and make it easier to\ntest and debug.\n\n. . .\n\n\\bigskip\n\nA well-designed function has a single purpose, which makes it easier to\nunderstand and reason about.\n\n. . .\n\n### Rule of Thumb\n\nIf you find yourself writing the same piece of code more than, say,\ntwice, then it is probably a good idea to turn it into a function.\n\n## Function Syntax\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nHere is a simple function that takes two arguments and returns their sum.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, y) {\n  x + y\n}\n```\n:::\n\n\n. . .\n\nA function has three components: **arguments**, **body**, and **environment**.\n\n\\bigskip\n\n. . .\n\n### Arguments\n\nIn this case, `x` and `y` are the arguments.\n\n:::\n\n. . .\n\n::: {.column width=\"47%\"}\n\n### Body\n\nThe body of the function is everything inside the curly braces `{}`,\nthat is, `x + y`.\n\n. . .\n\n### Environment\n\nThe environment is where the function was created. It is used to look up\nvariables that are not defined inside the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenvironment(f)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<environment: R_GlobalEnv>\n```\n\n\n:::\n:::\n\n\n:::\n\n::::\n\n## Arguments\n\nArguments are specified in the parentheses after the function name.\n\n. . .\n\n### Arguments can have default values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- function(x, y = 2) {\n  x + y\n}\n\ng(3) # y takes the default value 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Named arguments can be passed in any order.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng(y = 3, x = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n:::\n\n\n## Copy-on-Write\n\n\nR uses **copy-on-write** semantics for function arguments, which means that\narguments are only copied if they are modified inside the function.\n\n. . .\n\n\\bigskip\n\nThis makes function calls efficient, and also means that you can modify\narguments without shooting yourself in the foot.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh <- function(x) {\n  x[1] <- 100\n  x\n}\n\na <- 1:5\n\nh(a) # a is not modified\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100   2   3   4   5\n```\n\n\n:::\n:::\n\n\n. . .\n\n:::\n\n::: {.column width=\"47%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\na # a is still 1:5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3 4 5\n```\n\n\n:::\n:::\n\n\n. . .\n\n### `<<-` Operator\n\nYou can use the `<<-` operator to modify variables in the parent environment,\nbut please don't.\n\n:::\n\n::::\n\n\n## Environment and Scoping\n\nWhen a function is called, a new environment is created for the function.\n\n\\medskip\n\n. . .\n\nThis environment is used to look up variables that are not defined inside the\nfunction.\n\n\\medskip\n\n. . .\n\nThe new environment has as its parent the environment where the function was\ncreated.\n\n\\medskip\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\nThis is called **lexical scoping**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 10\nf <- function(y) {\n  x + y\n}\nf(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"47%\"}\n\nAssuming that `x` is not defined inside `f()`, R looks for `x` in the\nenvironment where `f()` was created, which is the global environment in this case.\n\n. . .\n\nAs a rule of thumb, avoid using variables from the parent environment inside\nfunctions.\n\n:::\n\n::::\n\n\n## Return Values\n\nThe return value of a function is the value of the last expression in the body.\n\n. . .\n\n\\bigskip\n\nUnless you explicitly use the `return()` function, which immediately exits the\nfunction and returns the specified value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, y) {\n  return(x + y)\n  x * y # This line is never reached\n}\n```\n:::\n\n\n. . .\n\nWhether or not to use `return()` is a matter of style. Personally, I prefer to\nnot use it unless returning early.\n\n## Functional Programming\n\nFunctions are **first-class citizens**: they can be passed as\narguments to other functions, returned as values from functions, and assigned to\nvariables. R is a **functional programming** language.\n\n\\bigskip\n\n. . .\n\nThis allows for a high degree of abstraction and code reuse, for instance\nthrough the use of the `apply` family of functions.\n\n. . .\n\nLet's write our own apply function.\n\n\\bigskip\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply <- function(x, fun) {\n  val <- numeric(length(x))\n  for (i in seq_along(x)) {\n    val[i] <- fun(x[[i]])\n  }\n  val\n}\n```\n:::\n\n\n## Testing Our Apply Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(1:10, exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]     2.718282     7.389056    20.085537    54.598150   148.413159\n [6]   403.428793  1096.633158  2980.957987  8103.083928 22026.465795\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]     2.718282     7.389056    20.085537    54.598150   148.413159\n [6]   403.428793  1096.633158  2980.957987  8103.083928 22026.465795\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Assumptions of `our_apply()`\n\n`x` is a \"list-like\" structure, `fun` takes a single argument, and `fun` returns a numeric.\n\n## What if `fun` Needs Additional Arguments?\n\nThen we get an error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, rpois)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in fun(x[[i]]): argument \"lambda\" is missing, with no default\n```\n\n\n:::\n:::\n\n\n. . .\n\n### Anonymous Functions\n\nWe can use an anonymous function to pass additional arguments to `fun()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(\n  1:10,\n  function(lambda) rpois(1, lambda = 0.9)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 1 2 0 2 3 1 1 0\n```\n\n\n:::\n:::\n\n\n## Ellipsis (`...`)\n\nAn arbitrary number of arguments can be forwarded via the  `...` (ellipsis) argument,\nwhich allows us to pass additional arguments to `fun()`.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply <- function(x, fun, ...) { # <1>\n  val <- numeric(length(x))\n  for (i in seq_along(x)) {\n    val[i] <- fun(x[[i]], ...) # <2>\n  }\n  val\n}\n```\n:::\n\n\n1. `...` in the argument list of `our_apply()` collects additional arguments.\n2. `...` in the call to `fun()` passes these additional arguments to `fun()`.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_apply(1:10, rpois, n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  0  1  4  3  7  6  8 16  8 12\n```\n\n\n:::\n:::\n\n\n## Assertions\n\nIt is a good idea to assert that the arguments to a function are valid,\nand stop early if they are not.\n\n. . .\n\n\\bigskip\n\nThe simplest way to do this is to use the `stopifnot()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_mean <- function(x) {\n  stopifnot(is.numeric(x))\n  sum(x) / length(x)\n}\n\nmy_mean(c(\"asdf\", \"qwer\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in my_mean(c(\"asdf\", \"qwer\")): is.numeric(x) is not TRUE\n```\n\n\n:::\n:::\n\n\n. . .\n\nIf `x` is not numeric, we would otherwise get an error deep inside `sum()`.\n\n. . .\n\n## Custom Error Messages\n\nFor more control over the error message, use `if` and `stop()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_mean <- function(x) {\n  if (!is.numeric(x)) {\n    stop(\"x must be numeric\")   \n  }\n  sum(x) / length(x)\n}\n```\n:::\n\n\n## Document Your Functions\n\nDocument your functions, including their arguments, return values, and any side effects.\n\n. . .\n\n\\medskip\n\nDocumentation can be either ad-hoc comments or using a documentation system like\n[roxygen2](https://roxygen2.r-lib.org/).\n\n. . .\n\n\\medskip\n\nAim to document **why** something is done, not just **what** is done. The latter\nis often obvious from the code itself.\n\n## Naming Functions\n\n> There are only two hard things in Computer Science: cache invalidation and\n> naming things.\n>\n> \\medskip\n> \n> \\hfill_--Phil Karlton_\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"47%\"}\n\n### Favor Descriptive Names\n\nBegin to see if you can use a _verb_. Better long and descriptive than short and\ncryptic.\n\n. . .\n\n### Honor Common Conventions\n\nAvoid `.` in names; it is used for **methods** (upcoming).\n\n. . .\n\n:::\n\n::: {.column width=\"47%\"}\n\n### Use a Consistent Style\n\n- `lowercase`\n- `snake_case` (tidyverse)\n- `camelCase`\n- `UpperCamelCase`\n\n. . .\n\n### Namespace Clashes\n\nAvoid names of existing functions.\n\n:::\n\n::::\n\n\n## Best Practices\n\n\n### Keep Functions Short and Focused\n\nFunctions should do one thing and do it well. If a function is too long or\ncomplex, consider breaking it up into smaller functions.\n\n. . .\n\n### Testing\n\nWrite tests for your functions to ensure that they work as expected. (More\non this later in the course.)\n\n. . .\n\n### Debugging\n\nUse `browser()`, `traceback()`, and `debug()` to debug your functions. (More on\nthis later in the course.)\n\n## Summary\n\n- Functions are the building blocks of R code.\n- Functions help you structure your code, make it reusable, and make it easier to\n  test and debug.\n- Functions have arguments, a body, and an environment.\n- Functions can be passed as arguments to other functions, returned as values\n  from functions, and assigned to variables.\n- Use `...` to pass additional arguments to functions.\n- Document your functions and use descriptive names.\n- Write tests for your functions and use debugging tools.\n\n\n# Exercises\n\n### Exercise 1: Write a Function with a Default Argument  \n\nWrite a function `greet` that takes a name as an argument and prints “Hello,\n<name>!”. If no name is given, it should print “Hello, world!”.\n\n. . .\n\n### Exercise 2: Count Missing Values  \n\nWrite a function `count_na` that takes a vector and returns the number of\nmissing (`NA`) values in it.\n\n. . .\n\n### Exercise 3: Check if a Number is Even\n\nWrite a function `is_even()` that returns `TRUE` if its argument is even, `FALSE`\notherwise.\n\n\n\n<!-- # Benchmarking -->\n<!---->\n<!-- ## R Is Slow ... -->\n<!---->\n<!-- ... when used like a low-level language. -->\n<!---->\n<!-- - R is an **interpreted** (as opposed to _compiled_) language. -->\n<!-- - It was written mainly for specifying statistical models (not for developing -->\n<!--   new numerical methods). -->\n<!-- - It is suitable for high-level programming where most low-level computations -->\n<!--   are implemented in a compiled language (e.g. `lm()` and `qr()`.) -->\n<!-- - It is also quite old. -->\n<!---->\n<!-- ## R Is Fast ... -->\n<!---->\n<!-- ... when most computations are carried out by calls to compiled code. -->\n<!---->\n<!-- ```{r} -->\n<!-- x <- rnorm(1e4) -->\n<!-- bench_res <- bench::mark( -->\n<!--   loop = { -->\n<!--     y <- numeric(length(x)) -->\n<!--     for (i in seq_along(x)) { -->\n<!--       y[i] <- 10 * x[i] -->\n<!--     } -->\n<!--     y -->\n<!--   }, -->\n<!--   vectorized = 10 * x -->\n<!-- ) -->\n<!-- ``` -->\n<!---->\n<!-- ## Plot Benchmark Results -->\n<!---->\n<!-- ```{r bench-plot} -->\n<!-- #| fig-width: 5 -->\n<!-- #| fig-cap: Benchmark results for a loop vs. a vectorized computation -->\n<!-- autoplot(bench_res) -->\n<!-- ``` -->\n<!---->\n<!-- ## Vectorization -->\n<!---->\n<!-- Vectorization is the process of rewriting code to use vectorized operations, -->\n<!-- which operate on entire vectors at once, instead of using loops to operate on -->\n<!-- individual elements. -->\n<!---->\n<!-- . . . -->\n<!---->\n<!-- The term is somewhat misleading, since it does not necessarily involve -->\n<!-- vector processors. -->\n<!---->\n<!-- . . . -->\n<!---->\n<!-- ### Example: Counting Zeros, Vectorized -->\n<!---->\n<!-- ```{r} -->\n<!-- count_zeros_vec <- function(x) { -->\n<!--   sum(x == 0) -->\n<!-- } -->\n<!-- ``` -->\n<!---->\n<!-- . . . -->\n<!---->\n<!-- - `x == 0` checks if each entry of `x` is 0 and returns a vector of logicals. -->\n<!-- - `sum()` computes and returns the sum of all elements in a vector. Logicals are -->\n<!--   coerced to integers. -->\n<!-- - In this case the vectorized implementation is cohesive and clear. -->\n<!-- - The vectorized computations are performed by compiled code (C/C++/Fortran), -->\n<!--   which run faster than pure R code. -->\n<!-- - Writing vectorized code requires a larger knowledge of R functions. -->\n<!---->\n<!-- ## Beware of Loops in Disguise -->\n<!---->\n<!-- Just because you ran a function, it does not mean that it is vectorized. -->\n<!---->\n<!-- . . . -->\n<!---->\n<!-- ```{r} -->\n<!-- #| fig-width: 5 -->\n<!-- #| fig-height: 2 -->\n<!-- bench::mark( -->\n<!--   sapply(x, function(x_i) 10 * x_i), -->\n<!--   10 * x -->\n<!-- ) |> -->\n<!--   plot() -->\n<!-- ``` -->\n<!---->\n<!-- ## Development Cycle Sketch -->\n<!---->\n<!-- - Is there a good-enough existing implemention for your problem? If yes, then -->\n<!--   you are done. -->\n<!-- - If not, implement a solution and test it. Does it solve your problem -->\n<!--   sufficiently well? If yes, then you're done. -->\n<!-- - If not, then profile (next week!), benchmark, and debug (week 5). Then -->\n<!--   refactor and optimize. -->\n<!---->\n<!-- . . . -->\n<!---->\n<!-- ### The Root of All Evil -->\n<!---->\n<!-- \\medskip -->\n<!---->\n<!-- > We _should_ forget about small efficiencies, say about 97% of the time: -->\n<!-- > premature optimization is the root of all evil. Yet we should not pass up our -->\n<!-- > opportunities in that critical 3%. -->\n<!-- > -->\n<!-- > _—Donald Knuth_ -->\n<!---->\n<!-- ## Example: Density Estimation -->\n<!---->\n<!-- ```{r kernDens} -->\n<!-- kern_dens <- function(x, h, m = 512) { -->\n<!--   rg <- range(x) -->\n<!--   xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) -->\n<!--   y <- numeric(m) -->\n<!---->\n<!--   for (i in seq_along(xx)) { -->\n<!--     for (j in seq_along(x)) { -->\n<!--       y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2)) -->\n<!--     } -->\n<!--   } -->\n<!---->\n<!--   y <- y / (sqrt(2 * pi) * h * length(x)) -->\n<!---->\n<!--   list(x = xx, y = y) -->\n<!-- } -->\n<!-- ``` -->\n<!---->\n<!-- ## Vectorizing Our Density Estimator -->\n<!---->\n<!-- ```{r kernDens-vec} -->\n<!-- kern_dens_vec <- function(x, h, m = 512) { -->\n<!--   rg <- range(x) -->\n<!--   xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) -->\n<!--   y <- numeric(m) -->\n<!--   const <- (sqrt(2 * pi) * h * length(x)) -->\n<!---->\n<!--   for (i in seq_along(xx)) { -->\n<!--     y[i] <- sum(exp(-(xx[i] - x)^2 / (2 * h^2))) / const -->\n<!--   } -->\n<!---->\n<!--   list(x = xx, y = y) -->\n<!-- } -->\n<!-- ``` -->\n<!---->\n<!-- ## Benchmarking -->\n<!---->\n<!-- ```{r kern-bench} -->\n<!-- kern_bench <- bench::mark( -->\n<!--   kern_dens(phipsi$psi, 0.2), -->\n<!--   kern_dens_vec(phipsi$psi, 0.2) -->\n<!-- ) -->\n<!-- ``` -->\n\n<!-- ## Plot Benchmark Results -->\n<!---->\n<!-- ```{r kern-bench-autoplot} -->\n<!-- #| message: false -->\n<!-- #| fig-height: 2.5 -->\n<!-- #| fig-width: 5 -->\n<!-- plot(kern_bench) -->\n<!-- ``` -->\n<!---->\n<!-- ## Parameterized Benchmarking -->\n<!---->\n<!-- ```{r kern-bench-grid, cache = TRUE, message = FALSE} -->\n<!-- kern_benchmarks <- bench::press( -->\n<!--   n = 2^(6:9), -->\n<!--   m = 2^(5:11), -->\n<!--   { -->\n<!--     bench::mark( -->\n<!--       loop = kern_dens(x[1:n], h = 0.2, m = m), -->\n<!--       vec = kern_dens_vec(x[1:n], h = 0.2, m = m) -->\n<!--     ) -->\n<!--   } -->\n<!-- ) -->\n<!-- ``` -->\n\n<!-- ## Plotting Results -->\n<!---->\n<!-- ```{r kern-bench-fig} -->\n<!-- #| message: false -->\n<!-- #| warning: false -->\n<!-- #| fig-width: 7 -->\n<!-- library(tidyverse) -->\n<!-- mutate(kern_benchmarks, expression = as.character(expression)) |> -->\n<!--   ggplot(aes(m, median, color = expression)) + -->\n<!--   geom_point() + -->\n<!--   geom_line() + -->\n<!--   facet_grid(cols = vars(n)) -->\n<!-- ``` -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}