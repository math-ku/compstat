---
title: "Object-Oriented Programming and Density Estimation"
subtitle: Computational Statistics
author: "Johan Larsson, Niels Richard Hansen"
date: "September 5, 2024"
---

```{r init, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)

knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  fig.align = "center",
  cache = TRUE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

theme_set(theme_grey(base_size = 16))

load(file.path("data", "top100dih.RData"))

phipsi2 <- na.omit(dataset)
phipsi <- read.table(file.path("data", "phipsi.tsv"), header = TRUE)
phipsi[, c("phi", "psi")] <- pi * phipsi[, c("phi", "psi")] / 180
```

class: middle, center

# Object-Oriented Programming in R

---

## Many Systems

### In Base R

- S3
- S4
- Reference Classes

--

### Through Packages

- [R6](https://cran.r-project.org/package=R6)
- [S7](https://cran.r-project.org/package=S7)

### This Course

We will focus **entirely** on S3.

---

## Example: Integration

The function `integrate()` takes a function as argument and returns the 
value of numerically integrating the function.
--


It is an example of a *functional*. 
--


```{r}
integral <- integrate(sin, 0, 1)
integral
```

The numerical value of the integral 
$$\int_0^1 \sin(x) \mathrm{d}x$$ 
is printed nicely aboveâ€”including an indication of the numerical error. 

---
## Return Values

In fact, `integrate()` returns a list with a *class label*.

```{r}
str(integral)
```
--

Return values that don't fit into one of the basic data structures 
can be returned as a list as above.

---
## The Return Value of `integrate()`

The class label can be extracted directly.

```{r}
class(integral)
```
--

- The printed result of `integrate()` is not the same as the object itself. 
- What you see is the result of the **method** `print.integrate()`.

---
## Printing Objects of Class Integrate 

```{r}
stats:::print.integrate
```

(The `print.integrate()` function is not exported from the stats package. It is in the 
namespace of the stats package, and to access it directly we use `stats:::`.)

---
## Histogram Objects

```{r temp-hist, fig.width = 9, fig.height = 6}
phi_hist <- hist(phipsi$phi, main = NULL)
```

---
## Histogram Objects

```{r}
class(phi_hist)
```

--

```{r}
str(phi_hist)
```

---
## Histogram Objects

```{r}
phi_hist[1:4]
```


---
## Getting Help for Objects

You can find documentation for `plot()` using e.g. 

```{r help-plot, eval = FALSE}
?plot
```

--
However, this will be uninformative on how an object of class histogram is plotted. 
Try instead 

```{r help-plot-histogram, eval = FALSE}
?plot.histogram
```

This will give the documentation for the plot method for objects of class histogram. 


---
## S3 Overview

* S3 classes are standard data structures (typically lists) with *class labels*.
--

* It is an informal system. No checks of object content.
--

* One implements a *generic* function via `UseMethod()`. E.g.
```{r, eval=FALSE}
plot
```

```{r, echo=FALSE}
cat(deparse(plot))
```

--

* **Methods** for specific classes are implemented as standard R functions 
with the naming convention `f.classname()` for a method for class `classname` of the function `f()`. 
--

* The system is widely used to write methods for the generic functions `print()`, 
`plot()` and `summary()`. 


---
## Constructing a New Class

```{r, echo=FALSE}
count_zeros_vec <- function(x) {
  sum(x == 0)
}
```

Recall the function `count_zeros_vec()` that counts the number of zeros in a vector.
--


If we need this number many times it is beneficial to compute it once and then 
extract it whenever needed. 

--

We first write a *constructor function* that returns a list with a class
label.

```{r}
count_object <- function(x) {
  structure(
    list(
      x = x,
      n = count_zeros_vec(x)
    ),
    class = "count_object"
  )
}
```

---
## A Data Example

```{r}
set.seed(1234)

count_data <- count_object(rpois(10, 2))
count_data
```

---
## The Generic Function

To activate looking up a method for a specific class, one needs to tell R 
that the function `count_zeros()` is a *generic function*. 

```{r}
count_zeros <- function(x) {
  UseMethod("count_zeros")
}
```
--

We can let the default method be the vectorized version.
```{r}
count_zeros.default <- function(x) {
  count_zeros_vec(x)
}
```

--

Then we can implement a class-specific version for the class `count_object`.

```{r}
count_zeros.count_object <- function(x) {
  x[["n"]]
}
```

---

## A Print Method

And we can also implement a print method.

```{r}
print.count_object <- function(x) {
  cat("Values:", x$x, "\nNumber of zeros:", x$n, "\n")
}
```


```{r}
count_data # Invokes print.count_object()
```

```{r}
count_zeros(count_data) # Invokes count_zeros.count_object()
```

---

## Exercise

### Step 1

Create a constructor function called `summarize_vector()` that 
that takes a numeric vector as input and returns an object of class `vector_summary` containing
the mean, median, and standard deviation of the input vector.

```{r include = FALSE}
summarize_vector <- function(x) {
  structure(
    list(
      mean = mean(x),
      median = median(x),
      sd = sd(x)
    ),
    class = "vector_summary"
  )
}
```

### Step 2

Write a `print()` method for the class `vector_summary` that prints the mean, median, and standard deviation in a neatly formatted way.

## Angle Data

```{r, fig.asp = 0.8}
hist(phipsi$psi, prob = TRUE, xlab = expression(psi), main = NULL)
rug(phipsi$psi)
```

---

class: middle, center

# Density Estimation

---

## Density Estimation

Let $f_0$ denote the unknown desnity we want to estimate.

- If we fit a parameterize statistical model $(f_\theta)_\theta$ to
  data using the estimator $\hat{\theta}$, then $f_{\hat{\theta}}$ 
  is an estimate of $f_0$.
--

- The histogram is a nonparameteric density estimator, $\hat{f}$.
--

- We are interested in nonparametric estimators because
  * we want to compare data with the parametric estimate,
  * we don't know a suitable parametric model, and
  * we want aid in visualization.

---

## Density Estimation

Density estimation relies on the approximation
$$P(X \in (x-h, x+h)) = \int_{x-h}^{x+h} f_0(z) \ dz \simeq f_0(x) 2h.$$
--

Rearranging and using LLN gives

\begin{align*}
  f_0(x) & \simeq \frac{1}{2h}P(X \in (x-h, x+h)) \\
  & \simeq \frac{1}{2h} \frac{1}{n} \sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\
  & =  \frac{1}{2hn} \sum_{i=1}^n 1_{(-h, h)}(x - x_i) = \hat{f}_h(x)
\end{align*}

---

## Kernels 

We will consider *kernel estimators* 
$$\hat{f}_h(x) = \frac{1}{hn} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right).$$

--
The *uniform* or *rectangular kernel* is 
$$K(x) = \frac{1}{2} 1_{(-1,1)}(x).$$
--
The *Gaussian kernel* is 
$$K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.$$

???

Two types of possible density estimators
  - Pointwise 
  - Everything at one 


---

## Implementation with Gaussian Kernel

```{r kern-dens-impl}
kern_dens <- function(x, h, m = 512) {
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)

  for (i in seq_along(xx)) {
    for (j in seq_along(x)) {
      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))
    }
  }

  y <- y / (sqrt(2 * pi) * h * length(x))

  list(x = xx, y = y)
}
```

---
## Test

```{r}
f_hat <- kern_dens(phipsi$psi, 0.2)
f_hat_dens <- density(phipsi$psi, 0.2)
```

.pull-left[
```{r test-dens, eval = FALSE}
plot(
  f_hat,
  type = "l",
  lwd = 4,
  xlab = "x",
  ylab = "Density"
)

lines(
  f_hat_dens,
  col = "red",
  lwd = 2
)
```
]

.pull-right[
```{r test-dens-output, ref.label="test-dens", echo=FALSE, fig.width = 6, fig.asp = 0.9}
```
]

---
## Test

```{r, fig.width = 7, fig.asp = 0.7}
plot(
  f_hat$x,
  f_hat$y - f_hat_dens$y,
  type = "l",
  lwd = 2,
  xlab = "x",
  ylab = "Difference"
)
```

---

---
## Density Estimation

For a parametric family we can use the MLE
$$\hat{\theta} = \text{arg max}_{\theta} \sum_{j=1}^n \log f_{\theta}(x_j).$$

--
For nonparametric estimation we can still introduce the log-likelihood:
$$\ell(f) = \sum_{j=1}^n \log f(x_j)$$
--
Let's see what happens for the Gaussian kernel density estimate
$$f(x) = f_h(x) = \frac{1}{nh \sqrt{2 \pi}} \sum_{j=1}^n e^{- \frac{(x - x_j)^2}{2 h^2} }.$$

---

## Density Estimation

```{r, fig.align='center'}
f_h <- function(x, h) mean(dnorm(x, phipsi$psi, h))
f_h <- Vectorize(f_h)
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 1), add = TRUE, col = "red")
```

---
## Density Estimation, $h = 1$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 1), add = TRUE, col = "red")
```

---
## Density Estimation, $h = 0.25$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.25), add = TRUE, col = "red", n = 1001)
```

---
## Density Estimation, $h = 0.01$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.1), add = TRUE, col = "red", n = 1001)
```

---
## Density Estimation, $h = 0.025$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.025), add = TRUE, col = "red", n = 10001)
```

---
## Density Estimation, $h = 0.01$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.01), add = TRUE, col = "red", n = 10001)
```

---
## Density Estimation, $h \to 0$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.0001), add = TRUE, col = "red", n = 10001)
```

---
## Log-Likelihood

If $x_i \neq x_j$ when $i \neq j$

\begin{aligned}
\ell(f_h) & = \sum_{i} \log\left(1 + \sum_{j \neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \right) - 
n \log(nh\sqrt{2 \pi}) \\
& \sim - n \log(nh\sqrt{2 \pi})
\end{aligned}

for $h \to 0$. 
--

Hence, $\ell(f_h) \to \infty$ for $h \to 0$ and there is no MLE in the set of distributions
with densities.

---
## Log-Likelihood

It actually holds that 
$$f_h \cdot m \overset{\mathrm{wk}}{\longrightarrow} 
\varepsilon_n = \frac{1}{n} \sum_{j=1}^n \delta_{x_j}$$
for $h \to 0$ (weak convergence). 
--

The *empirical measure* $\varepsilon_n$ can sensibly be regarded as 
the nonparametric MLE of the distribution. 

---

## ISE, MISE, and MSE

Quality of $\hat{f}_h$ can be quantified by the *integrated squared error*,
$$\mathrm{ISE}(\hat{f}_h) = \int (\hat{f}_h(x) - f_0(x))^2 \ dx = ||\hat{f}_h - f_0||_2^2.$$ 

--

Quality of the estimation procedure can be quantified by the mean ISE,
$$\mathrm{MISE}(h) = E(\mathrm{ISE}(\hat{f}_h)),$$
where the expectation integral is over the data.

--

$$\mathrm{MISE}(h) = \int \mathrm{MSE}_h(x) \ dx$$ 
where $\mathrm{MSE}_h(x) = \mathrm{var}(\hat{f}_h(x)) + \mathrm{bias}(\hat{f}_h(x))^2$.


---
## AMISE

If $K$ is a square integrable probability density with mean 0,
$$\mathrm{MISE}(h) = \mathrm{AMISE}(h) + o((nh)^{-1} + h^4)$$
where the *asymptotic mean integrated squared error* is
$$\mathrm{AMISE}(h) = \frac{\|K\|_2^2}{nh} + \frac{h^4 \sigma^4_K \|f_0''\|_2^2}{4}$$
with $\sigma_K^2 = \int t^2 K(t) \ dt.$

---
## Asymptotically Optimal Bandwidth

Optimizing $\mathrm{AMISE}(h)$ over $h$ gives the asymptotically optimal 
bandwidth 
$$h_n = \left( \frac{\|K\|_2^2}{ \|f_0''\|^2_2  \sigma_K^4} \right)^{1/5} n^{-1/5}.$$
--

and the asymptotically optimal error 
$$\mathrm{AMISE}(h_n) = C n^{-4/5}.$$

--

Using various plug-in estimates of $\|f_0''\|_2^2$, AMISE can be used to 
estimate the *asymptotically optimal bandwidth* in a mean integrated squared
error sense. 

---
## Asymptotically optimal bandwidth

If we pretend $f_0$ is the density for $\mathcal{N}(0, \sigma^2)$, the optimal
bandwidth becomes 
$$h_n = \left( \frac{8 \sqrt{\pi} \|K\|_2^2}{3 \sigma_K^4} \right)^{1/5} \sigma n^{-1/5}.$$

--

With the Gaussian kernel, $\|K\|_2^2 = 1/(2\sqrt{\pi})$ and $\sigma_K^4 = 1$, and 
the optimal bandwidth can be estimated as 

$$\hat{h}_n = \left(\frac{4}{3}\right)^{1/5} \tilde{\sigma} n^{-1/5}$$

with a (robust) estimate $\tilde{\sigma}$ of $\sigma.$  

--

*Silverman's rule of thumb* uses the bandwidth
$$\hat{h}_n = 0.9 \tilde{\sigma} n^{-1/5}.$$

---
## Amino Acid Angles (Silverman, the default)

.two-column-left[
```{r dens3, eval = FALSE}
density(phipsi$phi)
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens4, eval = FALSE}
density(phipsi$psi)
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi), col = "red", lwd = 2)
```
]

---
## Amino Acid Angles (Scott, using 1.06)

.two-column-left[
```{r dens5, eval = FALSE, fig.width = 5, fig.asp = 1}
density(phipsi$phi, bw = "nrd")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi, bw = "nrd"), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens6, eval = FALSE}
density(phipsi$psi, bw = "nrd")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi, bw = "nrd"), col = "red", lwd = 2)
```
]


---
## Amino Acid Angles (Sheather & Jones)

.two-column-left[
```{r dens7, eval = FALSE}
density(phipsi$phi, bw = "SJ")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi, bw = "SJ"), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens8, eval = FALSE}
density(phipsi$psi, bw = "SJ")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi, bw = "SJ"), col = "red", lwd = 2)
```
]


