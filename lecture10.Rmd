---
title: "The EM-Algorithm"
subtitle: "Computational Statistics"
author: "Johan Larsson, Niels Richard Hansen"
date: "October 3, 2024"
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 5,
  fig.retina = 3,
  fig.align = "center",
  cache = FALSE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

library(animate)
library(tidyverse)
library(patchwork)

old_options <- options(digits = 4)

xaringanExtra::use_tile_view()

theme_set(theme_grey(base_size = 18))
```

## Today

### The EM Algorithm

Useful method for likelihood optimization in the presence of missing data or latet variables.

--

### Fisher Information

When the likelihood is optimized, the Fisher information gives the variance of the MLE.

--

### Gaussian Mixtures

Example of a finite mixture model where the EM-algorithm is useful.

```{r moth_likelihood, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, prob, constraint = function(par) TRUE) {
  function(par) {
    pr <- prob(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2, 2 * p[1] * p[2], 2 * p[1] * p[3],
    p[2]^2, 2 * p[2] * p[3], p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 && par[1] >= 0 && par[2] <= 1 && par[2] >= 0 && 1 - par[1] - par[2] >= 0
}

loglik <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), loglik)
```

---

class: center, middle

# The EM Algorithm

---

## Incomplete Data Likelihood

Suppose that $Y$ is a random variable and $X = M(Y)$. Suppose that $Y$ has density
$f(\cdot \mid \theta)$ and that $X$ has marginal density $g(x \mid \theta)$.

--

The marginal density is typically of the form
$$g(x \mid \theta) = \int_{\{y: M(y) = x\}} f(y \mid \theta) \ \mu_x(\mathrm{d} y)$$
for a suitable measure $\mu_x$ depending on $M$ and $x$ but not $\theta$.

--

If $Y = (X, Z)$, then
$$g(x \mid \theta) = \int f(x, z \mid \theta) \, \mathrm{d} z = \int f(x \mid z, \theta) h(z \mid \theta) \, \mathrm{d}z.$$

--

### Problem

$g(x \mid \theta)$ typically impossible (or difficult) to compute.

???

The general argument for the marginal density relies on the coarea formula.

---

## Conditional Expectation

The complete log-likelihood, $\log f(y \mid \theta)$, **is** often easy to
compute, but we don't know $Y$, only that $M(Y) = x$.

--

In some cases it is possible to compute

$$Q(\theta \mid \theta') := \operatorname{E}_{\theta'}(\log f(Y \mid \theta) \mid X = x),$$

which is the *conditional expectation of the complete log-likelihood given the observed
data and under the probability measure given by $\theta'$.*

---

## The EM Algorithm

### Idea

With an initial guess of $\theta' = \theta^{(n)}$, iteratively compute
$$\theta^{(n + 1)} = \textrm{arg max} \ Q(\theta \mid \theta^{(n)})$$
for $n = 0, 1, 2, \ldots$.

--

### Steps

* **E-step**: Compute the conditional expectation $Q(\theta \mid \theta^{(n)})$.
--

* **M-step**: Maximize $Q(\theta \mid \theta^{(n)})$ in $\theta$.

---


## Conditional Distributions

Generally, conditional distribution of $Y$ given $X = x$ has density
$$h(y \mid x, \theta) = \frac{f(y \mid \theta)}{g(x \mid \theta)}$$
w.r.t. a suitable measure $\mu_x$ that does not depend upon $\theta$.

--

$Y = (Z, X)$ with joint density w.r.t. a product measure $\mu \otimes \nu$ that does not depend upon $\theta$.

--

In the latter case, $f(y \mid \theta) = f(z, x \mid \theta)$ and
$$g(x \mid \theta) = \int f(z, x \mid \theta) \ \mathrm{d} z.$$

---

## The Central Identity

In general
$$\ell(\theta) = \log g(x \mid \theta) = \log f(y \mid \theta) - \log h(y \mid x, \theta),$$
and under some integrability conditions, this decomposition is used to show
that the EM-algorithm increases the log-likelihood, $\ell(\theta)$, in each iteration.

---

## An Ascent Method

It can be shown that
$$\log g(x \mid \theta) - \log g(x \mid \theta') \geq Q(\theta | \theta') - Q(\theta' \mid \theta)$$

--

In other words, marginal likelihood **weakly** increases in each iteration.

--

If $Q(\theta^{(n+1)} \mid \theta) > Q(\theta^{(n)} \mid \theta)$ then the
algorithm **strictly** increases the likelihood.

---

```{r emiterations-figure, echo = FALSE, out.width = "87%", fig.cap = "EM iterations, Â© 2024 Matthew N. Bernstein"}
knitr::include_graphics("images/em-iterations.png")
```

---

## Multinomial Complete Data Likelihood

If $Y \sim \textrm{Mult}(p, n)$ the complete data log-likelihood is
$$\ell_{\textrm{complete}}(p) = \sum_{i=1}^K Y_i \log(p_i).$$

--

Thus
$$Q(p \mid p') = \operatorname{E}_{p'}( \ell_{\textrm{complete}}(p) \mid X = x) = \sum_{i=1}^K \operatorname{E}_{p'}( Y_i \mid X = x) \log(p_i)$$
for any $X = M(Y)$.

---

## E-step for Multinomial Model

For the multinomial model with $M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}$ the
cell collapsing map corresponding to the partition
$A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K \}$,
$$\operatorname{E}_p (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.$$
for $k \in A_j$.

--

### Abstract E-Step Implementation

```{r}
EStep_mult <- function(p, x, group) {
  x[group] * p / M(p, group)[group]
}
```

---

## Multinomial MLE for Moths

With $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete observation,
it can be shown that the MLE is
\begin{align}
  \hat{p}_C & = (n_{CC} + (n_{CI} + n_{CT}) / 2) / n \\
  \hat{p}_I & = ((n_{CI} + n_{IT}) / 2 + n_{II}) / n
\end{align}
where $n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.

--

For us $\hat{p} = \frac{1}{n} \mathbf{X} y$.

.pull-left[
```{r x-matrix}
X <- matrix(
  c(
    2, 1, 1, 0, 0, 0,
    0, 1, 0, 2, 1, 0
  ) / 2,
  2, 6,
  byrow = TRUE
)
```
]

.pull-right[
```{r x-matrix-eval}
X
```
]

---

## Abstract M-Step

MLE of complete log-likelihood is linear estimator in $y / n$

```{r mstep-mult}
MStep_mult <- function(n, X) {
  as.vector(X %*% n / (sum(n)))
}
```
--

`EStep_mult()` and `MStep_mult()` are abstract implementations. Requires
arguments `group` and `X`.

--

M-step only implemented when complete-data MLE is
*linear estimator*.

---

### EM Factory for Multinomial Models

```{r EM-factory}
EM_multinomial <- function(x, group, prob, X) {
  force(x)
  force(group)
  force(prob)
  force(X)

  EStep <- function(p) EStep_mult(prob(p), x, group)
  MStep <- function(n) MStep_mult(n, X)

  function(par, epsilon = 1e-6, maxit = 20, cb = NULL) {
    for (i in seq_len(maxit)) {
      par0 <- par
      par <- MStep(EStep(par))
      if (!is.null(cb)) cb()
      if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {
        break
      }
    }
    par
  }
}
```

---

## Peppered Moths EM Algorithm

```{r, results='hold'}
EM <- EM_multinomial(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  X = matrix(
    c(
      2, 1, 1, 0, 0, 0,
      0, 1, 0, 2, 1, 0
    ) / 2,
    2, 6,
    byrow = TRUE
  )
)

EM(c(0.3, 0.3))
moth_optim$par
```

---

## Inside the EM Algorithm

```{r}
library(CSwR)
EM_tracer <- tracer("par")
EM(c(0.3, 0.3), cb = EM_tracer$tracer)

summary(EM_tracer)
```

--

---

### Tracing the EM Algorithm

```{r em-tracer}
EM_tracer <- tracer(c("par0", "par"), Delta = 0)
phat <- EM(c(0.3, 0.3), epsilon = 0, cb = EM_tracer$tracer)
phat
EM_trace <- summary(EM_tracer)
tail(EM_trace)
```

---

## Adding Computed Values

```{r}
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))
EM_trace <- transform(
  EM_trace,
  n = seq_len(nrow(EM_trace)),
  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),
  loglik = loglik_pep(par.1, par.2)
)
```

---

```{r plot-curves, echo = FALSE, warning = FALSE, fig.width = 9, fig.height = 8.5}
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))

group <- c(1, 1, 1, 2, 2, 3)

EStep <- function(p) EStep_mult(prob(p), x, group)
MStep <- function(n) MStep_mult(n, X)

epsilon <- 1e-6
maxit <- 5

exp_vals <- matrix(NA, nrow = maxit, ncol = length(group))

par <- c(0.3, 0.3)

level_curves <- vector("list", length = maxit)

ll <- double(maxit)
x <- c(85, 196, 341)

n_mat <- 100
pC <- seq(0, 1, length.out = n_mat)
pI <- seq(0, 1, length.out = n_mat)

pars <- vector("list", length = maxit)

for (i in seq_len(maxit)) {
  par0 <- par
  y <- EStep(par)

  x_new <- M(y, group)
  z <- matrix(NA, n_mat, n_mat)

  x <- c(85, 196, 341)

  for (k in seq_len(n_mat)) {
    for (j in seq_len(n_mat)) {
      # z[k, j] <- neg_loglik_pep(c(pC[k], pI[j]), x)
      val <- -sum(y * log(prob(c(pC[k], pI[j]))))
      if (is.finite(val)) {
        z[k, j] <- val
      } else {
        z[k, j] <- -Inf
      }
    }
  }

  level_curves[[i]] <- z

  ll[i] <- -sum(y * log(prob(par)))

  exp_vals[i, ] <- y
  par <- MStep(y)
  pars[[i]] <- par
}

# min_z <- min(sapply(level_curves, min))
# levels <- exp(seq(log(min_z), log(3000), length.out = 25))

pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

par_optim <- c(0.07084, 0.18874)

old_par <- par(no.readonly = TRUE)

par(mfrow = c(2, 2), mai = c(0.5, 0.5, 0.1, 0.1))
for (i in 1:4) {
  contour(
    pC,
    pI,
    level_curves[[i]],
    nlevels = 25,
    col = pal(25),
    asp = 1,
    drawlabels = FALSE
  )
  text(0.9, 0.9, labels = paste0("k = ", i))

  points(par_optim[1], par_optim[2], pch = 19, col = "dark orange")
  points(pars[[i]][1], pars[[i]][2], pch = 19)
}
par(old_par)
```


---

class: middle

```{r tracer-plot1, echo = FALSE, fig.height = 5.5}
ggplot(EM_trace, aes(n, par_norm_diff)) +
  geom_point() +
  geom_line() +
  labs(y = expression(paste("||", theta^
    {
      (n)
    } - theta^{
      (n - 1)
    }, "||"))) +
  scale_y_log10()
```

Note the log-axis. The EM-algorithm converges linearly.

---

## Linear Convergence

The log-rate of the convergence can be estimated by least-squares.

```{r linear-convergence}
log_lm <- lm(log(par_norm_diff) ~ n, data = EM_trace)
exp(coefficients(log_lm)["n"])
```

--

It is very small in this case implying fast convergence.

--

This is not always the case.
If the log-likelihood is flat, the EM-algorithm can become quite slow with a
rate close to 1.

---

class: middle

```{r loglikelihood-convergence, warning=FALSE, echo = FALSE, fig.cap="Log-likelihood convergence", fig.height = 6}
mutate(EM_trace, diff_loglik = loglik - min(loglik)) |>
  ggplot(aes(n, diff_loglik)) +
  labs(y = expression(l[theta] - min(l))) +
  geom_point() +
  geom_line() +
  scale_y_log10()
```

---

## Optimization and Statistics

The EM-algorithm is a general algorithm for numerical optimization of a log-likelihood function. It works
by iteratively optimizing

$$Q(\theta \mid \theta^{(n)}) = E_{\theta^{(n)}}(\log f(Y \mid \theta) \mid X = x).$$

--

For numerical optimization of $Q$ or variants of EM (generalized EM algorithms)
the gradient and Hessian of $Q$ can be useful.

--

For statistics we need the observed Fisher information (Hessian of the negative log-likelihood for the observed data).

---

## Local Maxima

- $Q$ is not typically convex, so no guarantee that
  the EM-algorithm converges to the global maximum.
--

- How to deal with this?
--

- Random starting values.
--

- Global optimization (e.g. simulated annealing, genetic algorithms).

---

class: middle, center

# Fisher Information

---

## Gradient of $Q$ for the Multinomial Model

Note that with $p = p(\theta)$ in some parametrization of the
cell probabilities,
$$Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)} p_i(\theta')}{M(p(\theta'))_{j(i)}} \log p_i(\theta),$$
where $j(i)$ is defined by $i \in A_{j(i)}$.

--

The gradient of $Q$ w.r.t. $\theta$
$$\nabla_{\theta} Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)}p_i(\theta')}{M(p(\theta'))_{j(i)}p_i(\theta)} \nabla p_i(\theta).$$

--

When evaluated in $\theta'$ it is

$$\nabla_{\theta} Q(\theta' \mid \theta') = \sum_{i} \frac{x_{j(i)}}{M(p(\theta'))_{j(i)}} \nabla p_i(\theta').$$

---

### Gradient Implementation

```{r}
Dprob <- function(p) {
  matrix(
    c(
      2 * p[1], 0,
      2 * p[2], 2 * p[1],
      2 * p[3] - 2 * p[1], -2 * p[1],
      0, 2 * p[2],
      -2 * p[2], 2 * p[3] - 2 * p[2],
      -2 * p[3], -2 * p[3]
    ),
    ncol = 2, nrow = 6, byrow = TRUE
  )
}

grad_Q <- function(p, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] / M(prob(p), group)[group]) %*% Dprob(p)
}
```

---

### Gradient Identity

Though computed as the gradient of $Q$,
$$\nabla_{\theta} Q(\theta' \mid \theta') = \nabla_{\theta} \ell(\theta')$$
from the fact that
$$\theta' = \arg\max_{\theta} \left(Q(\theta \mid \theta') -  \ell(\theta)\right).$$

--

Can also be verified by direct computation.

--

The gradient is effectively zero in the limit.

```{r p-hat-call}
p_hat <- EM(c(0.3, 0.3), epsilon = 1e-20)
```

.pull-left[

```{r p-hat}
p_hat
```
]

.pull-right[

```{r grad-q}
grad_Q(p_hat)
```
]

---

## Empirical Fisher Information

Recall that a multinomial observation with size parameter $n$
can be regarded as $n$ i.i.d. observations.

--

For i.i.d. observations the Fisher information (for one sample)
can be estimated as the empirical variance of the gradient
of the log-likelihood. By the identity
$$\nabla_{\theta} Q_i(\theta' \mid \theta') = \nabla_{\theta} \ell_i(\theta')$$
holding for each observation, we can compute the empirical variance.

---

### Moths Empirical Fisher

Can think of the moths as i.i.d. observations.

```{r}
emp_Fisher <- function(p, x = c(85, 196, 341)) {
  grad1 <- grad_Q(p, c(1, 0, 0))
  grad2 <- grad_Q(p, c(0, 1, 0))
  grad3 <- grad_Q(p, c(0, 0, 1))
  x[1] * t(grad1) %*% grad1 +
    x[2] * t(grad2) %*% grad2 +
    x[3] * t(grad3) %*% grad3
}
```
--

```{r}
emp_Fisher(p_hat)
```

---

### Numerical Fisher information

.pull-left[
#### `stats::optimHess()`

```{r}
-optimHess(p_hat, loglik, grad_Q)
```
]

--

.pull-right[
#### `numDeriv::jacobian()`

```{r}
library(numDeriv)
ihat <- -jacobian(grad_Q, p_hat)
ihat
```
]

- Different estimates of same quantity
- Should be close but not identical

---

### Measuring Variance

- Fisher information can be used to compute standard errors of MLEs
--

- Empirical Fisher information can be computed for i.i.d. observations using
  the gradients $\nabla_{\theta} Q_i(\bar{\theta} \mid \hat{\theta})$ evaluated in
  $\bar{\theta} = \hat{\theta}$.
--

- Observed Fisher information can be computed by numerical differentiation
  of the gradient $-\nabla_{\theta} Q(\bar{\theta} \mid \bar{\theta})$
  evaluated in $\bar{\theta} = \hat{\theta}$.

--

#### Bootstrap

- Another alternative is to use the bootstrap to compute standard errors.
--

- Can be done nonparametrically
--

- Or parametrically using the model assumptions.
--

- Computationally more expensive

---

## Second Way: The Information Identity

From
$$\ell(\theta) = Q(\theta \mid \theta') + H(\theta \mid \theta')$$
it follows that the observed Fisher information equals
$$\hat{i}_X := - D^2_{\theta} \ell(\hat{\theta}) =
\underbrace{-D^2_{\theta} Q(\hat{\theta} \mid \theta')}_{\hat{i}_Y(\theta')} -
\underbrace{D^2_{\theta} H(\hat{\theta} \mid \theta')}_{\hat{i}_{Y \mid X}(\theta')}.$$

--

Introducing $\hat{i}_Y := \hat{i}_Y(\hat{\theta})$ and $\hat{i}_{Y \mid X} = \hat{i}_{Y \mid X}(\hat{\theta})$. We have the *information identity*
$$\hat{i}_X = \hat{i}_Y - \hat{i}_{Y \mid X}.$$

--

### Interpretation

- $\hat{i}_Y$ is the Fisher information for complete $Y$.
--

- $\hat{i}_{Y \mid X}$ is the information "lost" from not observing full $X$.


---

### More Identities

How to compute the information loss?

--

The second **Bartlett identity** can be reformulated as

$$\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})
= - \partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})$$
which follows from differentiation under the integral of $\int h(y \mid x, \theta) \mu_x(d y) = 1.$

--

And since we also have

$$Q(\theta \mid \theta') = \ell(\theta) - H(\theta \mid \theta')$$

we find that

$$\partial_{\theta'_i} \partial_{\theta_j} Q(\bar{\theta} \mid \bar{\theta}) =  -
\partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}) =
\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}).$$
--

Thus

$$\hat{i}_{Y \mid X} = D_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta}).$$

---

### New Implementations

First we implement the map $Q$ as an R function.

```{r q-def}
Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  pp[3] <- 1 - pp[1] - pp[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p))
}
```
--

and a modified `grad_Q()` of two arguments:

```{r gradq-def}
grad_Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / (M(prob(pp), group)[group] * prob(p))) %*% Dprob(p)
}
```

---

### Numerical Differentiation of $Q$

We use numDeriv functions `jacobian()` and `hessian()` to differentiate numerically.

```{r q-hess}
iY <- -hessian(Q, p_hat, pp = p_hat)
iY
```
--

```{r q-jacobian}
-jacobian(grad_Q, p_hat, pp = p_hat) # This should be the same as iY
```

--

```{r iyx-jacobian}
iYX <- jacobian(function(pp) grad_Q(p_hat, pp), p_hat)
iY - iYX # same as `emp_Fisher(p_hat)`
```

---

## Third Way: The EM-Mapping

Define $\Phi : \Theta \mapsto \Theta$ by
$$\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta').$$

--

A global maximum of the likelihood is a fixed point of $\Phi$, $\Phi(\theta) = \theta.$

--

Using that the limit of the EM algorithm, $\hat{\theta}$, is a fixed point and
other identities above, it can be shown that
$$D_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.$$

--

Hence
\begin{aligned}
\hat{i}_X & = \left(I - \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y = \left(I - D_{\theta} \Phi(\hat{\theta})^T\right) \hat{i}_Y.
\end{aligned}

--

$D_{\theta} \Phi(\hat{\theta})$ can be computed via numerical differentiation.

---

### EM Step Implementation

```{r}
EM_multinomial_step <- function(x, group, prob, X) {
  force(x)
  force(group)
  force(prob)
  force(X)

  EStep <- function(p) EStep_mult(prob(p), x, group)

  MStep <- function(n) MStep_mult(n, X)

  function(par) MStep(EStep(par))
}
```

--

```{r}
Phi <- EM_multinomial_step(c(85, 196, 341), c(1, 1, 1, 2, 2, 3), prob, X)
p_hat
Phi(p_hat) # The limit is a fixed point
```

---

### Differentiating the EM Map

```{r}
DPhi <- jacobian(Phi, p_hat) # Using numDeriv function 'jacobian()'
iX <- (diag(1, 2) - t(DPhi)) %*% iY
iX
ihat # Computed using numerical differentiation of grad_Q
```

---

class: middle, center

# Gaussian Mixtures

---

## Finite Mixtures

Let $Z \in \{1, \ldots, K\}$ with $P(Z = k) = p_k$, and the conditional distribution
of $X$ given $Z = k$ has density $f_k( \cdot \mid \psi_k)$.

--

The joint density is
$$(x, k) \mapsto f_k(x \mid \psi_k) p_k$$

--

and the marginal density for the distribution of $X$ is
$$f(x \mid \theta) =  \sum_{k=1}^K f_k(x \mid \psi_k) p_k.$$

---

## Gaussian Mixtures $(K = 2)$

The two Gaussian distributions are parametrized by five parameters
$\mu_1, \mu_2 \in \mathbb{R}$ and $\sigma_1, \sigma_2 > 0$, and $p = P(Z = 1) = 1 - P(Z = 2)$.

--

The conditional distribution of $X$ given $Z = k$ is
$$f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.$$

--

The marginal density is
$$f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.$$

---

### Simulation

.pull-left[
```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5

n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)

# Conditional simulation
# from mixture components
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```
]

.pull-right[
```{r, echo=FALSE, fig.height=5, fig.width = 5.5}
gausdens <- function(x) {
  (p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) / sqrt(2 * pi)
}
xx <- seq(-3, 11, 0.1)
hist(x, freq = FALSE, ylim = c(0, 0.25), main = "")
lines(xx, gausdens(xx), col = "red", lwd = 2)
```
]

---

## Log-likelihood

We assume $\sigma_1$ and $\sigma_2$ known.

```{r gaus-mix-loglik, cache = TRUE}
loglik <- function(par, x) {
  p <- par[1]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  mu1 <- par[2]
  mu2 <- par[3]

  -sum(log(p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2))
}
```

---

## Optimizing

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]
```

--

Again, however, initialization matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), loglik, x = x)[c(1, 2)]
```

---

## The $Q$-Function

The complete data log-likelihood is
$$\sum_{i=1}^n 1(z_i = 1) \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + 1(z_i = 2)\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right) $$

--

and

$$Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)$$

where $\hat{p}_i = P_{\theta'}(Z_i = 1 \mid X_i = x_i)$.

--

The maximum is attained at

$$\theta = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).$$

---

### The E-Step

The conditional probability in a mixture model is generally

$$P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}$$

--

which for the $K = 2$ Gaussian case gives
$$\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.$$


--

### Implementation

See [`em_gauss_mix.R`](R/em_gauss_mix.R) for the source file of the implementation.

```{r source-em-mix}
source("R/em_gauss_mix.R")
```

---

## EM

```{r run-em-gauss-mix}
EM <- EM_gauss_mix(x)
```

--
and testing

```{r test-em-gauss-mix}
EM(c(0.5, -0.5, 4))
```

--

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]
```

--

```{r run-em-gauss-mix-again}
EM(c(0.9, 3, 1)) # Starting value still matters
```

---

## Gradients and Numerical Differentiation

```{r numderiv-grad1}
library(numDeriv)
grad1 <- function(par) grad(function(par) loglik(par, x), par)
```

--

```{r numderiv-grad2}
EStep <- environment(EM)$EStep
Q <- function(par, par_prime) {
  phat <- EStep(par_prime)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(phat * (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
    (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2)))
}
grad2 <- function(par) -grad(Q, par, par_prime = par)
```

---

## Gradient Identity

```{r gradient-identities1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

--

```{r gradient-identities2}
par_hat <- EM(c(0.5, 0, 4))
grad1(par_hat)
grad2(par_hat)
```

---

### Convergence

```{r tracing-convergence}
library(CSwR)

EM_tracer <- tracer(
  c("par0", "par", "loglik", "h_prime"),
  Delta = 0,
  expr = quote({
    loglik <- loglik(par, x)
    h_prime <- sum(grad2(par)^2)
  })
)

par_hat <- EM(c(0.2, 2, 2), cb = EM_tracer$tracer)

EM_trace <- summary(EM_tracer)
tail(EM_trace, 4)
```

---

class: middle

.pull-left[
```{r convergence-plots-1, warning=FALSE, fig.width = 5.5}
autoplot(
  EM_trace,
  y = loglik - min(loglik)
)
```
]

.pull-right[
```{r convergence-plots-2, warning=FALSE, fig.width = 5.5}
autoplot(
  EM_trace,
  y = h_prime
)
```
]

---

class: center, middle

```{r mixture-convergence, echo = FALSE, fig.width = 10, fig.height = 8, fig.show = "animate", cache = TRUE}
gaussdens2 <- function(x, p, mu1, mu2) {
  (p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) / sqrt(2 * pi)
}

old_par <- par(no.readonly = TRUE)

# par(mfrow = c(2, 3), mai = c(0.5, 0.5, 0.1, 0.1))

for (i in seq_len(20)) {
  hist(x, freq = FALSE, ylim = c(0, 0.25), main = "", border = "transparent")
  lines(xx, gausdens(xx), col = "steelblue4", lwd = 2)
  p_i <- EM_trace$par0.1[i]
  mu1_i <- EM_trace$par0.2[i]
  mu2_i <- EM_trace$par0.3[i]
  lines(xx, gaussdens2(xx, p_i, mu1_i, mu2_i), col = "dark orange", lwd = 2)
  text(9, 0.20, labels = paste0("k = ", i))
}

# par(old_par)
```


---

## Summary

- We introduced the EM-algorithm, using the pepper moth example throughout
--

- We showed three ways to compute the Fisher information
--

- We covered Gaussian mixtures and how to
 optimize over their parameters using the EM algorithm
