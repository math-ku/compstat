---
title: "The EM-Algorithm"
execute:
  cache: false
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}

library(patchwork)

old_options <- options(digits = 4)
```

```{r moth_likelihood, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, prob, constraint = function(par) TRUE) {
  function(par) {
    pr <- prob(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}

loglik <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), loglik)
```

## Last Time

### Likelihood Optimization

We introduced the Peppered Moths problem and solved the full likelihood using
general-purpose and constrained optimization.

\medskip\pause

We talked about the barrier methods for constrained optimization.

. . .

### The EM Algorithm

We introduced the EM-algorithm as a general algorithm for likelihood
optimization when the likelihood is complicated.

## Recap: The EM Algorithm

The EM-algorithm iteratively optimizes
$$
\theta^{(t+1)} = \argmin_\theta Q(\theta \mid \theta^{(n)}) = E_{Z \mid X, \theta^{(t)}}(\log f(X \mid \theta) \mid Z).
$$

\medskip\pause

The algorithm monotonically increases the observed data log-likelihood at each
step.

## Today

### More on the EM Algorithm

Examples on Gaussian mixtures and the multinomial model.

. . .

### Fisher Information

When the likelihood is optimized, the Fisher information gives the variance of
the MLE.

. . .

### Gaussian Mixtures

Example of a finite mixture model where the EM-algorithm is useful.

## Multinomial Complete Data Likelihood

If $Y \sim \textrm{Mult}(p, n)$ the complete data log-likelihood is
$$
\ell_{\textrm{complete}}(p) = \sum_{i=1}^K Y_i \log(p_i).
$$

. . .

Thus
$$
Q(p \mid p') = \operatorname{E}_{p'}( \ell_{\textrm{complete}}(p) \mid X = x) = \sum_{i=1}^K \operatorname{E}_{p'}( Y_i \mid X = x) \log(p_i)
$$
for any $X = M(Y)$.

## E-step for Multinomial Model

For the multinomial model with $M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}$
the cell collapsing map corresponding to the partition
$A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K \}$,
$$
\operatorname{E}_p (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.
$$
for $k \in A_j$.

. . .

### Abstract E-Step Implementation

```{r}
e_step_mult <- function(p, x, group) {
  x[group] * p / M(p, group)[group]
}
```

## Multinomial MLE for Moths

With $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete
observation, it can be shown that the MLE is
$$
\begin{aligned}
  \hat{p}_C & = \frac{n_{CC} + (n_{CI} + n_{CT}) / 2}{n} \\
  \hat{p}_I & = \frac{n_{II} + (n_{CI} + n_{IT}) / 2}{n}
\end{aligned}
$$
where $n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.

\medskip\pause

For us $\hat{p} = \frac{1}{n} \symbf{X} y$.

##

For us $\hat{p} = \frac{1}{n} \symbf{X} y$.

```{r x-matrix}
X <- matrix(
  c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2,
  2,
  6,
  byrow = TRUE
)
```


```{r x-matrix-eval}
X
```

## Abstract M-Step

MLE of complete log-likelihood is linear estimator in $y / n$

```{r mstep-mult}
m_step_mult <- function(n, X) {
  as.vector(X %*% n / (sum(n)))
}
```

\medskip\pause

`EStep_mult()` and `MStep_mult()` are abstract implementations. Requires
arguments `group` and `X`.

\medskip\pause

M-step only implemented when complete-data MLE is _linear estimator_.

## EM Factory for Multinomial Models

```{r}
create_em_mult_step <- function(x, group, prob, X) {
  force(x)
  force(group)
  force(prob)
  force(X)

  e_step <- function(p) e_step_mult(prob(p), x, group)
  m_step <- function(n) m_step_mult(n, X)

  function(par) m_step(e_step(par))
}
```

```{r EM-factory}
em <- function(par, em_step, epsilon = 1e-6, maxit = 20, cb = NULL) {
  for (i in seq_len(maxit)) {
    par0 <- par
    par <- em_step(par)
    if (!is.null(cb)) {
      cb()
    }
    if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {
      break
    }
  }
  par
}
```

## Peppered Moths EM Algorithm

```{r, results='hold'}
# EM <- EM_multinomial(
# x = c(85, 196, 341),
# group = c(1, 1, 1, 2, 2, 3),
# prob = prob,
# X = matrix(
# c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2,
# 2,
# 6,
# byrow = TRUE
# )
# )

em_mult_step <- create_em_mult_step(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  X = matrix(
    c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2,
    2,
    6,
    byrow = TRUE
  )
)

em(c(0.3, 0.3), em_mult_step)
moth_optim$par
```

## Inside the EM Algorithm

```{r}
library(CSwR)
em_tracer <- tracer("par")
em(c(0.3, 0.3), em_mult_step, cb = em_tracer$tracer)

summary(em_tracer)
```

## Tracing the EM Algorithm

```{r em-tracer}
em_tracer <- tracer(c("par0", "par"), Delta = 0)
phat <- em(c(0.3, 0.3), em_mult_step, epsilon = 0, cb = em_tracer$tracer)
phat
em_trace <- summary(em_tracer)
tail(em_trace)
```

## Adding Computed Values

```{r}
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))
em_trace <- transform(
  em_trace,
  n = seq_len(nrow(em_trace)),
  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),
  loglik = loglik_pep(par.1, par.2)
)
```

##

```{r plot-curves, echo = FALSE, warning = FALSE}
#| fig-width: 5
#| fig-height: 3.1
#| fig-cap: "EM algorithm iterations shown as points on the log-likelihood contour plot"
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))

group <- c(1, 1, 1, 2, 2, 3)

EStep <- function(p) e_step_mult(prob(p), x, group)
MStep <- function(n) m_step_mult(n, X)

epsilon <- 1e-6
maxit <- 5

exp_vals <- matrix(NA, nrow = maxit, ncol = length(group))

par <- c(0.3, 0.3)

level_curves <- vector("list", length = maxit)

ll <- double(maxit)
x <- c(85, 196, 341)

n_mat <- 100
pC <- seq(0, 1, length.out = n_mat)
pI <- seq(0, 1, length.out = n_mat)

pars <- vector("list", length = maxit)

for (i in seq_len(maxit)) {
  par0 <- par
  y <- EStep(par)

  x_new <- M(y, group)
  z <- matrix(NA, n_mat, n_mat)

  x <- c(85, 196, 341)

  for (k in seq_len(n_mat)) {
    for (j in seq_len(n_mat)) {
      val <- -sum(y * log(prob(c(pC[k], pI[j]))))
      if (is.finite(val)) {
        z[k, j] <- val
      } else {
        z[k, j] <- -Inf
      }
    }
  }

  level_curves[[i]] <- z

  ll[i] <- -sum(y * log(prob(par)))

  exp_vals[i, ] <- y
  par <- MStep(y)
  pars[[i]] <- par
}

par_optim <- c(0.07084, 0.18874)

# Build long data frame for first 4 iterations
build_df <- function(z_mat, iter) {
  out <- data.frame(
    pC = rep(pC, times = length(pI)),
    pI = rep(pI, each = length(pC)),
    z = as.vector(z_mat),
    iter = iter
  )
  out
}

plot_iters <- 1:4
df_all <- do.call(
  rbind,
  lapply(plot_iters, function(i) build_df(level_curves[[i]], i))
)

# Points (current parameter in each iteration)
iter_points <- data.frame(
  pC = sapply(pars[plot_iters], `[[`, 1),
  pI = sapply(pars[plot_iters], `[[`, 2),
  iter = plot_iters
)

mle_point <- data.frame(
  pC = par_optim[1],
  pI = par_optim[2]
)

ggplot(df_all, aes(pC, pI)) +
  geom_contour(
    aes(z = z, color = after_stat(level)),
    breaks = seq(800, 3000, by = 200)
  ) +
  geom_point(data = mle_point, colour = "darkorange", size = 3, pch = 4) +
  geom_point(data = iter_points, colour = "black", size = 2) +
  facet_wrap(~iter, labeller = labeller(iter = function(i) paste0("k = ", i))) +
  coord_equal() +
  guides(color = "none") +
  labs(x = expression(p[C]), y = expression(p[I])) +
  theme(panel.spacing = unit(0.8, "lines"))
```

##

```{r tracer-plot1, echo = FALSE}
#| fig-height: 2.7
#| fig-width: 3.5
#| fig-cap: "Parameter convergence of the EM algorithm applied to the Peppered moths problem."
ggplot(em_trace, aes(n, par_norm_diff)) +
  geom_point() +
  geom_line() +
  labs(
    y = expression(paste(
      "||",
      theta^{
        (n)
      } -
        theta^{
          (n - 1)
        },
      "||"
    ))
  ) +
  scale_y_log10()
```

## Linear Convergence

The log-rate of the convergence can be estimated by least-squares.

```{r linear-convergence}
log_lm <- lm(log(par_norm_diff) ~ n, data = em_trace)
exp(coefficients(log_lm)["n"])
```

\medskip\pause

It is very small in this case implying fast convergence.

\medskip\pause

This is not always the case. If the log-likelihood is flat, the EM-algorithm can
become arbitrarily slow with a rate close to 1.

##

```{r loglikelihood-convergence, warning=FALSE, echo = FALSE, fig.cap="Log-likelihood convergence"}
#| fig-width: 4
#| fig-height: 2.5
mutate(em_trace, diff_loglik = 1e-12 + loglik - min(loglik)) |>
  ggplot(aes(n, diff_loglik)) +
  labs(y = expression(l[theta] - min(l))) +
  geom_point() +
  geom_line() +
  scale_y_log10()
```

## Pitfalls

Bad initialization, degeneracy, label switching

## Multi-Start

## Optimization and Statistics

The EM-algorithm is a general algorithm for numerical optimization of a
log-likelihood function. It works by iteratively optimizing
$$
Q(\theta \mid \theta^{(n)}) = E_{\theta^{(n)}}(\log f(Y \mid \theta) \mid X = x).
$$

\medskip\pause

For numerical optimization of $Q$ or variants of EM (generalized EM algorithms)
the gradient and Hessian of $Q$ can be useful.

\medskip\pause

For statistics we need the observed Fisher information (Hessian of the negative
log-likelihood for the observed data).

## Local Maxima

- $Q$ is not typically convex, so no guarantee that the EM-algorithm converges
  to the global maximum.
- How to deal with this?
- Random starting values.
- Global optimization (e.g. simulated annealing, genetic algorithms).

# Fisher Information

## Gradient of $Q$ for the Multinomial Model

Note that with $p = p(\theta)$ in some parametrization of the cell
probabilities,
$$
Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)} p_i(\theta')}{M(p(\theta'))_{j(i)}} \log p_i(\theta),
$$
where $j(i)$ is defined by $i \in A_{j(i)}$.

\medskip\pause

The gradient of $Q$ w.r.t. $\theta$
$$
\nabla_{\theta} Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)}p_i(\theta')}{M(p(\theta'))_{j(i)}p_i(\theta)} \nabla p_i(\theta).
$$

\medskip\pause

When evaluated in $\theta'$ it is
$$
\nabla_{\theta} Q(\theta' \mid \theta') = \sum_{i} \frac{x_{j(i)}}{M(p(\theta'))_{j(i)}} \nabla p_i(\theta').
$$

## Gradient Implementation

```{r}
Dprob <- function(p) {
  matrix(
    c(
      2 * p[1],
      0,
      2 * p[2],
      2 * p[1],
      2 * p[3] - 2 * p[1],
      -2 * p[1],
      0,
      2 * p[2],
      -2 * p[2],
      2 * p[3] - 2 * p[2],
      -2 * p[3],
      -2 * p[3]
    ),
    ncol = 2,
    nrow = 6,
    byrow = TRUE
  )
}

grad_Q <- function(p, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] / M(prob(p), group)[group]) %*% Dprob(p)
}
```

## Gradient Identity

Though computed as the gradient of $Q$,
$$
\nabla_{\theta} Q(\theta' \mid \theta') = \nabla_{\theta} \ell(\theta')
$$
from the fact that
$$
\theta' = \arg\max_{\theta} \left(Q(\theta \mid \theta') -  \ell(\theta)\right).
$$

\medskip\pause

Can also be verified by direct computation.

\medskip\pause

The gradient is effectively zero in the limit.

```{r p-hat-call}
p_hat <- em(c(0.3, 0.3), em_mult_step, epsilon = 1e-20)
```

:::: {.columns}

::: {.column width="47%"}

```{r p-hat}
p_hat
```

:::

::: {.column width="47%"}

```{r grad-q}
grad_Q(p_hat)
```

:::

::::

## Empirical Fisher Information

Recall that a multinomial observation with size parameter $n$ can be regarded as
$n$ i.i.d. observations.

. . .

For i.i.d. observations the Fisher information (for one sample) can be estimated
as the empirical variance of the gradient of the log-likelihood. By the identity
$$
\nabla_{\theta} Q_i(\theta' \mid \theta') = \nabla_{\theta} \ell_i(\theta')
$$
holding for each observation, we can compute the empirical variance.

## Moths Empirical Fisher

Can think of the moths as i.i.d. observations.

```{r}
emp_fisher <- function(p, x = c(85, 196, 341)) {
  grad1 <- grad_Q(p, c(1, 0, 0))
  grad2 <- grad_Q(p, c(0, 1, 0))
  grad3 <- grad_Q(p, c(0, 0, 1))
  x[1] *
    t(grad1) %*% grad1 +
    x[2] * t(grad2) %*% grad2 +
    x[3] * t(grad3) %*% grad3
}
```

. . .

```{r}
emp_fisher(p_hat)
```

## Numerical Fisher information

:::: {.columns}

::: {.column width="47%"}

### `stats::optimHess()`

```{r}
-optimHess(
  p_hat,
  loglik,
  grad_Q
)
```

:::

. . .

::: {.column width="47%"}

### `numDeriv::jacobian()`

```{r}
library(numDeriv)
ihat <- -jacobian(grad_Q, p_hat)
ihat
```

:::

::::

. . .

Different estimates of same quantity. Should be close but not identical.

## Measuring Variance

- Fisher information can be used to compute standard errors of MLEs
- Empirical Fisher information can be computed for i.i.d. observations using the
  gradients $\nabla_{\theta} Q_i(\bar{\theta} \mid \hat{\theta})$ evaluated in
  $\bar{\theta} = \hat{\theta}$.
- Observed Fisher information can be computed by numerical differentiation of
  the gradient $-\nabla_{\theta} Q(\bar{\theta} \mid \bar{\theta})$ evaluated in
  $\bar{\theta} = \hat{\theta}$.

. . .

### Bootstrap

- Another alternative is to use the bootstrap to compute standard errors.
- Can be done nonparametrically
- Or parametrically using the model assumptions.
- Computationally more expensive

## Second Way: The Information Identity

From
$$
\ell(\theta) = Q(\theta \mid \theta') + H(\theta \mid \theta')
$$
it follows that the observed Fisher information equals
$$
\hat{i}_X := - D^2_{\theta} \ell(\hat{\theta}) =
\underbrace{-D^2_{\theta} Q(\hat{\theta} \mid \theta')}_{\hat{i}_Y(\theta')} -
\underbrace{D^2_{\theta} H(\hat{\theta} \mid \theta')}_{\hat{i}_{Y \mid X}(\theta')}.
$$

\medskip\pause

Introducing $\hat{i}_Y := \hat{i}_Y(\hat{\theta})$ and
$\hat{i}_{Y \mid X} = \hat{i}_{Y \mid X}(\hat{\theta})$. We have the
_information identity_
$$
\hat{i}_X = \hat{i}_Y - \hat{i}_{Y \mid X}.
$$

### Interpretation

- $\hat{i}_Y$ is the Fisher information for complete $Y$.
- $\hat{i}_{Y \mid X}$ is the information "lost" from not observing full $X$.

## More Identities

How to compute the information loss?

\medskip\pause

The second **Bartlett identity** can be reformulated as
$$
\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})
= - \partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})
$$
which follows from differentiation under the integral of
$\int h(y \mid x, \theta) \mu_x(d y) = 1.$

\medskip\pause

And since we also have
$$
Q(\theta \mid \theta') = \ell(\theta) - H(\theta \mid \theta')
$$
we find that
$$
\partial_{\theta'_i} \partial_{\theta_j} Q(\bar{\theta} \mid \bar{\theta}) =  -
\partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}) =
\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}).
$$

\medskip\pause

Thus
$$
\hat{i}_{Y \mid X} = D_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta}).
$$

## New Implementations

First we implement the map $Q$ as an R function.

```{r q-def}
Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  pp[3] <- 1 - pp[1] - pp[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p))
}
```

. . .

and a modified `grad_Q()` of two arguments:

```{r gradq-def}
grad_Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / (M(prob(pp), group)[group] * prob(p))) %*% Dprob(p)
}
```

## Numerical Differentiation of $Q$

We use numDeriv functions `jacobian()` and `hessian()` to differentiate
numerically.

```{r q-hess}
iY <- -hessian(Q, p_hat, pp = p_hat)
iY
```

\medskip\pause

```{r q-jacobian}
-jacobian(grad_Q, p_hat, pp = p_hat) # This should be the same as iY
```

. . .

```{r iyx-jacobian}
iYX <- jacobian(function(pp) grad_Q(p_hat, pp), p_hat)
iY - iYX # same as `emp_Fisher(p_hat)`
```

## Third Way: The EM-Mapping

Define $\Phi : \Theta \mapsto \Theta$ by
$$
\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta').
$$

\medskip\pause

A global maximum of the likelihood is a fixed point of $\Phi$,
$\Phi(\theta) = \theta.$

\medskip\pause

Since the limit of the EM algorithm, $\hat{\theta}$, is a fixed point and other
identities above, it can be shown that
$$
D_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.
$$

\medskip\pause

Hence
$$
\begin{aligned} \hat{i}_X & = \left(I - \hat{i}_{Y\mid X}
\left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y = \left(I - D_{\theta}
\Phi(\hat{\theta})^T\right) \hat{i}_Y. \end{aligned}
$$

\medskip\pause

$D_{\theta} \Phi(\hat{\theta})$ can be computed via numerical differentiation.

## EM Step

```{r}
Phi <- create_em_mult_step(c(85, 196, 341), c(1, 1, 1, 2, 2, 3), prob, X)
p_hat
Phi(p_hat) # The limit is a fixed point
```

## Differentiating the EM Map

```{r}
DPhi <- jacobian(Phi, p_hat) # Using numDeriv function 'jacobian()'
iX <- (diag(1, 2) - t(DPhi)) %*% iY
iX
ihat # Computed using numerical differentiation of grad_Q
```

# Gaussian Mixtures

## Finite Mixtures

Let $Z \in \{1, \ldots, K\}$ with $P(Z = k) = p_k$, and the conditional
distribution of $X$ given $Z = k$ has density $f_k( \cdot \mid \psi_k)$.

\medskip\pause

The joint density is
$$
(x, k) \mapsto f_k(x \mid \psi_k) p_k
$$

. . .

and the marginal density for the distribution of $X$ is
$$
f(x \mid \theta) =  \sum_{k=1}^K f_k(x \mid \psi_k) p_k.
$$

## Gaussian Mixtures $(K = 2)$

The two Gaussian distributions are parametrized by five parameters
$\mu_1, \mu_2 \in \mathbb{R}$ and $\sigma_1, \sigma_2 > 0$, and
$p = P(Z = 1) = 1 - P(Z = 2)$.

\medskip\pause

The conditional distribution of $X$ given $Z = k$ is
$$
f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.
$$

\medskip\pause

The marginal density is
$$
f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.
$$

## Simulation

:::: {.columns}

::: {.column width="47%"}

```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5

n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

:::

::: {.column width="47%"}

```{r, echo=FALSE}
gausdens <- function(x) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}
xx <- seq(-3, 11, 0.1)
hist(x, freq = FALSE, ylim = c(0, 0.25), main = "")
lines(xx, gausdens(xx), col = "red", lwd = 2)
```

:::

::::

## Log-likelihood

We assume $\sigma_1$ and $\sigma_2$ known.

```{r gaus-mix-loglik, cache = TRUE}
loglik <- function(par, x) {
  p <- par[1]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  mu1 <- par[2]
  mu2 <- par[3]

  -sum(log(
    p *
      exp(-(x - mu1)^2 / (2 * sigma1^2)) /
      sigma1 +
      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  ))
}
```

## Optimizing

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]
```

. . .

Again, however, initialization matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), loglik, x = x)[c(1, 2)]
```

## The $Q$-Function

The complete data log-likelihood is
$$
\sum_{i=1}^n 1(z_i = 1) \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + 1(z_i = 2)\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$

. . .

and
$$
Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$
where $\hat{p}_i = P_{\theta'}(Z_i = 1 \mid X_i = x_i)$.

\medskip\pause

The maximum is attained at
$$
\theta = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).
$$

## The E-Step

The conditional probability in a mixture model is generally
$$
P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}
$$

. . .

which for the $K = 2$ Gaussian case gives
$$
\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.
$$


## Implementation

See [`em_gauss_mix.R`](R/em_gauss_mix.R) for the source file of the
implementation.

```{r source-em-mix}
source(here::here("R", "em_gauss_mix.R"))
```

## EM

```{r run-em-gauss-mix}
e_step_gauss <- function(par, x, sigma1 = 1, sigma2 = sigma1) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  a <- p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1
  b <- (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  a / (a + b)
}

m_step_gauss <- function(p_hat, x) {
  n <- length(x)
  N1 <- sum(p_hat)
  N2 <- n - N1
  c(N1 / n, sum(p_hat * x) / N1, sum((1 - p_hat) * x) / N2)
}

EM_gauss_mix_step <- function(x, sigma1 = 1, sigma2 = sigma1) {
  force(x)
  force(sigma1)
  force(sigma2)

  function(par) m_step_gauss(e_step_gauss(par, x, sigma1, sigma2), x)
}

em_gauss_step <- EM_gauss_mix_step(x, sigma1, sigma2)
```

. . .

And testing

```{r test-em-gauss-mix}
em(c(0.5, -0.5, 4), em_gauss_step)
```

. . .

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]
```

. . .

```{r run-em-gauss-mix-again}
em(c(0.9, 3, 1), em_gauss_step) # Starting value still matters
```

## Gradients and Numerical Differentiation

```{r numderiv-grad1}
library(numDeriv)
grad1 <- function(par) grad(function(par) loglik(par, x), par)
```

. . .

```{r numderiv-grad2}
Q <- function(par, par_prime) {
  phat <- e_step_gauss(par_prime, x, sigma1, sigma2)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    phat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}
grad2 <- function(par) -grad(Q, par, par_prime = par)
```

## Gradient Identity

```{r gradient-identities1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

. . .

```{r gradient-identities2}
par_hat <- em(c(0.5, 0, 4), em_gauss_step)
grad1(par_hat)
grad2(par_hat)
```

## Convergence

```{r tracing-convergence}
library(CSwR)

em_tracer <- tracer(
  c("par0", "par", "loglik", "h_prime"),
  Delta = 0,
  expr = quote({
    loglik <- loglik(par, x)
    h_prime <- sum(grad2(par)^2)
  })
)
par_hat <- em(c(0.2, 2, 2), em_gauss_step, cb = em_tracer$tracer)
em_trace <- summary(em_tracer)
tail(em_trace, 4)
```

##

:::: {.columns}

::: {.column width="47%"}

```{r convergence-plots-1, warning=FALSE}
autoplot(
  em_trace,
  y = loglik - min(loglik)
)
```

:::

. . .

::: {.column width="47%"}

```{r convergence-plots-2, warning=FALSE}
autoplot(
  em_trace,
  y = h_prime
)
```

:::

::::

##

```{r mixture-convergence}
#| echo: false
gaussdens2 <- function(x, p, mu1, mu2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

em_gauss_illustration <- function(i, em_trace) {
  hist(x, freq = FALSE, ylim = c(0, 0.25), main = "", border = "transparent")
  lines(xx, gausdens(xx), col = "steelblue3", lwd = 2)
  p_i <- em_trace$par0.1[i]
  mu1_i <- em_trace$par0.2[i]
  mu2_i <- em_trace$par0.3[i]
  lines(xx, gaussdens2(xx, p_i, mu1_i, mu2_i), col = "dark orange", lwd = 2)
  text(9, 0.20, labels = paste0("k = ", i))
}
```

## 

```{r}
#| fig-width: 4.4
#| fig-height: 3.7
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(1, em_trace)
```

```{r}
#| fig-width: 4.4
#| fig-height: 3.7
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(2, em_trace)
```

## Summary

- We introduced the EM-algorithm, using the pepper moth example throughout
- We showed three ways to compute the Fisher information
- We covered Gaussian mixtures and how to optimize over their parameters using
  the EM algorithm
