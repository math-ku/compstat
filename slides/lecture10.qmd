---
title: "A Deeper Look into the EM Algorithm"
execute:
  cache: false
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
set.seed(1618)
library(patchwork)
library(testthat)

old_options <- options(digits = 4)
```

```{r moth_likelihood, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, pi_map, constraint = function(par) TRUE) {
  function(par) {
    pr <- pi_map(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

pi_map <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}

neg_noglik_obs <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  pi_map = pi_map,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), neg_noglik_obs)
```

## Last Time

### Likelihood Optimization

We introduced the Peppered Moths problem and solved the full likelihood using
general-purpose and constrained optimization.

\medskip\pause

We talked about the barrier methods for constrained optimization.

. . .

### The EM Algorithm

We introduced the EM algorithm as a general algorithm for likelihood
optimization when the likelihood is complicated.

## Recap: The EM Algorithm

The EM algorithm iteratively optimizes
$$
\theta^{(t+1)} = \argmax_\theta Q(\theta \mid \theta^{(t)}),
$$
with
$$
Q(\theta \mid \theta^{(t)}) = \E_{Z \mid X,\theta^{(t)}}\big(\log p(X, Z \mid \theta)\big).
$$

\medskip\pause

The algorithm monotonically increases the observed data log-likelihood at each
step.

## Today

### More on the EM Algorithm

We tackle the Peppered Moths example with the EM algorithm.

. . .

### Fisher Information

After we have optimized the likelihood, we often want to
estimate variance of estimates. For this we need the Fisher
information.

. . .

### Gaussian Mixture

We tackle an example of a mixture of Gaussians---the prototypical 
EM example.

## Recap: The Peppered Moths

:::: {.columns}

::: {.column width="47%"}

### Alleles

C, I, T with frequencies $p_C$, $p_I$, $p_T$ and
$$
p_C + p_I + p_T = 1.
$$

### Genotypes

CC, CI, CT, II, IT, TT

### Phenotypes

Black, Mottled, Light-Colored

:::

::: {.column width="47%"}

![](../images/peppered-moth.jpg){width=100%}


: Genotype to Phenotype Map

|       | **C** | **I**   | **T**         |
| ----- | ----- | ------- | ------------- |
| **C** | Black | Black   | Black         |
| **I** | Black | Mottled | Mottled       |
| **T** | Black | Mottled | Light-Colored |

:::

::::

## Latent Variables

### Genotype Labels

Let each moth $i=1,\ldots,n$ have an (unobserved) genotype label
$$
Z_i \in \{1,2,3,4,5,6\} \quad\text{(CC, CI, CT, II, IT, TT)}.
$$

. . .

Via the Hardy--Weinberg principle, the **genotype** probabilities are
$$
p = (p_C^2,\ 2p_Cp_I,\ 2p_Cp_T,\ p_I^2,\ 2p_Ip_T,\ p_T^2).
$$

. . .

### Genotype Counts

The genotype counts $Y_k$ are the sufficient statistics for the complete data,
and also latent, with
$$
Y_k = \sum_{i=1}^n \mathbf{1}(Z_i = k), \quad Y = (Y_1,\ldots,Y_6), \quad \sum_k Y_k = n.
$$

## Observed Data

What we actually observe are the **phenotype** counts
$$
X_j = \sum_{k \in A_j} Y_k,\quad X = M(Y).
$$
with partitions
$$
A_1=\{1,2,3\}\ (\text{Black}),\quad A_2=\{4,5\}\ (\text{Mottled}),\quad A_3=\{6\}\ (\text{Light}),
$$

. . .

Here, $M : \mathbb{N}_0^6 \to \mathbb{N}_0^3$ is the cell collapsing map
$$
M(y) = (y_1 + y_2 + y_3,\ y_4 + y_5,\ y_6).
$$

### Multinomial Distribution

If $Y \sim \operatorname{Mult}(p, n)$ with $p = (p_1, \ldots, p_K)$ then
$$
X = M(Y) \sim \operatorname{Mult}(M(p), n).
$$

## Complete-Data Log-Likelihood

Using the genotype counts $Y$, the complete-data log-likelihood is
$$
\ell(p \mid Y) = \sum_{k=1}^6 Y_k \log p_k.
$$

. . .

### Optimization Variable

For our EM algorithm, we will optimize over the **allele frequencies**
$$
\theta = (p_C, p_I), \qquad p_T = 1 - p_C - p_I.
$$

\medskip\pause

Let's define the function $\pi$, that maps $\theta$ to the genotype
probabilities
$$
\pi(\theta) =
  \begin{bmatrix}
    p_C^2 \\
    2 p_C p_I \\
    2 p_C (1 - p_C - p_I) \\
    p_I^2 \\
    2 p_I (1 - p_C - p_I) \\
    (1 - p_C - p_I)^2
  \end{bmatrix}
$$

## The Q Function

EM $Q$-function:
$$
\begin{aligned}
Q(\theta \mid \theta')
  &= \E_{Y \mid X, \theta'} \log p (Y \mid \theta) \\
  &= \sum_{k=1}^6 \E_{Y \mid X, \theta'}(Y_k) \log \pi_k(\theta) \\
  &= \sum_{k=1}^6 \frac{X_{j(k)} \pi_k(\theta')}{M(\pi(\theta'))_{j(k)}} \log \pi_k(\theta) \quad \text{where } k \in A_{j(k)}.
\end{aligned}
$$

. . .

Note that
$$
\E_{Y \mid X, \theta'}(Y_k) = \frac{X_{j(k)} \pi_k(\theta')}{M(\pi(\theta'))_{j(k)}}
$$
since
$Y \mid X, \theta' \sim \operatorname{Mult}\left(X_{j(k)}, \left( \frac{\pi_k(\theta')}{M(\pi(\theta'))_{j(k)}} \right)_{k \in A_{j(k)}}\right)$.

\pdfpcnote{
Intuition: If we know the phenotype counts X and the allele frequencies
theta', we can compute the expected genotype counts Y.
}

## E-step for Multinomial Model

```{r}
e_step_mult <- function(theta, x, group) {
  p <- pi_map(theta)
  x[group] * p / M(p, group)[group]
}
```

. . .

:::: {.columns}

::: {.column width="47%"}

```{r}
pi_map <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}
```

:::

. . .

::: {.column width="47%"}

```{r}
M <- function(y, group) {
  as.vector(
    tapply(y, group, sum)
  )
}
```

:::

::::


## M-Step for the Multinomial Model

We need to maximize $Q(\theta \mid \theta')$ w.r.t. $\theta$.

\medskip\pause

With $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete
observation, it can be shown that the MLE is
$$
\begin{aligned}
  \hat{p}_C & = \frac{n_{CC} + (n_{CI} + n_{CT}) / 2}{n} \\
  \hat{p}_I & = \frac{n_{II} + (n_{CI} + n_{IT}) / 2}{n}
\end{aligned}
$$
where $n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.

## M-Step Implementation

MLE of complete log-likelihood:

```{r mstep-mult}
m_step_mult <- function(y) {
  n <- sum(y)
  pC <- (y[1] + (y[2] + y[3]) / 2) / n
  pI <- (y[4] + (y[2] + y[5]) / 2) / n
  c(pC, pI)
}
```

## EM Step Function Factory

A **function factory** returns a function. Here, we create a function that
performs one EM step for the multinomial model.

. . .

```{r}
create_em_mult_step <- function(x, group) {
  force(x) # Force evaluation of x
  force(group)

  e_step <- function(theta) e_step_mult(theta, x, group)
  m_step <- function(y) m_step_mult(y)

  function(par) m_step(e_step(par))
}
```

## EM Algorithm Implementation

```{r EM-factory}
em <- function(par, em_step, eps = 1e-6, maxit = 20, cb = NULL) {
  for (i in seq_len(maxit)) {
    par0 <- par
    par <- em_step(par)

    if (!is.null(cb)) {
      cb() # Callback function for tracing, to be explained
    }

    if (sum((par - par0)^2) <= eps * (sum(par^2) + eps)) {
      break
    }
  }
  par
}
```

## Peppered Moths EM Algorithm

First, we create the EM step function.

```{r, results='hold'}
em_mult_step <- create_em_mult_step(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3)
)
```

. . .

Then we can run the EM algorithm.

```{r}
em(c(0.3, 0.3), em_mult_step)
moth_optim$par
```

## Inside the EM Algorithm

The `tracer()` function in the `CSwR` package can be used to trace the
iterations of an algorithm.^[Use `Delta` to control verbosity. `0`: no output.]

. . .

```{r}
library(CSwR)
em_tracer <- tracer("par", Delta = 0)
```

. . .

Each call to `tracer()` returns a S3 object of class tracer. The object has a
field `tracer`, which is a function that will record the current value of given
parameters each time it is called.

. . .

```{r}
em(c(0.3, 0.3), em_mult_step, cb = em_tracer$tracer)
```

##

You can then summarize the collected values using `summary()`.

```{r}
summary(em_tracer)
```

## Tracing the EM Algorithm

```{r em-tracer}
em_tracer <- tracer(c("par0", "par"), Delta = 0)
p_hat <- em(c(0.3, 0.3), em_mult_step, eps = 0, cb = em_tracer$tracer)
p_hat

em_trace <- summary(em_tracer)
```

##

```{r plot-curves, echo = FALSE, warning = FALSE}
#| fig-width: 5
#| fig-height: 3.4
#| fig-cap: "EM algorithm iterations for the Peppered Moths example. The orange cross marks
#|   the MLE. The surfaces are the current negative observed log-likelihoods, with the 
#|   black dot indicating the EM step."
nll_pep <- Vectorize(function(p1, p2) neg_noglik_obs(c(p1, p2)))

em_trace <- transform(
  em_trace,
  n = seq_len(nrow(em_trace)),
  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),
  nll = nll_pep(par.1, par.2)
)

group <- c(1, 1, 1, 2, 2, 3)

EStep <- function(theta) e_step_mult(theta, x, group)
MStep <- function(y) m_step_mult(y)

eps <- 1e-6
maxit <- 5

exp_vals <- matrix(NA, nrow = maxit, ncol = length(group))

par <- c(0.3, 0.3)

level_curves <- vector("list", length = maxit)

ll <- double(maxit)
x <- c(85, 196, 341)

n_mat <- 100
pC <- seq(0, 1, length.out = n_mat)
pI <- seq(0, 1, length.out = n_mat)

pars <- vector("list", length = maxit)

for (i in seq_len(maxit)) {
  par0 <- par
  y <- EStep(par)

  x_new <- M(y, group)
  z <- matrix(NA, n_mat, n_mat)

  x <- c(85, 196, 341)

  for (k in seq_len(n_mat)) {
    for (j in seq_len(n_mat)) {
      val <- -sum(y * log(pi_map(c(pC[k], pI[j]))))
      if (is.finite(val)) {
        z[k, j] <- val
      } else {
        z[k, j] <- -Inf
      }
    }
  }

  level_curves[[i]] <- z

  ll[i] <- -sum(y * log(pi_map(par)))

  exp_vals[i, ] <- y
  par <- MStep(y)
  pars[[i]] <- par
}

par_optim <- c(0.07084, 0.18874)

# Build long data frame for first 4 iterations
build_df <- function(z_mat, iter) {
  out <- data.frame(
    pC = rep(pC, times = length(pI)),
    pI = rep(pI, each = length(pC)),
    z = as.vector(z_mat),
    iter = iter
  )
  out
}

plot_iters <- 1:4
df_all <- do.call(
  rbind,
  lapply(plot_iters, function(i) build_df(level_curves[[i]], i))
)

# Points (current parameter in each iteration)
iter_points <- data.frame(
  pC = sapply(pars[plot_iters], `[[`, 1),
  pI = sapply(pars[plot_iters], `[[`, 2),
  iter = plot_iters
)

mle_point <- data.frame(
  pC = par_optim[1],
  pI = par_optim[2]
)

ggplot(df_all, aes(pC, pI)) +
  geom_contour(
    aes(z = z, color = after_stat(level)),
    breaks = seq(800, 3000, by = 200)
  ) +
  geom_point(data = mle_point, colour = "darkorange", size = 3, pch = 4) +
  geom_point(data = iter_points, colour = "black", size = 2) +
  facet_wrap(~iter, labeller = labeller(iter = function(i) paste0("k = ", i))) +
  coord_equal() +
  guides(color = "none") +
  labs(x = expression(p[C]), y = expression(p[I])) +
  theme(panel.spacing = unit(0.8, "lines"))
```

##

```{r tracer-plot1, echo = FALSE}
#| fig-height: 2.7
#| fig-width: 3.5
#| fig-cap: "Parameter convergence of the EM algorithm applied to the Peppered
#|   moths problem."
ggplot(em_trace, aes(n, par_norm_diff)) +
  geom_point() +
  geom_line() +
  labs(
    y = expression(paste(
      "||",
      theta^{
        (n)
      } -
        theta^{
          (n - 1)
        },
      "||"
    ))
  ) +
  scale_y_log10()
```

##

```{r loglikelihood-convergence, warning=FALSE, echo = FALSE, fig.cap="Log-likelihood convergence"}
#| fig-width: 4
#| fig-height: 2.5
mutate(em_trace, diff_nll = 1e-14 + nll - min(nll)) |>
  ggplot(aes(n, diff_nll)) +
  labs(y = expression(l[theta] - min(l))) +
  geom_point() +
  geom_line() +
  scale_y_log10()
```

## Multi-Start

A common problem in likelihood optimization is local maxima. 

\medskip\pause

We can solve this using a grid of starting values.^[`neg_loglik_obs()` computes
the negative observed log-likelihood.]

```{r}
theta_grid <- expand.grid(
  pC = seq(0.01, 0.49, length.out = 5),
  pI = seq(0.01, 0.49, length.out = 5)
)

res <- apply(theta_grid, 1, em, em_step = em_mult_step)
neg_logliks <- apply(res, 2, neg_noglik_obs)

which.min(neg_logliks)
```

# Fisher Information

## The Fisher Information

When doing statistical inference, we need to measure the uncertainty of
estimates.

\medskip\pause

The Fisher information is a key quantity for this. It is defined as
$$
i(\theta) = - \E\left( \frac{\partial^2}{\partial \theta^2} \log p(X \mid \theta) \right).
$$

. . .

This is the expected curvature of the log-likelihood.

### Complete, Observed, and Missing Information

In the presence of latent variables, we can define three types of Fisher
information:

- **Complete-data information** $i_Y(\theta)$
- **Observed-data information** $i_X(\theta)$
- **Missing information** $i_{Y \mid X}(\theta) = i_Y(\theta) - i_X(\theta)$

```{r}
#| echo: false
pi_grad <- function(p) {
  matrix(
    c(
      2 * p[1],
      0,
      2 * p[2],
      2 * p[1],
      2 * p[3] - 2 * p[1],
      -2 * p[1],
      0,
      2 * p[2],
      -2 * p[2],
      2 * p[3] - 2 * p[2],
      -2 * p[3],
      -2 * p[3]
    ),
    ncol = 2,
    nrow = 6,
    byrow = TRUE
  )
}

score <- function(theta, x = c(85, 196, 341)) {
  theta <- c(theta, 1 - sum(theta))
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] / M(pi_map(theta), group)[group]) %*% pi_grad(theta)
}
```

## Empirical Fisher Information

The empirical observed Fisher information is defined as
$$
\hat{i}(\theta) = -\frac{\partial^2}{\partial \theta^2} \log p(X \mid \theta)
$$
where $\ell_i(\theta)$ is the log-likelihood of observation $i$.

\medskip\pause

In the EM context, we typically do not have access to the individual
log-likelihood contributions $\ell_i(\theta)$, but we can solve this through
other means.

. . .

### Three Ways of Computing the Observed Fisher Information

1. The **gradient identity**: For the score function from the Q function, then
   differentiate.\pause
2. The **information identity**: Estimate complete and missing information,
   then subtract.\pause
3. The **EM-mapping**: Differentiate the EM-mapping, which is a fixed point at the
   MLE.

## First Way: Via the Gradient Identity

Recall that the Q function is
$$
Q(\theta \mid \theta') = \E_{Y \mid X, \theta'} \log p(X, Y \mid \theta).
$$

\medskip\pause

At convergence, $\theta' = \hat{\theta}$ (the MLE of the observed data
log-likelihood).

\medskip\pause

If we differentiate once, we have
$$
\nabla_\theta Q(\theta \mid \theta') = \E_{Y \mid X, \theta'} \nabla_\theta \log p(X, Y \mid \theta).
$$

\medskip\pause

Setting $\theta'=\theta$ gives Fisher's (score) identity
$$
\nabla_\theta Q(\theta \mid \theta)
= \E_{Y \mid X,\theta}\nabla_{\theta} \log p(X, Y \mid \theta)
= \nabla_\theta \log p(X \mid \theta).
$$

## Gradient of $Q$ for the Multinomial Model

For the multinomial model, the Q function is 
$$
Q(\theta \mid \theta')  = \sum_{i} \frac{x_{j(i)} \pi_i(\theta')}{M(\pi(\theta'))_{j(i)}} \log \pi_i(\theta),
$$
where $j(i)$ is defined by $i \in A_{j(i)}$.

\medskip\pause

The gradient is
$$
\nabla_{\theta} Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)}\pi_i(\theta')}{M(\pi(\theta'))_{j(i)}\pi_i(\theta)} \nabla \pi_i(\theta).
$$

\medskip\pause

When evaluated in $\theta$ it is
$$
\nabla_{\theta} Q(\theta \mid \theta) = \sum_{i} \frac{x_{j(i)}}{M(\pi(\theta))_{j(i)}} \nabla \pi_i(\theta).
$$


## Numerical Validation of the Gradient Identity

```{r p-hat-call}
theta_hat <- em(c(0.3, 0.3), em_mult_step, eps = 1e-20)
```

:::: {.columns}

::: {.column width="47%"}

```{r p-hat}
theta_hat
```

:::

. . .

::: {.column width="47%"}

```{r grad-q}
score(theta_hat)
```

:::

::::


## Numerical Fisher information

We could differentiate analytically, but a simpler way is to use numerical
differentiation.

. . .

:::: {.columns}

::: {.column width="47%"}

### `stats::optimHess()`

```{r}
-optimHess(
  theta_hat,
  neg_noglik_obs,
  score
)
```

:::

. . .

::: {.column width="47%"}

### `numDeriv::jacobian()`

```{r}
library(numDeriv)
ihat <- -jacobian(
  score,
  theta_hat
)
ihat
```

:::

::::

## Second Way: The Information Identity

Recall that with $\theta'=\theta$,
$$
\nabla_\theta Q(\theta \mid \theta) = \nabla_\theta \log p(X \mid \theta).
$$

\pause

Differentiating again (w.r.t to $\theta$) gives
$$
-\nabla_\theta^2 Q(\theta \mid \theta') = -\E_{Y \mid X, \theta'} \nabla_\theta^2 \log p(X, Y \mid \theta).
$$

. . .

At $\theta'=\theta$, using $\log p(X, Y \mid \theta) = \log p(X \mid \theta) +
\log p(Y \mid X, \theta)$, we have
$$
-\nabla_\theta^2 Q(\theta \mid \theta) = -\nabla_\theta^2 \log p(X\mid \theta) - \E_{Y \mid X,\theta}\nabla_\theta^2 \log p(Y \mid X,\theta).
$$

. . .

Taking expectation also over $X$ recovers the decomposition
$$
i_Y(\theta)= i_X(\theta)+ i_{Y\mid X}(\theta),
$$
with
$$
\begin{aligned}
  i_Y(\theta)&= -\E_{X,Y\mid\theta}\nabla_\theta^2 \log p(X,Y\mid\theta),                           \\
  i_X(\theta)&= -\E_{X\mid\theta}\nabla_\theta^2 \log p(X\mid\theta),                               \\
  i_{Y\mid X}(\theta)&= -\E_{X\mid\theta}\E_{Y\mid X,\theta}\nabla_\theta^2 \log p(Y\mid X,\theta).
\end{aligned}
$$

## The Information Identity

At the MLE $\hat{\theta}$,
$$
\hat{i}_X := - \nabla^2_{\theta} \ell(\hat{\theta}) =
\underbrace{-\nabla^2_{\theta} Q(\hat{\theta} \mid \hat\theta)}_{\hat{i}_Y} -
\underbrace{\nabla^2_{\theta} H(\hat{\theta} \mid \hat\theta)}_{\hat{i}_{Y \mid X}}.
$$

\medskip\pause

Thus we have the **information identity** (Louis identity):
$$
\hat{i}_X = \hat{i}_Y - \hat{i}_{Y \mid X}.
$$

. . .

### Interpretation

- $\hat{i}_Y$ is the Fisher information for complete $Y$.
- $\hat{i}_{Y \mid X}$ is the information "lost" from not observing full $X$.

## The Complete-Data Information


## Complete and Missing Data

We can compute the complete-data information directly from the $Q$ function, but
how do we compute the missing information?

\medskip\pause

The second **Bartlett identity** can be used to show that
$$
\nabla_{\theta} \nabla_{\theta} H(\hat{\theta} \mid \hat{\theta})
= - \nabla_{\theta'} \nabla_{\theta} H(\hat{\theta} \mid \hat{\theta}).
$$

\medskip\pause

And since we also have
$$
Q(\theta \mid \theta') = \ell(\theta) - H(\theta \mid \theta')
$$
and because $\ell(\theta)$ does not depend on $\theta'$, we get
$$
\nabla_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta}) =  -
\nabla_{\theta'} \nabla_{\theta} H(\hat{\theta} \mid \hat{\theta}) =
\nabla_{\theta} \nabla_{\theta} H(\hat{\theta} \mid \hat{\theta}).
$$

\medskip\pause

Thus
$$
\hat{i}_{Y \mid X} = \nabla_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta}).
$$

## Implementations of $Q$

First we implement the map $Q$ as an R function.

```{r q-def}
Q <- function(theta, theta0, x = c(85, 196, 341)) {
  theta[3] <- 1 - theta[1] - theta[2]
  theta0[3] <- 1 - theta0[1] - theta0[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * pi_map(theta0) / M(pi_map(theta0), group)[group]) %*%
    log(pi_map(theta))
}
```

## Implementation of Gradient of $Q$

Next, we implement the gradient for Q:

```{r gradq-def}
Q_grad <- function(theta, theta0, x = c(85, 196, 341)) {
  theta[3] <- 1 - theta[1] - theta[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] *
    pi_map(theta0) /
    (M(pi_map(theta0), group)[group] * pi_map(theta))) %*%
    pi_grad(theta)
}
```

## Numerical Differentiation of $Q$

```{r q-hess}
iY <- -hessian(Q, theta_hat, theta0 = theta_hat)
iY
```

. . .

```{r iyx-jacobian}
iYX <- jacobian(function(theta) Q_grad(theta_hat, theta), theta_hat)
iX <- iY - iYX
```

## Third Way: The EM-Mapping

Define $\Phi : \Theta \mapsto \Theta$ by
$$
\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta').
$$

\medskip\pause

A global maximum of the likelihood is a fixed point of $\Phi$,
$\Phi(\hat{\theta}) = \hat{\theta}.$

\medskip\pause

Since the limit of the EM algorithm, $\hat{\theta}$, is a fixed point and other
identities above, it can be shown that
$$
\nabla_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.
$$

\medskip\pause

Hence
$$
\begin{aligned} \hat{i}_X & = \left(I - \hat{i}_{Y\mid X}
\left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y = \left(I - \nabla_{\theta}
\Phi(\hat{\theta})^T\right) \hat{i}_Y. \end{aligned}
$$

## EM Step Fixed Point

```{r}
Phi <- create_em_mult_step(c(85, 196, 341), c(1, 1, 1, 2, 2, 3))
theta_hat
```

. . .

Let's check if this is fixed point.

```{r}
test_that("Fixed point", {
  expect_equal(Phi(theta_hat), theta_hat, tol = 1e-8)
})
```

## Differentiating the EM Map

```{r}
DPhi <- jacobian(Phi, theta_hat)
iX <- (diag(2) - t(DPhi)) %*% iY
iX
```

. . .

Let's check that all of our methods agree.

```{r}
test_that("Methods 1, 2, and 3 are equivalent", {
  expect_equal(iX, ihat, tol = 1e-6)
  expect_equal(iX, iY - iYX, tol = 1e-6)
})
```

## 

```{r fisher-ellipses, warning=FALSE, message=FALSE}
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| fig-cap: "95% Fisher information ellipses for the observed data (solid) and
#|   complete data (dashed) for the Peppered moths example. The MLE is
#|   indicated by the cross."
# Requires: theta_hat, iY, iX already computed above
theta_hat <- em(c(0.3, 0.3), em_mult_step, eps = 1e-12)
iY <- -hessian(Q, theta_hat, theta0 = theta_hat)
iYX <- jacobian(function(theta) Q_grad(theta_hat, theta), theta_hat)
iX <- iY - iYX

alpha <- 0.05

ellipse_points <- function(I, center, level = 0.95, n = 200) {
  Sigma <- solve(I)
  eig <- eigen(Sigma, symmetric = TRUE)
  A <- eig$vectors %*% diag(sqrt(pmax(eig$values, 0)))
  r <- sqrt(qchisq(level, df = 2))
  t <- seq(0, 2 * pi, length.out = n)
  pts <- t(sapply(t, function(phi) center + r * A %*% c(cos(phi), sin(phi))))
  colnames(pts) <- c("pC", "pI")
  as.data.frame(pts)
}

clip_simplex <- function(df) {
  subset(df, pC >= 0 & pI >= 0 & pC + pI <= 1)
}

df_obs <- clip_simplex(transform(
  ellipse_points(iX, theta_hat, 1 - alpha / 2),
  Type = "Observed (i_X)"
))
df_comp <- clip_simplex(transform(
  ellipse_points(iY, theta_hat, 1 - alpha / 2),
  Type = "Complete (i_Y)"
))

simplex_boundary <- data.frame(
  pC = c(0, 1, 0, 0),
  pI = c(0, 0, 1, 0)
)

theta_df <- data.frame(pC = theta_hat[1], pI = theta_hat[2])

ggplot() +
  geom_path(data = df_obs, aes(pC, pI, color = Type)) +
  geom_path(
    data = df_comp,
    aes(pC, pI, color = Type),
    linetype = 2
  ) +
  geom_point(
    data = theta_df,
    aes(pC, pI),
    color = "darkorange",
    pch = 4,
    size = 3
  ) +
  coord_equal() +
  lims(x = c(0, 0.15), y = c(0.1, 0.25)) +
  scale_color_manual(
    values = c("Observed (i_X)" = "#1b6ca8", "Complete (i_Y)" = "#d95f02"),
    labels = c(expression(hat(i)[X]), expression(hat(i)[Y]))
  ) +
  labs(
    x = expression(p[C]),
    y = expression(p[I]),
    color = "Ellipse"
  )
```

## Convergence of the EM Algorithm

We have already shown that the EM algorithm monotonically increases the observed
data log-likelihood:
$$
\ell(\theta^{(t+1)}) \ge \ell(\theta^{(t)}).
$$

. . .

We have also just shown that the EM
algorithm converges to a fixed point of the EM mapping:
$$
\theta^{(t+1)} = \Phi(\theta^{(t)}), \quad \hat{\theta} = \Phi(\hat{\theta}).
$$

. . .

By the Taylor expansion of $\Phi$ around $\hat{\theta}$, we have
$$
\theta^{(t+1)} - \hat{\theta} \approx \nabla \Phi(\hat{\theta}) (\theta^{(t)} - \hat{\theta})
$$

. . .

So the error at step $t+1$ is approximately a linear transformation of the error at
step $t$.

\medskip\pause

And the rate of convergence is determined by the matrix $\nabla \Phi(\hat{\theta})$.

## Convergence: Connection to Fisher Information

Let $\rho(A)$ be the spectral radius of a matrix $A$, i.e., the largest absolute value of its eigenvalues.

\medskip\pause

This means that the convergence rate of the EM algorithm is governed by
$$
\rho(\nabla \Phi(\hat{\theta})) = \rho\left(\hat{i}_{Y \mid X} \left(\hat{i}_Y\right)^{-1}\right).
$$

\medskip\pause

If $\rho(\nabla \Phi(\hat{\theta}))$ is close to 0, the EM algorithm converges
quickly, and if it is close to 1, the EM algorithm converges slowly.

. . .

### Relation to Missing Information

If there is little missing information, i.e., $\hat{i}_{Y \mid X}$ is small,
then $\rho(\nabla \Phi(\hat{\theta}))$ is small and the EM algorithm converges
quickly.  

## Convergence Estimation

The EM algorithm converges linearly near the optimum:  
$$
\lVert\theta^{(t+1)} - \hat{\theta}\rVert \approx \rho \lVert\theta^{(t)} - \hat{\theta}\rVert, \qquad 0 < \rho < 1.
$$

\medskip\pause

Rate can be estimated by fitting a linear model to the log of the parameter differences:
$$
\log \lVert\theta^{(t+1)} - \hat{\theta}\rVert \approx \log \rho + \log \lVert \theta^{(t)} - \hat{\theta}\rVert
\implies \log \lVert \theta^{(t)} - \hat{\theta}\rVert \approx t \log \rho + \text{constant}.
$$

. . .

```{r linear-convergence}
log_lm <- lm(log(par_norm_diff) ~ n, data = em_trace)
exp(coefficients(log_lm)["n"]) # n: iteration (t above)
```

\medskip\pause

It is small in this case, implying fast convergence.

## The Generalized EM Algorithm

:::: {.columns}

::: {.column width="47%"}

### Key Idea

Relax the M-step: instead of fully maximizing
$Q(\theta \mid \theta^{(t)})$, we just require an **increase**:
$$
Q(\theta^{(t+1)} \mid \theta^{(t)}) \ge Q(\theta^{(t)} \mid \theta^{(t)}).
$$

. . .

Full maximization may be **hard or computationally expensive**. GEM allows
**simpler updates** (e.g., gradient step, coordinate-wise update).

. . .

:::

::: {.column width="47%"}

### Properties

E-step is unchanged and the **log-likelihood is still non-decreasing**: 
$$
\ell(\theta^{(t+1)}) \ge \ell(\theta^{(t)}).
$$

\medskip\pause

Still converges to a **stationary point** of the likelihood.

. . .

### Trade-Off

We trade slower convergence for faster iterations.

:::

::::

# Gaussian Mixtures

## Finite Mixtures

A finite mixture is 

Let $Z
\in \{1, \ldots, K\}$ with $P(Z = k) =
p_k$, and the conditional
distribution of $X$ given $Z
= k$ has density $f_k( \cdot \mid
\psi_k)$.

\medskip\pause

The joint density is
$$
f_k(x \mid \psi_k) p_k
$$

. . .

and the marginal density for the distribution of $X$ is
$$
f(x \mid \theta) =  \sum_{k=1}^K f_k(x \mid \psi_k) p_k.
$$

## Gaussian Mixtures $(K
= 2)$

The two Gaussian distributions are parametrized by five parameters
$\mu_1,
\mu_2 \in \mathbb{R}$ and $\sigma_1, \sigma_2 > 0$, and
$p = P(Z = 1) = 1
- P(Z = 2)$.

\medskip\pause

The conditional distribution of $X$ given $Z =
k$ is
$$
f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.
$$

\medskip\pause

The marginal density is
$$
f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.
$$

## Simulation

:::: {.columns align="center"}

::: {.column width="53%"}

```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

:::

::: {.column width="41%"}

```{r, echo=FALSE}
#| fig-cap: "Histogram of simulated data from a two-component Gaussian mixture model."
gausdens <- function(x, mu1, mu2, sigma1, sigma2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

xx <- seq(-3, 11, 0.1)

pl <- ggplot(data.frame(x), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightgray") +
  geom_line(
    aes(x, gausdens(x, mu1, mu2, sigma1, sigma2)),
    color = "black",
    linewidth = 1
  ) +
  labs(y = "Density", x = "x")

pl
```

:::

::::

## Log-likelihood

We assume $\sigma_1$ and $\sigma_2$ known.

```{r gaus-mix-loglik}
neg_loglik <- function(par, x) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  -sum(log(
    p *
      exp(-(x - mu1)^2 / (2 * sigma1^2)) /
      sigma1 +
      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  ))
}
```

## General-Purpose Optimization

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), neg_loglik, x = x)[1:2]
```

. . .

Again, however, initialization matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), neg_loglik, x = x)[1:2]
```

## The $Q$-Function

The complete data log-likelihood is
$$
\sum_{i=1}^n \symbf{1}_{z_i = 1} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + \symbf{1}_{z_i = 2}\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$

. . .

and
$$
Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$
where $\hat{p}_i
= P_{\theta'}(Z_i = 1 \mid X_i =
x_i)$.

\medskip\pause

The maximum is attained at
$$
\hat{\theta} = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).
$$

## The E-Step

The conditional probability in a mixture model is generally
$$
P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}
$$

. . .

which for the $K
= 2$ Gaussian case gives
$$
\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.
$$


## Implementation

See the source code for an implementation.

```{r source-em-mix}
#| echo: false
e_step_gauss <- function(par, x, sigma1 = 1, sigma2 = sigma1) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  a <- p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1
  b <- (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  a / (a + b)
}

m_step_gauss <- function(p_hat, x) {
  n <- length(x)
  N1 <- sum(p_hat)
  N2 <- n - N1
  c(N1 / n, sum(p_hat * x) / N1, sum((1 - p_hat) * x) / N2)
}

create_em_gauss_step <- function(x, sigma1 = 1, sigma2 = sigma1) {
  force(x)
  force(sigma1)
  force(sigma2)

  function(par) m_step_gauss(e_step_gauss(par, x, sigma1, sigma2), x)
}

em_gauss_step <- create_em_gauss_step(x, sigma1, sigma2)
```

## Testing

```{r test-em-gauss-mix}
em(c(0.5, -0.5, 4), em_gauss_step)
```

. . .

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), neg_loglik, x = x)[1:2]
```

. . .

```{r run-em-gauss-mix-again}
em(c(0.9, 3, 1), em_gauss_step) # Starting value still matters
```

## Gradients and Numerical Differentiation

```{r numderiv-grad1}
grad1 <- function(par) grad(function(par) neg_loglik(par, x), par)
```

. . .

```{r numderiv-grad2}
Q <- function(par, par_prime) {
  p_hat <- e_step_gauss(par_prime, x, sigma1, sigma2)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    p_hat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - p_hat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}
grad2 <- function(par) -grad(Q, par, par_prime = par)
```

## Gradient Identity

```{r gradient-identities1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

. . .

```{r gradient-identities2}
par_hat <- em(c(0.5, 0, 4), em_gauss_step)
grad1(par_hat)
grad2(par_hat)
```

## Convergence

```{r tracing-convergence}
em_tracer <- tracer(
  c("par0", "par", "nll", "h_prime"),
  Delta = 0,
  expr = quote({
    nll <- neg_loglik(par, x)
    h_prime <- sum(grad2(par)^2)
  })
)

par_hat <- em(c(0.2, 2, 2), em_gauss_step, cb = em_tracer$tracer)

em_trace <- summary(em_tracer)
```

##

:::: {.columns}

::: {.column width="47%"}

```{r convergence-plots-1, warning=FALSE}
autoplot(
  em_trace,
  y = nll - min(nll)
)
```

:::

. . .

::: {.column width="47%"}

```{r convergence-plots-2, warning=FALSE}
autoplot(
  em_trace,
  y = h_prime
)
```

:::

::::

##

```{r mixture-convergence}
#| echo: false
gaussdens2 <- function(x, p, mu1, mu2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

em_gauss_illustration <- function(i, em_trace) {
  p_i <- em_trace$par0.1[i]
  mu1_i <- em_trace$par0.2[i]
  mu2_i <- em_trace$par0.3[i]

  pl +
    geom_line(
      aes(y = gaussdens2(x, p_i, mu1_i, mu2_i)),
      color = "darkorange",
      size = 1
    ) +
    lims(y = c(0, 0.25)) +
    labs(title = paste0("k = ", i))
}
```

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(1, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(4, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(8, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(12, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(16, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(20, em_trace)
```

## Label Switching

If we set $\sigma_1 = \sigma_2$, then the likelihood is invariant to
switching the labels of the components, i.e., $(p, \mu_1, \mu_2)$ and
$(1-p, \mu_2, \mu_1)$ give the same likelihood.

\medskip\pause

So the EM algorithm may converge to
**either** of the two symmetric modes.

```{r mixture-gaus-sim2}
#| echo: false
sigma1 <- 1
sigma2 <- 1
mu1 <- -0.5
mu2 <- 4
p <- 0.8
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)

em_gauss_step_alt <- create_em_gauss_step(x, sigma1)
```

. . .

```{r}
em(c(0.2, 3, -0.5), em_gauss_step_alt)
```

. . .

```{r}
em(c(0.8, -3, -0.5), em_gauss_step_alt)
```

. . .

In this example, we generated data with $p = 0.8$ and $\sigma_1 = \sigma_2$.

## Exercise

Implement the EM algorithm for a missing data problem:

```{r}
x <- c(1, 0, 1, NA, 0, NA, 1, 1, 0)
```

Assume that $X_i \sim \text{Bernoulli}(\theta)$ i.i.d. and that the data are
missing at random. The complete data log-likelihood is
$$
\ell_C(\theta) = \sum_{i=1}^n \left( X_i \log(\theta) + (1 - X_i) \log(1 - \theta) \right).
$$

. . .

### Step 1 (Derive)

- E-step: Compute $Q(\theta \mid \theta') = \E_{X_\text{miss} \mid X_\text{obs}, \theta'}\left( \log p(X_\text{obs}, X_\text{miss} \mid \theta)\right)$.
- M-step: Maximize $Q(\theta \mid \theta')$ over $\theta$.

. . .

### Step 2 (Implement)

Implement the EM algorithm and run it from two different starting points. Record
the path of $\theta^{(t)}$ and the log-likelihood values $\ell(\theta^{(t)})$.

```{r}
#| include: false
x <- c(1, 0, 1, NA, 0, NA, 1, 1, 0)

# EM for Bernoulli with missing entries (treated as latent)
em_bern_missing <- function(x, p0 = 0.5, maxit = 6) {
  obs <- !is.na(x)
  ll <- function(p) sum(x[obs] * log(p) + (1 - x[obs]) * log(1 - p))
  p <- p0
  ps <- numeric(maxit + 1)
  lls <- numeric(maxit + 1)

  for (t in 0:maxit) {
    ps[t + 1] <- p
    lls[t + 1] <- ll(p)
    if (t < maxit) {
      # E-step: expected value of each missing Xi is p
      k <- sum(!obs)
      p <- (sum(x[obs]) + k * p) / length(x) # M-step: mean of completed data
    }
  }
  list(p_final = p, path = ps, loglik = lls, loglik_diff = diff(lls))
}

# Run from two starts
res1 <- em_bern_missing(x, p0 = 0.5)
res2 <- em_bern_missing(x, p0 = 0.1)

res1$path
res1$loglik_diff # ≥ 0 (monotone)
res2$path

# Closed-form fixed point: solves p = (sum(obs 1s) + 2 p)/9 -> p = 4/7
# Fisher information:
p_hat <- res1$p_final
n_obs <- sum(!is.na(x))
i_obs <- n_obs / (p_hat * (1 - p_hat)) # observed-data info
i_complete <- length(x) / (p_hat * (1 - p_hat)) # complete-data info
c(p_hat = p_hat, i_obs = i_obs, i_complete = i_complete)
```

## Summary

We introduced the EM algorithm, using the Peppered Moths example throughout

\medskip\pause 

We showed three ways to compute the Fisher information

\medskip\pause

We covered Gaussian mixtures and how to optimize over their parameters using
the EM algorithm.
