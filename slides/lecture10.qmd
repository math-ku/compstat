---
title: "The EM-Algorithm"
execute:
  cache: false
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
set.seed(1618)
library(patchwork)
library(testthat)

old_options <- options(digits = 4)
```

```{r moth_likelihood, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, prob, constraint = function(par) TRUE) {
  function(par) {
    pr <- prob(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}

loglik <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), loglik)
```

## Last Time

### Likelihood Optimization

We introduced the Peppered Moths problem and solved the full likelihood using
general-purpose and constrained optimization.

\medskip\pause

We talked about the barrier methods for constrained optimization.

. . .

### The EM Algorithm

We introduced the EM-algorithm as a general algorithm for likelihood
optimization when the likelihood is complicated.

## Recap: The EM Algorithm

The EM-algorithm iteratively optimizes
$$
\theta^{(t+1)} = \argmax_\theta Q(\theta \mid \theta^{(t)}),
$$
with
$$
Q(\theta \mid \theta^{(t)}) = \E_{Z \mid X,\theta^{(t)}}\big(\log p(X, Z \mid \theta)\big).
$$

\medskip\pause

The algorithm monotonically increases the observed data log-likelihood at each
step.

## Today

### More on the EM Algorithm

Examples on Gaussian mixtures and the multinomial model.

. . .

### Fisher Information

When the likelihood is optimized, the Fisher information gives the variance of
the MLE.

. . .

### Gaussian Mixtures

Example of a finite mixture model where the EM-algorithm is useful.

## Recap: The Peppered Moth

:::: {.columns}

::: {.column width="47%"}

### Alleles

C, Ci, T with frequencies $p_C$, $p_I$, $p_T$ and
$$
p_C + p_I + p_T = 1.
$$

### Genotypes

CC, CI, CT, II, IT, TT

### Phenotypes

Black, mottled, light-colored

:::

::: {.column width="47%"}

![](../images/peppered-moth.jpg){width=100%}


: Genotype to Phenotype Map

|       | **C** | **I**   | **T**         |
| ----- | ----- | ------- | ------------- |
| **C** | Black | Black   | Black         |
| **I** | Black | Mottled | Mottled       |
| **T** | Black | Mottled | Light-colored |

:::

::::

## Latent Variables

### Genotype Lables

Let each moth $i=1,\ldots,n$ have an (unobserved) genotype label
$$
Z_i \in \{1,2,3,4,5,6\} \quad\text{(CC, CI, CT, II, IT, TT)}.
$$

. . .

Via the Hardy--Weinberg principle, the **genotype** probabilities are
$$
p = (p_C^2,\ 2p_Cp_I,\ 2p_Cp_T,\ p_I^2,\ 2p_Ip_T,\ p_T^2).
$$

. . .

### Genotype Counts

The genotype counts $Y_k$ are the sufficient statistics for the complete data,
and also latent, with
$$
Y_k = \sum_{i=1}^n \mathbf{1}(Z_i = k), \quad Y = (Y_1,\ldots,Y_6), \quad \sum_k Y_k = n.
$$

## Observed Data

What we actually observe are the **phenotype** counts
$$
X_j = \sum_{k \in A_j} Y_k,\quad X = M(Y).
$$
with partitions
$$
A_1=\{1,2,3\}\ (\text{Black}),\quad A_2=\{4,5\}\ (\text{Mottled}),\quad A_3=\{6\}\ (\text{Light}),
$$

. . .

Here, $M : \mathbb{N}_0^6 \to \mathbb{N}_0^3$ is the cell collapsing map
$$
M(y) = (y_1 + y_2 + y_3,\ y_4 + y_5,\ y_6).
$$

### Multinomial Distribution

If $Y \sim \operatorname{Mult}(p, n)$ with $p = (p_1, \ldots, p_K)$ then
$$
X = M(Y) \sim \operatorname{Mult}(M(p), n).
$$

## Complete-Data Log-Likelihood

Using the genotype counts $Y$, the complete-data log-likelihood is
$$
\ell(p \mid Y) = \sum_{k=1}^6 Y_k \log p_k.
$$

## Optimization Variable

For our EM algorithm, we will optimize over the **allele frequencies**
$$
\theta = (p_C, p_I)
$$
with $p_T = 1 - p_C - p_I$.

\medskip\pause

Let's define the function $\pi$, that maps $\theta$ to the genotype probabilities
$$
\pi(\theta) = 
  \begin{bmatrix}
    p_C^2 \\
    2 p_C p_I \\
    2 p_C (1 - p_C - p_I) \\
    p_I^2 \\
    2 p_I (1 - p_C - p_I) \\
    (1 - p_C - p_I)^2
  \end{bmatrix}
$$

## The Q Function

EM $Q$-function:
$$
\begin{aligned}
Q(\theta \mid \theta')
  &= \E_{Y \mid X, \theta'} \log p (Y \mid \theta) \\
  &= \sum_{k=1}^6 \E_{Y \mid X, \theta'}(Y_k) \log \pi_k(\theta) \\
  &= \sum_{k=1}^6 \frac{X_{j(k)} \pi_k(\theta')}{M(\pi(\theta'))_{j(k)}} \log \pi_k(\theta) \quad \text{where } k \in A_{j(k)}.
\end{aligned}
$$

. . .

Note that 
$$
\E_{Y \mid X, \theta'}(Y_k) = \frac{X_{j(k)} \pi_k(\theta')}{M(\pi(\theta'))_{j(k)}}
$$
since $Y \mid X, \theta' \sim \operatorname{Mult}(X_{j(k)}, ( \pi_k(\theta') / M(\pi(\theta'))_{j(k)} )_{k \in A_{j(k)}})$.

## E-step for Multinomial Model

```{r}
e_step_mult <- function(theta, x, group) {
  p <- pi_map(theta)
  x[group] * p / M(p, group)[group]
}
```

. . .

```{r}
pi_map <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}
```

## M-Step for the Multinomial Model

We need to maximize $Q(\theta \mid \theta')$ w.r.t. $\theta$.

\medskip\pause

With $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete
observation, it can be shown that the MLE is
$$
\begin{aligned}
  \hat{p}_C & = \frac{n_{CC} + (n_{CI} + n_{CT}) / 2}{n} \\
  \hat{p}_I & = \frac{n_{II} + (n_{CI} + n_{IT}) / 2}{n}
\end{aligned}
$$
where $n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.

## M-Step Implementation

MLE of complete log-likelihood:

```{r mstep-mult}
m_step_mult <- function(y) {
  n <- sum(y)
  pC <- (y[1] + (y[2] + y[3]) / 2) / n
  pI <- (y[4] + (y[2] + y[5]) / 2) / n
  c(pC, pI)
}
```

## Factory for the EM Step

A function factory returns 


```{r}
create_em_mult_step <- function(x, group) {
  force(x)
  force(group)

  e_step <- function(theta) e_step_mult(theta, x, group)
  m_step <- function(y) m_step_mult(y)

  function(par) m_step(e_step(par))
}
```

## EM Algorithm Implementation

```{r EM-factory}
em <- function(par, em_step, epsilon = 1e-6, maxit = 20, cb = NULL) {
  for (i in seq_len(maxit)) {
    par0 <- par
    par <- em_step(par)
    if (!is.null(cb)) {
      cb()
    }
    if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {
      break
    }
  }
  par
}
```

## Peppered Moths EM Algorithm

```{r, results='hold'}
em_mult_step <- create_em_mult_step(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3)
)

em(c(0.3, 0.3), em_mult_step)
moth_optim$par
```

## Inside the EM Algorithm

The `tracer()` function in the `CSwR` package can be used to trace
the iterations of an algorithm.

. . .

```{r}
library(CSwR)
em_tracer <- tracer("par", Delta = 0)
```

. . .

Each call to `tracer()` returns a S3 object of class tracer. The 
object has a field `tracer`, which is a function that will
record the current value of given parameters each time it is called.

. . .

```{r}
em(c(0.3, 0.3), em_mult_step, cb = em_tracer$tracer)
```

##

You can then summarize the collected values using `summary()`.

```{r}
summary(em_tracer)
```

## Tracing the EM Algorithm

```{r em-tracer}
em_tracer <- tracer(c("par0", "par"), Delta = 0)
phat <- em(c(0.3, 0.3), em_mult_step, epsilon = 0, cb = em_tracer$tracer)
phat
em_trace <- summary(em_tracer)
tail(em_trace)
```

##

```{r plot-curves, echo = FALSE, warning = FALSE}
#| fig-width: 5
#| fig-height: 3.4
#| fig-cap: "EM algorithm iterations shown as points on the log-likelihood contour plot"
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))

em_trace <- transform(
  em_trace,
  n = seq_len(nrow(em_trace)),
  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),
  loglik = loglik_pep(par.1, par.2)
)

group <- c(1, 1, 1, 2, 2, 3)

EStep <- function(theta) e_step_mult(theta, x, group)
MStep <- function(y) m_step_mult(y)

epsilon <- 1e-6
maxit <- 5

exp_vals <- matrix(NA, nrow = maxit, ncol = length(group))

par <- c(0.3, 0.3)

level_curves <- vector("list", length = maxit)

ll <- double(maxit)
x <- c(85, 196, 341)

n_mat <- 100
pC <- seq(0, 1, length.out = n_mat)
pI <- seq(0, 1, length.out = n_mat)

pars <- vector("list", length = maxit)

for (i in seq_len(maxit)) {
  par0 <- par
  y <- EStep(par)

  x_new <- M(y, group)
  z <- matrix(NA, n_mat, n_mat)

  x <- c(85, 196, 341)

  for (k in seq_len(n_mat)) {
    for (j in seq_len(n_mat)) {
      val <- -sum(y * log(prob(c(pC[k], pI[j]))))
      if (is.finite(val)) {
        z[k, j] <- val
      } else {
        z[k, j] <- -Inf
      }
    }
  }

  level_curves[[i]] <- z

  ll[i] <- -sum(y * log(prob(par)))

  exp_vals[i, ] <- y
  par <- MStep(y)
  pars[[i]] <- par
}

par_optim <- c(0.07084, 0.18874)

# Build long data frame for first 4 iterations
build_df <- function(z_mat, iter) {
  out <- data.frame(
    pC = rep(pC, times = length(pI)),
    pI = rep(pI, each = length(pC)),
    z = as.vector(z_mat),
    iter = iter
  )
  out
}

plot_iters <- 1:4
df_all <- do.call(
  rbind,
  lapply(plot_iters, function(i) build_df(level_curves[[i]], i))
)

# Points (current parameter in each iteration)
iter_points <- data.frame(
  pC = sapply(pars[plot_iters], `[[`, 1),
  pI = sapply(pars[plot_iters], `[[`, 2),
  iter = plot_iters
)

mle_point <- data.frame(
  pC = par_optim[1],
  pI = par_optim[2]
)

ggplot(df_all, aes(pC, pI)) +
  geom_contour(
    aes(z = z, color = after_stat(level)),
    breaks = seq(800, 3000, by = 200)
  ) +
  geom_point(data = mle_point, colour = "darkorange", size = 3, pch = 4) +
  geom_point(data = iter_points, colour = "black", size = 2) +
  facet_wrap(~iter, labeller = labeller(iter = function(i) paste0("k = ", i))) +
  coord_equal() +
  guides(color = "none") +
  labs(x = expression(p[C]), y = expression(p[I])) +
  theme(panel.spacing = unit(0.8, "lines"))
```

##

```{r tracer-plot1, echo = FALSE}
#| fig-height: 2.7
#| fig-width: 3.5
#| fig-cap: "Parameter convergence of the EM algorithm applied to the Peppered moths problem."
ggplot(em_trace, aes(n, par_norm_diff)) +
  geom_point() +
  geom_line() +
  labs(
    y = expression(paste(
      "||",
      theta^{
        (n)
      } -
        theta^{
          (n - 1)
        },
      "||"
    ))
  ) +
  scale_y_log10()
```

## Linear Convergence

The log-rate of the convergence can be estimated by least-squares.

```{r linear-convergence}
log_lm <- lm(log(par_norm_diff) ~ n, data = em_trace)
exp(coefficients(log_lm)["n"])
```

\medskip\pause

It is very small in this case implying fast convergence.

\medskip\pause

This is not always the case. If the log-likelihood is flat, the EM-algorithm can
become arbitrarily slow with a rate close to 1.

##

```{r loglikelihood-convergence, warning=FALSE, echo = FALSE, fig.cap="Log-likelihood convergence"}
#| fig-width: 4
#| fig-height: 2.5
mutate(em_trace, diff_loglik = 1e-12 + loglik - min(loglik)) |>
  ggplot(aes(n, diff_loglik)) +
  labs(y = expression(l[theta] - min(l))) +
  geom_point() +
  geom_line() +
  scale_y_log10()
```

## Pitfalls

### Local Maxima

$Q$ is not typically convex, so no guarantee that the EM-algorithm converges
to the global maximum.

\medskip\pause

For this reason, the EM-algorithm is also sensitive to starting values.

. . .

### Bad Initialization

The EM-algorithm can be sensitive to starting values.


. . .

### Label Switching

In mixture models, the components can be permuted without changing the
likelihood.

## Multi-Start

One way to deal with local maxima is to use multiple starting values.

\medskip\pause

For the moths, we can use a grid of starting values.

# Fisher Information

## Statistics

When doing statistical inference, we need to measure the uncertainty of
estimates.

\medskip\pause

The Fisher information is a key quantity for this.

## Gradient of $Q$ for the Multinomial Model

Recall that
$$
Q(\theta \mid \theta') = \sum_{k} \frac{x_{j(i)} p_i(\theta')}{M(\pi(\theta'))_{j(i)}} \log \pi_i(\theta),
$$
where $j(i)$ is defined by $i \in A_{j(i)}$.

\medskip\pause

The gradient of $Q$ w.r.t. $\theta$
$$
\nabla_{\theta} Q(\theta \mid \theta') = \sum_{i} \frac{x_{j(i)}p_i(\theta')}{M(p(\theta'))_{j(i)}p_i(\theta)} \nabla p_i(\theta).
$$

\medskip\pause

When evaluated in $\theta'$ it is
$$
\nabla_{\theta} Q(\theta' \mid \theta') = \sum_{i} \frac{x_{j(i)}}{M(p(\theta'))_{j(i)}} \nabla p_i(\theta').
$$

## Gradient Implementation

```{r}
#| echo: false
Dprob <- function(p) {
  matrix(
    c(
      2 * p[1],
      0,
      2 * p[2],
      2 * p[1],
      2 * p[3] - 2 * p[1],
      -2 * p[1],
      0,
      2 * p[2],
      -2 * p[2],
      2 * p[3] - 2 * p[2],
      -2 * p[3],
      -2 * p[3]
    ),
    ncol = 2,
    nrow = 6,
    byrow = TRUE
  )
}

grad_Q <- function(p, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] / M(prob(p), group)[group]) %*% Dprob(p)
}
```

## Gradient at the MLE

The EM algorithm is a fixed-point algorithm. The limit $\hat{\theta}$ satisfies
$$
\nabla_{\theta} Q(\theta' \mid \theta') = \nabla_{\theta} \ell(\theta')
$$
from the fact that
$$
\theta' = \arg\max_{\theta} \left(Q(\theta \mid \theta') -  \ell(\theta)\right).
$$

. . .

### Emprical Verification

```{r p-hat-call}
p_hat <- em(c(0.3, 0.3), em_mult_step, epsilon = 1e-20)
```

:::: {.columns}

::: {.column width="47%"}

```{r p-hat}
p_hat
```

:::

::: {.column width="47%"}

```{r grad-q}
grad_Q(p_hat)
```

:::

::::

## Empirical Fisher Information

Recall that a multinomial observation with size parameter $n$ can be regarded as
$n$ i.i.d. observations.

. . .

For i.i.d. observations the Fisher information (for one sample) can be estimated
as the empirical variance of the gradient of the log-likelihood. By the identity
$$
\nabla_{\theta} Q_i(\theta' \mid \theta') = \nabla_{\theta} \ell_i(\theta')
$$
holding for each observation, we can compute the empirical variance.

## Moths Empirical Fisher

Can think of the moths as i.i.d. observations.

```{r}
emp_fisher <- function(p, x = c(85, 196, 341)) {
  grad1 <- grad_Q(p, c(1, 0, 0))
  grad2 <- grad_Q(p, c(0, 1, 0))
  grad3 <- grad_Q(p, c(0, 0, 1))
  x[1] *
    t(grad1) %*% grad1 +
    x[2] * t(grad2) %*% grad2 +
    x[3] * t(grad3) %*% grad3
}
```

. . .

```{r}
emp_fisher(p_hat)
```

## Numerical Fisher information

:::: {.columns}

::: {.column width="47%"}

### `stats::optimHess()`

```{r}
-optimHess(
  p_hat,
  loglik,
  grad_Q
)
```

:::

. . .

::: {.column width="47%"}

### `numDeriv::jacobian()`

```{r}
library(numDeriv)
ihat <- -jacobian(grad_Q, p_hat)
ihat
```

:::

::::

. . .

Different estimates of same quantity. Should be close but not identical.

## Measuring Variance

Fisher information can be used to compute standard errors of MLEs

\medskip\pause

Empirical Fisher information can be computed for i.i.d. observations using the
  gradients $\nabla_{\theta} Q_i(\bar{\theta} \mid \hat{\theta})$ evaluated in
  $\bar{\theta} = \hat{\theta}$.

\medskip\pause

Observed Fisher information can be computed by numerical differentiation of
  the gradient $-\nabla_{\theta} Q(\bar{\theta} \mid \bar{\theta})$ evaluated in
  $\bar{\theta} = \hat{\theta}$.

## Second Way: The Information Identity

From
$$
\ell(\theta) = Q(\theta \mid \theta') + H(\theta \mid \theta')
$$
it follows that the observed Fisher information equals
$$
\hat{i}_X := - D^2_{\theta} \ell(\hat{\theta}) =
\underbrace{-D^2_{\theta} Q(\hat{\theta} \mid \theta')}_{\hat{i}_Y(\theta')} -
\underbrace{D^2_{\theta} H(\hat{\theta} \mid \theta')}_{\hat{i}_{Y \mid X}(\theta')}.
$$

\medskip\pause

Introducing $\hat{i}_Y := \hat{i}_Y(\hat{\theta})$ and
$\hat{i}_{Y \mid X} = \hat{i}_{Y \mid X}(\hat{\theta})$. We have the
_information identity_
$$
\hat{i}_X = \hat{i}_Y - \hat{i}_{Y \mid X}.
$$

### Interpretation

- $\hat{i}_Y$ is the Fisher information for complete $Y$.
- $\hat{i}_{Y \mid X}$ is the information "lost" from not observing full $X$.

## More Identities

How to compute the information loss?

\medskip\pause

The second **Bartlett identity** can be reformulated as
$$
\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})
= - \partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta})
$$
which follows from differentiation under the integral of
$\int h(y \mid x, \theta) \mu_x(d y) = 1.$

\medskip\pause

And since we also have
$$
Q(\theta \mid \theta') = \ell(\theta) - H(\theta \mid \theta')
$$
we find that
$$
\partial_{\theta'_i} \partial_{\theta_j} Q(\bar{\theta} \mid \bar{\theta}) =  -
\partial_{\theta'_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}) =
\partial_{\theta_i} \partial_{\theta_j} H(\bar{\theta} \mid \bar{\theta}).
$$

\medskip\pause

Thus
$$
\hat{i}_{Y \mid X} = D_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta}).
$$

## New Implementations

First we implement the map $Q$ as an R function.

```{r q-def}
Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  pp[3] <- 1 - pp[1] - pp[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p))
}
```

. . .

and a modified `grad_Q()` of two arguments:

```{r gradq-def}
grad_Q <- function(p, pp, x = c(85, 196, 341)) {
  p[3] <- 1 - p[1] - p[2]
  group <- c(1, 1, 1, 2, 2, 3)
  (x[group] * prob(pp) / (M(prob(pp), group)[group] * prob(p))) %*% Dprob(p)
}
```

## Numerical Differentiation of $Q$

We use **numDeriv** functions `jacobian()` and `hessian()` to differentiate
numerically.

:::: {.columns}

::: {.column width="47%"}

```{r q-hess}
iY <- -hessian(
  Q,
  p_hat,
  pp = p_hat
)
iY
```

. . .

```{r q-jacobian}
-jacobian(
  grad_Q,
  p_hat,
  pp = p_hat
)
```

:::

. . .

::: {.column width="47%"}

```{r iyx-jacobian}
iYX <- jacobian(function(
  pp
) grad_Q(p_hat, pp), p_hat)

iY - iYX
```

:::

::::

## Third Way: The EM-Mapping

Define $\Phi : \Theta \mapsto \Theta$ by
$$
\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta').
$$

\medskip\pause

A global maximum of the likelihood is a fixed point of $\Phi$,
$\Phi(\theta) = \theta.$

\medskip\pause

Since the limit of the EM algorithm, $\hat{\theta}$, is a fixed point and other
identities above, it can be shown that
$$
D_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.
$$

\medskip\pause

Hence
$$
\begin{aligned} \hat{i}_X & = \left(I - \hat{i}_{Y\mid X}
\left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y = \left(I - D_{\theta}
\Phi(\hat{\theta})^T\right) \hat{i}_Y. \end{aligned}
$$

\medskip\pause

$D_{\theta} \Phi(\hat{\theta})$ can be computed via numerical differentiation.

## EM Step

```{r}
Phi <- create_em_mult_step(c(85, 196, 341), c(1, 1, 1, 2, 2, 3))
p_hat
```

. . .

Let's test it!

```{r}
test_that("Fixed point", {
  expect_equal(Phi(p_hat), p_hat, tol = 1e-8)
})
```

## Differentiating the EM Map

```{r}
DPhi <- jacobian(Phi, p_hat) # Using numDeriv function 'jacobian()'
iX <- (diag(1, 2) - t(DPhi)) %*% iY
iX
ihat # Computed using numerical differentiation of grad_Q
```

# Gaussian Mixtures

## Finite Mixtures

Let $Z \in \{1, \ldots, K\}$ with $P(Z = k) = p_k$, and the conditional
distribution of $X$ given $Z = k$ has density $f_k( \cdot \mid \psi_k)$.

\medskip\pause

The joint density is
$$
(x, k) \mapsto f_k(x \mid \psi_k) p_k
$$

. . .

and the marginal density for the distribution of $X$ is
$$
f(x \mid \theta) =  \sum_{k=1}^K f_k(x \mid \psi_k) p_k.
$$

## Gaussian Mixtures $(K = 2)$

The two Gaussian distributions are parametrized by five parameters
$\mu_1, \mu_2 \in \mathbb{R}$ and $\sigma_1, \sigma_2 > 0$, and
$p = P(Z = 1) = 1 - P(Z = 2)$.

\medskip\pause

The conditional distribution of $X$ given $Z = k$ is
$$
f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.
$$

\medskip\pause

The marginal density is
$$
f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.
$$

## Simulation

:::: {.columns}

::: {.column width="50%"}

```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

:::

::: {.column width="44%"}

```{r, echo=FALSE}
#| fig-cap: "Histogram of simulated data from a two-component Gaussian mixture model."
gausdens <- function(x, mu1, mu2, sigma1, sigma2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

xx <- seq(-3, 11, 0.1)

pl <- ggplot(data.frame(x), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightgray") +
  geom_line(
    aes(x, gausdens(x, mu1, mu2, sigma1, sigma2)),
    color = "black",
    linewidth = 1
  ) +
  labs(y = "Density", x = "x")

pl
```

:::

::::

## Log-likelihood ua

We assume $\sigma_1$ and $\sigma_2$ known.

```{r gaus-mix-loglik}
loglik <- function(par, x) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  -sum(log(
    p *
      exp(-(x - mu1)^2 / (2 * sigma1^2)) /
      sigma1 +
      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  ))
}
```

## Optimizing

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[1:2]
```

. . .

Again, however, initialization matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), loglik, x = x)[1:2]
```

## The $Q$-Function

The complete data log-likelihood is
$$
\sum_{i=1}^n 1(z_i = 1) \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + 1(z_i = 2)\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$

. . .

and
$$
Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$
where $\hat{p}_i = P_{\theta'}(Z_i = 1 \mid X_i = x_i)$.

\medskip\pause

The maximum is attained at
$$
\theta = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).
$$

## The E-Step

The conditional probability in a mixture model is generally
$$
P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}
$$

. . .

which for the $K = 2$ Gaussian case gives
$$
\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.
$$


## Implementation

See [`em_gauss_mix.R`](R/em_gauss_mix.R) for the source file of the
implementation.

```{r source-em-mix}
source(here::here("R", "em_gauss_mix.R"))
```

## Testing

```{r test-em-gauss-mix}
em(c(0.5, -0.5, 4), em_gauss_step)
```

. . .

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), loglik, x = x)[c(1, 2)]
```

. . .

```{r run-em-gauss-mix-again}
em(c(0.9, 3, 1), em_gauss_step) # Starting value still matters
```

## Gradients and Numerical Differentiation

```{r numderiv-grad1}
library(numDeriv)
grad1 <- function(par) grad(function(par) loglik(par, x), par)
```

. . .

```{r numderiv-grad2}
Q <- function(par, par_prime) {
  phat <- e_step_gauss(par_prime, x, sigma1, sigma2)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    phat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}
grad2 <- function(par) -grad(Q, par, par_prime = par)
```

## Gradient Identity

```{r gradient-identities1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

. . .

```{r gradient-identities2}
par_hat <- em(c(0.5, 0, 4), em_gauss_step)
grad1(par_hat)
grad2(par_hat)
```

## Convergence

```{r tracing-convergence}
library(CSwR)

em_tracer <- tracer(
  c("par0", "par", "loglik", "h_prime"),
  Delta = 0,
  expr = quote({
    loglik <- loglik(par, x)
    h_prime <- sum(grad2(par)^2)
  })
)
par_hat <- em(c(0.2, 2, 2), em_gauss_step, cb = em_tracer$tracer)
em_trace <- summary(em_tracer)
tail(em_trace, 4)
```

##

:::: {.columns}

::: {.column width="47%"}

```{r convergence-plots-1, warning=FALSE}
autoplot(
  em_trace,
  y = loglik - min(loglik)
)
```

:::

. . .

::: {.column width="47%"}

```{r convergence-plots-2, warning=FALSE}
autoplot(
  em_trace,
  y = h_prime
)
```

:::

::::

##

```{r mixture-convergence}
#| echo: false
gaussdens2 <- function(x, p, mu1, mu2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

em_gauss_illustration <- function(i, em_trace) {
  p_i <- em_trace$par0.1[i]
  mu1_i <- em_trace$par0.2[i]
  mu2_i <- em_trace$par0.3[i]

  pl +
    geom_line(aes(y = gaussdens2(x, p_i, mu1_i, mu2_i)), color = "darkorange", size = 1) +
    labs(title = paste0("k = ", i))
}
```

## 

```{r}
#| fig-width: 4
#| fig-height: 3.2
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(1, em_trace)
```

```{r}
#| fig-width: 4
#| fig-height: 3.2
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(2, em_trace)
```

## Summary

- We introduced the EM-algorithm, using the pepper moth example throughout
- We showed three ways to compute the Fisher information
- We covered Gaussian mixtures and how to optimize over their parameters using
  the EM algorithm
