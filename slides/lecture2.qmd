---
title: "Object-Oriented Programming and Density Estimation"
---

{{< include _common.qmd >}}

```{r init, echo=FALSE, message=FALSE, warning=FALSE}
load(here::here("data", "top100dih.RData"))

phipsi2 <- na.omit(dataset)
phipsi <- read.table(here::here("data", "phipsi.tsv"), header = TRUE)
phipsi[, c("phi", "psi")] <- pi * phipsi[, c("phi", "psi")] / 180
```

## Today's Agenda

### Object-Oriented Programming (OOP)

We learn what OOP is and how the S3 OOP system in R works.

. . .

### Kernel Density Estimation

We learn how to create non-parametric density estimates using kernel density
estimation.

# Object-Oriented Programming

## What Is Object-Oriented Programming (OOP)?

A paradigm based on the concept of "objects", which contain data and
methods.

. . .

:::: {.columns}

::: {.column width="47%"}

### Data

Contains attributes and properties (e.g. name, type, size, color, etc.)

. . .

### Methods

Functions that operate on the data (e.g. print, plot, summarize, etc.).

:::

::: {.column width="47%"}

. . .

### Benefits of OOP

OOP allows us to bundle data and methods together.

. . .

\medskip

It enables **inheritance**, which means that different classes can share
functionality.

\medskip

. . .

It also facilitates **polymorphism**, which means that the same method can
behave differently for different classes.

:::

::::

## OOP in R

### In Base R

There are three OOP systems in base R:

- S3
- S4
- Reference Classes

. . .

### Through Packages

- [R6](https://cran.r-project.org/package=R6)
- [S7](https://cran.r-project.org/package=S7)

. . .

### This Course

In this course, we focus **entirely** on S3: a _very_ informal OOP system.

## Example: Integration

The function `integrate()` takes a function as argument and returns the value of
numerically integrating the function. It is an example of a _functional_.

. . .

```{r}
integral <- integrate(sin, 0, 1)
integral
```

. . .

The numerical value of the integral
$$
\int_0^1 \sin(x) \mathrm{d}x
$$
is helpfully printed---including an indication of the numerical error.

## Return Values

In fact, `integrate()` returns a class object: a list with a _class label_.

```{r}
str(integral)
```

. . .

### What is the Point of This Class Label?

It allows us to write functions that act based on the
class of the argument.

. . .

\medskip

If `x` is an object of class `numeric`, then do A. If `x` is an object of class
`integrate`, then do B.

## The Return Value of `integrate()`

We can extract the class label with `class()`.

```{r}
class(integral)
```

. . .

The printed result of `integrate()` is not the same as the object itself. What
you see is the result of a **method**, `print.integrate()`.

## Printing Objects of Class Integrate

```{r}
#| eval: false
stats:::print.integrate
```

```{r}
#| eval: false
function(x, digits = getOption("digits"), ...) {
  if (x$message == "OK") {
    cat(
      format(x$value, digits = digits),
      " with absolute error < ",
      format(x$abs.error, digits = 2L),
      "\n",
      sep = ""
    )
  } else {
    cat("failed with message ", sQuote(x$message), "\n", sep = "")
  }
  invisible(x)
}
```

\pdfpcnote{
  Tell the students about `:::` and that it is not something they should use
  themselves.
}

## Histogram Objects

:::: {.columns align=center}

::: {.column width="47%"}

`hist()` also returns an object (of class `histogram`), which has methods
defined.

. . .

```{r temp-hist-src}
#| eval: false
phi_hist <- hist(
  phipsi$ph
)
```

:::

::: {.column width="47%"}

```{r temp-hist-plot}
#| ref-label: temp-hist-src
#| fig-height: 2.5
#| echo: false
```

:::

::::

. . .

```{r}
class(phi_hist)
```

## Histogram Object Structure

```{r}
str(phi_hist)
```

## Getting Help for Methods

You can find documentation for `plot()` using for instance

```{r help-plot, eval = FALSE}
?plot
```

. . .

But this will be uninformative about how an object of class histogram is
plotted. Instead, try

```{r help-plot-histogram, eval = FALSE}
?plot.histogram
```

. . .

This will give the documentation for the plot method for objects of class
histogram.

## S3 Overview

S3 classes are standard data structures (typically lists) with _class labels_.

. . .

\bigskip

It is a simple but informal system: no formal object defintion and no type checking.

. . .

\bigskip

To use S3, we implement a \alert{generic} function via `UseMethod()`. E.g. `plot()`:

```{r}
#| eval: false
plot <- function(x, y, ...) {
  UseMethod("plot")
}
```

. . .

**Methods** for specific classes are implemented as standard R functions with
the naming convention `f.classname()` for a method for class `classname` of the
function `f()`.

. . .

\bigskip

The system is widely used to write methods for the generic functions `print()`,
`plot()` and `summary()`.

## Use Cases for OOP

### Handle function output

One common use of OOP is to make it easier to handle object that your
functions create, especially if they are complex (e.g. `lm()`).

. . .

\medskip

Instead of having to write specialized functions, e.g. `print_lm()`, `plot_lm()`,
you can write methods for the generic functions `print()`, `plot()`, etc.

. . .

### Dispatch based on input type


. . .

\medskip

For instance, you may want a function that behaves differently
depending on whether the input is a vector or a matrix.

## Constructing a New Class

:::: {.columns}

::: {.column width="47%"}

Here's a function that computes summary statistics of a numeric
vector. 

```{r}
summarize_data <- function(x) {
  structure(
    list(
      mean = mean(x),
      median = median(x),
      sd = sd(x)
    ),
    class = "data_summary"
  )
}
```

:::

. . .

::: {.column width="47%"}

`structure()` is a common way to create an object with a class label.

\medskip

. . .

Returns a list with three components: the mean, median, and
standard deviation of the input vector.

. . .

### Constructor Functions

Sometimes it's worth writing a constructor function to
encapsulate the creation of an object.

:::

::::

## Print Method

:::: {.columns align=center}

::: {.column width="47%"}

```{r}
vec <- rnorm(100)
summary <- summarize_data(vec)
summary # calls print.default()
```

:::


::: {.column width="47%"}

. . .

This is fine, but we may want a more compact display.

. . .

\medskip

To achieve this, we can write a new \alert{method} for
`print()` that acts on our class.

. . .

\medskip

The reason we can do this is that `print()` is
a generic.

:::

::::

## A Print Method

:::: {.columns}

::: {.column width="47%"}

The print method is written by just
suffixing the class name with a dot
after the generic name.

```{r}
print.data_summary <-
  function(x) {
    cat(
      "Mean:",
      x$mean,
      "\nMedian:",
      x$median,
      "\nStandard deviation:",
      x$sd,
      "\n"
    )
  }
```

:::

. . .

::: {.column width="47%"}

Let's test our new method.

```{r}
summary
```

. . .

Same call as before, but now there's a method
available, so `print.data_summary()` is called.

:::

::::

## Generics

:::: {.columns align=center}

::: {.column width="47%"}

Let's say that we want to specialize `summarize_data()`{.r}. We could write our own 
generic:

```{r}
summarize_data <- function(x) {
  UseMethod("summarize_data")
}
```

. . .

We use our previous implementation as the \alert{default}
method.

:::

::: {.column width="47%"}

. . .

```{r}
summarize_data.default <-
  function(x) {
    structure(
      list(
        mean = mean(x),
        median = median(x),
        sd = sd(x)
      ),
      class = "data_summary"
    )
  }
```

:::

::::

## Adding Methods

What if we want our function to have specific behavior for,
say, **matrices**?

\medskip

. . .

```{r}
summarize_data.matrix <- function(x) {
  stats <- apply(x, 2, summarize_data)
  class(stats) <- "matrix_summary"

  stats
}
```

. . .

```{r}
mat <- matrix(rnorm(200), ncol = 2)

matrix_stats <- summarize_data(mat)
```

## Recursive Printing

:::: {.columns align=center}

::: {.column width="47%"}

Printing the matrix summary calls the print method recursively through the list.

```{r}
#| eval: false
matrix_stats
```

. . .

:::

::: {.column width="47%"}


```{r}
#| echo: false
matrix_stats
```

:::

::::

## Common Pitfalls in S3

:::: {.columns}

::: {.column width="47%"}

### Naming conflicts

Accidentally overwriting existing generics or methods (e.g., redefining
`print()`).

```{r}
#| include: false
old_print <- print
```


```{r}
print <- function(x) {
  cat("Oops!\n")
}
```

. . .

Now `print()` no longer works as expected.

```{r}
print("Hello")
```

```{r}
#| include: false
print <- old_print
```

:::

::: {.column width="47%"}

. . .

### Forgetting the default method

Forgetting the default method (`myfun.default`), can cause confusing
errors.

```{r}
#| eval: false
myfun <- function(x, ...) {
  UseMethod("myfun")
}

myfun.numeric <-
  function(x, ...) {
    "It's numeric!"
  }

myfun("a") # error!
```

:::

::::

## Best Practices for S3

Use clear, descriptive class and method names to avoid conflicts.

. . .

\medskip

Document your classes and methods so users know how to extend or interact with them.

. . .

\medskip

Keep objects simple---prefer lists with named elements for S3 objects.

## Criticisms of OOP

### Portability

OOP code is often less portable than functional code.

. . .

\bigskip

> ... the problem with object-oriented languages is theyâ€™ve got all this
> implicit environment that they carry around with them. You wanted a banana
> but what you got was a gorilla holding the banana and the entire jungle.
>
> \medskip
> \hfill---Joe Armstrong

. . .

### Complexity

OOP can introduce unnecessary complexity. You have to understand the class system and
how methods are dispatched.

# Density Estimation

## Density Estimation

**Basic idea:** we have $f_0$, an unknown density that we want to estimate.

\medskip

. . .

We could use a \alert{parametric} approach, where we assume that $f_0$ is in a
family of densities $(f_\theta)_\theta$ indexed by a parameter $\theta$.

. . .

\medskip

Alternatively, we can use a \alert{nonparametric} approach, where we do not
assume a specific parametric form for $f_0$, e.g. the **histogram**, which
we can write as $\hat{f}$.

. . .

### Why are we interested in nonparametric estimators?

Because

. . .

- we want to compare data with the parametric estimate,\pause
- we don't know a suitable parametric model, and\pause
- we want to explore the data (through visualization).

## Density Estimation

Density estimation relies on the approximation
$$
  P(X \in (x-h, x+h)) = \int_{x-h}^{x+h} f_0(z) \ dz \pause \simeq f_0(x) 2h.
$$

. . .

Rearranging and using the law of large numbers gives
$$
  \begin{aligned}
    f_0(x) & \simeq \frac{1}{2h} P(X \in (x-h, x+h)) \\ \pause
           & \simeq \frac{1}{2hn} \sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\ \pause
           & = \frac{1}{2hn} \sum_{i=1}^n 1_{(-h, h)}(x - x_i) \\ \pause
           & = \hat{f}_h(x).
  \end{aligned}
$$

\pdfpcnote{
  - Probability of X in (x-h, x+h) is small for small h
  - Density at x times width of interval
  - Approximation gets better as h gets smaller
}

## Kernels

We will consider _kernel estimators_
$$
  \hat{f}_h(x) = \frac{1}{hn} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

### The Uniform Kernel

$$
  K(x) = \frac{1}{2} 1_{(-1,1)}(x)
$$

```{r}
#| echo: false
#| fig-height: 1.5
ggplot(tibble(x = seq(-2, 2, length.out = 100)), aes(x)) +
  geom_line(aes(y = dunif(x, -1, 1) / 2)) +
  labs(x = "x", y = "K(x)")
```

Leads to expression on last slide.

. . .

:::

::: {.column width="47%"}

### The Gaussian Kernel

$$
  K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
$$

```{r}
#| echo: false
#| fig-height: 1.5
ggplot(tibble(x = seq(-4, 4, length.out = 100)), aes(x)) +
  geom_line(aes(y = dnorm(x))) +
  labs(x = "x", y = "K(x)")
```

:::

::::

## An Illustration

```{r, echo = FALSE}
x <- c(-1, 3, 5, 6, 9)
n <- length(x)

l <- lapply(x, function(xi) {
  a <- seq(-4, 4, length.out = 100)
  d <- dnorm(a) / n
  data.frame(
    x = xi,
    a = a + xi,
    d = d
  )
})

res <- do.call(rbind, l)

p1 <- ggplot() +
  xlim(c(-5, 15)) +
  ylim(c(0, 0.15)) +
  labs(x = "x", y = "Density")

p2 <- p1 +
  geom_rug(aes(x), data = tibble(x = x), linewidth = 1, col = "navy")

p3 <- p2 +
  geom_line(aes(a, d, group = x), col = "dark orange", data = res)

p4 <- p3 +
  geom_density(aes(x), bw = 1, data = tibble(x = x))
```

:::: {.columns}

::: {.column width="47%"}

Let's say we have a data set
$$
\boldsymbol{x} = (-1, 3, 5, 6, 9).
$$

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 2.5
p2
```

:::

::::

## An Illustration: Gaussian Kernel

:::: {.columns}

::: {.column width="47%"}


Let's say we have a data set
$$
\boldsymbol{x} = (-1, 3, 5, 6, 9).
$$

\smallskip

Now we add a Gaussian density kernel with bandwidth 1 for each point.

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 2.5
p3
```

:::

::::

## An Illustration: Gaussian Kernel

:::: {.columns}

::: {.column width="47%"}

Let's say we have a data set
$$
\boldsymbol{x} = (-1, 3, 5, 6, 9).
$$

\smallskip

Now we add a Gaussian density kernel with bandwidth 1 for each point.

\bigskip

Finally, we average (sum) the kernels to get the density estimate.

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 2.5
p4
```

:::

::::

## Implementation with the Gaussian Kernel

```{r kern-dens-impl}
kern_dens <- function(x, h, m = 512) {
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)

  for (i in seq_along(xx)) {
    for (j in seq_along(x)) {
      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))
    }
  }

  y <- y / (sqrt(2 * pi) * h * length(x))

  list(x = xx, y = y)
}
```

## Angle Data

Let's see if we can use our implementation to estimate the density of the
$\Psi$ angle in the amino acid data.

. . .

\bigskip

```{r}
#| fig-width: 4
#| fig-height: 2
#| fig-cap: Histogram of the $\Psi$ angle in the amino acid data.
#| echo: false
ggplot(phipsi, aes(psi)) +
  geom_histogram() +
  geom_rug(alpha = 0.3) +
  labs(x = expression(psi), y = "Density")
```

## A First Test

```{r}
f_hat <- kern_dens(phipsi$psi, 0.2)
f_hat_dens <- density(phipsi$psi, 0.2)
```

. . .

```{r test-dens}
#| echo: false
#| fig-width: 5.5
df <- bind_rows(
  as.data.frame(f_hat) |> mutate(method = "ours"),
  tibble(x = f_hat_dens$x, y = f_hat_dens$y, method = "density()")
)

ggplot(df, aes(x, y)) +
  geom_line() +
  facet_wrap(~method) +
  labs(x = "x", y = "density")
```

## Plotting Differences

:::: {.columns align=center}

::: {.column width="47%"}

Plotting the difference between the two estimates,
we see that they are very close.

```{r plot-diff}
#| eval: false
df_diff <- tibble(
  x = f_hat$x,
  y = f_hat$y - f_hat_dens$y
)

ggplot(df_diff, aes(x, y)) +
  geom_line() +
  labs(
    x = "x",
    y = "difference"
  )
```

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| ref-label: plot-diff
#| fig-cap: Difference between our density estimate and that of `density()`.
```

:::

::::

## Testing

\alert{Correctness} is one of the major computational topics in the course.

. . .

\medskip

These tests are ad-hoc. Later in the course we will learn about formal testing
using the `testthat` package.\pause Here's a rough idea:

```{r}
test <- all.equal(f_hat$y, f_hat_dens$y)

isTRUE(test)
```

. . .

```{r}
test
```

. . .

### Numerical Tolerance

You need to decide on the tolerance level on a case-by-case basis.

## Maximum Likelihood Estimation

One might wonder if we can use maximum likelihood estimation (MLE) to
estimate the density. I.e., solve
$$
  \hat{\theta} = \text{arg max}_{\theta} \sum_{j=1}^n \log f_{\theta}(x_j).
$$

. . . 

:::: {.columns align=center}

::: {.column width="47%"}

### First Try

Use the empirical mean and standard deviation to estimate
the parameters of a Gaussian density.

. . .

\medskip

How does this work?

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: "Answer: rather poorly."
psi_mean <- mean(phipsi$psi)
psi_sd <- sd(phipsi$psi)

ggplot(phipsi, aes(psi)) +
  geom_histogram(aes(y = ..density..), fill = "grey60") +
  geom_rug(alpha = 0.3) +
  stat_function(
    fun = dnorm,
    args = list(mean = psi_mean, sd = psi_sd),
    color = "blue"
  ) +
  labs(x = expression(psi), y = "density")
```

:::

::::

## A Second Try

How about maximizing over \alert{all possible} densities?

\medskip

. . .

For nonparametric estimation we can still introduce the log-likelihood:
$$
\ell(f) = \sum_{j=1}^n \log f(x_j)
$$

. . .

Let's see what happens for the Gaussian kernel density estimate
$$
\bar{f}_h(x) = \frac{1}{nh \sqrt{2 \pi}} \sum_{j=1}^n e^{- \frac{(x - x_j)^2}{2 h^2} }.
$$

\pdfpcnote{
  $\bar{f}_h(x)$ is just a KDE with Gaussian kernel and bandwidth $h$.
}

---

```{r}
#| echo: false
#| fig-cap: $h = 1$.
#| fig-width: 3.5
#| fig-height: 2.5
f_h <- function(x, h) mean(dnorm(x, phipsi$psi, h))
f_h <- Vectorize(f_h)

psi_df <- data.frame(psi = phipsi$psi)

plot_psi_density <- function(h, n = 1001) {
  x_seq <- seq(min(phipsi$psi), max(phipsi$psi), length.out = n)
  dens_df <- data.frame(x = x_seq, y = f_h(x_seq, h))
  ggplot(psi_df, aes(x = psi)) +
    geom_histogram(aes(y = ..density..), fill = "grey60") +
    geom_rug(alpha = 0.3) +
    geom_line(data = dens_df, aes(x = x, y = y), color = "blue") +
    labs(x = expression(psi), y = "density") +
    lims(y = c(0, 0.5))
}

# Example plot for h = 1
plot_psi_density(1)
```

---


```{r}
#| echo: false
#| fig-cap: $h = 0.25$.
#| fig-width: 3.5
#| fig-height: 2.5
plot_psi_density(0.25)
```

---


```{r}
#| echo: false
#| fig-cap: $h = 0.1$.
#| fig-width: 3.5
#| fig-height: 2.5
plot_psi_density(0.1)
```

---

```{r}
#| echo: false
#| fig-cap: $h = 0.025$.
#| fig-width: 3.5
#| fig-height: 2.5
plot_psi_density(0.025, n = 10001)
```

---

```{r}
#| echo: false
#| fig-cap: $h = 0.01$.
#| fig-width: 3.5
#| fig-height: 2.5
plot_psi_density(0.01, n = 10001)
```

---

```{r}
#| echo: false
#| fig-cap: $h = 0.0001$.
#| fig-width: 3.5
#| fig-height: 2.5
plot_psi_density(0.0001, n = 10001)
```

## The MLE Does Not Exist

If $x_i \neq x_j$ when $i \neq j$, then
$$
\begin{aligned}
  \ell(f-h) & = \sum_{i} \log\left(1 + \sum_{j \neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \right) - n \log(nh\sqrt{2 \pi}) \\ \pause
            & \sim - n \log(nh\sqrt{2 \pi})
\end{aligned}
$$

. . .

\bigskip

Hence, $\ell(f_h) \to \infty$ for $h \to 0$ and there is **no MLE**.

## ISE, MISE, and MSE

Quality of $\hat{f}_h$ can be quantified by the _integrated squared error_,
$$\operatorname{ISE}(\hat{f}_h) = \int (\hat{f}_h(x) - f_0(x))^2 \ dx = ||\hat{f}_h - f_0||_2^2.$$

. . .

Quality of the estimation procedure can be quantified by the mean ISE,
$$
  \operatorname{MISE}(h) = \operatorname{E}(\mathrm{ISE}(\hat{f}_h)),
$$
where
the expectation integral is over the data.

. . .

$$
  \operatorname{MISE}(h) = \int \operatorname{MSE}_h(x) \ dx
$$ where
$\operatorname{MSE}_h(x) = \operatorname{var}(\hat{f}_h(x)) + \mathrm{bias}(\hat{f}_h(x))^2$.


\pdfpcnote{
  Derive this for the uniform kernel (if you have time.)
}

## AMISE

If $K$ is a square-integrable probability density with mean 0,
$$
  \mathrm{MISE}(h) = \mathrm{AMISE}(h) + o((nh)^{-1} + h^4)
$$ \pause
where the _asymptotic mean integrated squared error_ is
$$
  \mathrm{AMISE}(h) = \frac{\|K\|_2^2}{nh} + \frac{h^4 \sigma^4_K \|f_0''\|_2^2}{4}
$$
with $\sigma_K^2 = \int t^2 K(t) \ dt.$

. . .

\medskip

### Asymptotically Optimal Bandwidth

Using plug-in estimates of $\|f_0''\|_2^2$, AMISE can be used to
estimate the _asymptotically optimal bandwidth_ (with respect to MISE).

## Amino Acid Angles (Silverman, the default)

```{r}
#| echo: false
library(patchwork)

phipsi_plot <- function(bw) {
  phi <-  ggplot(phipsi, aes(x = phi)) +
    geom_histogram(aes(y = ..density..), fill = "grey60") +
    geom_rug(alpha = 0.3) +
    geom_density(bw = bw, color = "blue") +
    labs(x = expression(Phi), y = "density")

  psi <- ggplot(phipsi, aes(x = psi)) +
    geom_histogram(aes(y = ..density..), fill = "grey60") +
    geom_rug(alpha = 0.3) +
    geom_density(bw = bw, color = "blue") +
    labs(x = expression(Psi), y = "density")

  phi + psi + plot_layout(axes = "collect")
}
```

$$
  h = 0.9 \min \left(\hat{\sigma},\frac{\text{IQR}}{1.34}\right) n^{-1/5}
$$

\medskip

```{r}
#| echo: false
#| fig-width: 5.5
#| fig-cap: Silverman's rule of thumb bandwidth.
phipsi_plot("nrd0")
```

## Amino Acid Angles (Scott, using 1.06)

$$
  h = 1.06 \hat{\sigma} n^{-1/5}
$$

\medskip

```{r}
#| echo: false
#| fig-width: 5.5
#| fig-cap: Scott's rule of thumb bandwidth.
phipsi_plot("nrd")
```

## Amino Acid Angles (Sheather & Jones)

More elaborate method!

\medskip

```{r}
#| echo: false
#| fig-width: 5.5
#| fig-cap: Sheather & Jones' bandwidth.
phipsi_plot("sj")
```

::: {.standout}

##

Questions?

:::

# Exercises

## Exercises

### Exercise 1: Create a Simple S3 Class

Write a function `make_point(x, y)` that returns an object of class `"point"`
with elements `x` and `y`.

. . .

\medskip

Write a print method `print.point()` that prints the point as `(x, y)`.

. . .

### Exercise 2: Extend the Point Class

Write a function `distance_from_origin(point)` that computes the Euclidean
distance from the origin for a `"point"` object.

. . .

Add a default method `distance_from_origin.default()` that works for numeric
vectors of length 2.

. . .

### Exercise 3: Manually Insert Bandwidth Selection

Sample a data set from a Gaussial mixture model, varying the
standard deviation of the components. How does the different bandwidth
selection methods in `density()` perform?
