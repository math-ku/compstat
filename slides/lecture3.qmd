---
title: "Measuring and Improving Performance"

execute:
  cache: true
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
source(here::here("R/kernel.R"), keep.source = TRUE)
source(here::here("R/gauss.R"), keep.source = TRUE)
```

## Today

### Measuring Performance

#### Profiling

Identifying bottlenecks in code

#### Benchmarking

Comparing performance of different implementations

. . .

### Improving Performance

Writing efficient code

## Profiling

A profiler quantifies how much time each part of a function takes up. R uses a
**sampling** profiler.

. . .

### profvis

The R package [profvis](https://CRAN.R-project.org/package=profvis) provides
useful visualization tools. Can also be called activated through the RStudio
IDE.

```{r load_profvis}
library(profvis)
```

. . .

### Remember!

> We _should_ forget about small efficiencies, say about 97% of the time:
> premature optimization is the root of all evil. Yet we should not pass up our
> opportunities in that critical 3%.
>
> _—Donald Knuth_

## Example

Let's profile the following reimplementation of `gauss()`, which computes the
density of a standard Gaussian distribution.

```{r gauss_step}
gauss_step
```

## Usage

### Simple to Use

1. Source the code you want to profile using `source()`.
2. Run `profvis()` on the expression you want to profile. (Or use the RStudio
   tool.)

The result is an interactive webpage (opens in tab in RStudio).

```{r source_profvis}
source(here::here("R/gauss.R"), keep.source = TRUE)
x <- rnorm(1e7)
# profvis(gauss_step(x))
```

## Benchmarking Density Estimation

Let's profile our kernel density function.

```{r}
kern_dens
```

## Dissecting Expressions

The one-liner in the loop did too much (for profiling). `kern_dens_detail()`
spells out the steps.

```{r kern_dens_detail, echo=FALSE}
kern_dens_detail
```

## Line Profiling

```{r profile_kern_dens_detail, eval=FALSE}
source(here::here("R/kernel.R"), keep.source = TRUE)

x <- rnorm(1e3)
# profvis(kern_dens_detail(x, 0.2))
```

The result from profiling this call is more informative.

```{r profile-kern-dens-detail-output, echo = FALSE, ref.label = "profile_kern_dens_detail"}

```

## Exercise

- Profile this implementation of computing a matrix-vector inner product for
  different choices of $n$ and $p$.
- Before starting: where do you think the bottleneck is?

```{r matvec-mul-ex}
matrix_vector_dot <- function(x, y) {
  p <- NCOL(x)

  out <- c()

  for (i in seq_len(p)) {
    xi_y <- t(x[, i]) %*% y
    out <- c(out, xi_y)
  }

  out
}
```

## Caveats About Line Profiling

- Anonymous functions complicates profiling; prefer named functions.
- Resolution (sampling frequency) may too low for small pieces of code (but do
  you really need to profile them then?)

## Benchmarking

The purpose of benchmarking is to measure and compare the computational
resources used by one or more implementations of a computation.

. . .

### Microbenchmarking

Benchmarking small pieces of code.

Typically: 1) find bottleneck through profiling and 2) (micro)benchmark
alternatives.

### The bench Package

We use the R package [bench](https://cran.r-project.org/web/package=bench) for
benchmarking.

```{r}
library(bench)
```

## `bench::mark()`

- Main function of package
- High-precision timer
- Adaptive number of iterations
- Checks result for correctness (unless `check = FALSE`)

. . .

```{r}
x <- runif(100)

sqrt_bench <- bench::mark(
  sqrt(x),
  x^0.5
)

sqrt_bench
```

## `plot.bench_mark()`

[ggbeeswarm](https://CRAN.R-project.org/package=ggbeeswarm) necessary for
default plot behavior.

```{r bee-bench, fig.asp = 0.5, fig.width = 9, message = FALSE}
library(ggbeeswarm)

plot(sqrt_bench)
```

## Exercise

Consider the following implementation of the Gaussian kernel.

```{r gauss-def}
gauss <- function(x, h = 1) {
  exp(-x^2 / (2 * h^2)) / (h * sqrt(2 * pi))
}
```

Benchmark `gauss()` against `dnorm()`; plot and compare the results. Before
starting: which do you think will be faster?

```{r gauss-bench, include = FALSE}
x <- seq(-3, 3, length.out = 10000)

gauss_bench <- bench::mark(
  gauss(x),
  dnorm(x)
)
plot(gauss_bench)
```

## `bench::press()`

Parameterized benchmarking; runs `bench::mark()` across outer product of its
initial arguments

. . .

### Benchmarking Density Estimation

```{r dens-bench, message = FALSE, cache = TRUE, warning = FALSE}
dens_bench <- bench::press(
  n = 2^(5:13),
  {
    x <- rnorm(n)
    bench::mark(
      base = density(x, 0.2),
      loop = kern_dens(x, 0.2),
      vectorized = kern_dens_vec(x, 0.2),
      check = FALSE
    )
  }
)
```

[Source for R code](R/kernel.R)

## Bench Press Results

```{r head-bench}
head(dens_bench, 6)
```

## Default Plotting Benchpress Results

```{r default-press-plot, fig.width = 12, fig.asp = 0.5}
plot(dens_bench)
```

## Custom Plotting

A bit tricky since `bench_mark` objects have custom classes for time.

```{r custom-bench-plot-code, echo=TRUE, warning=FALSE, results = "hide", fig.show="hide", message = FALSE, fig.width = 10, fig.asp = 0.65}
library(tidyverse)

mutate(
  dens_bench,
  expr = as.character(expression),
  median = as.numeric(median)
) |>
  ggplot(aes(n, median, color = expr)) +
  geom_point() +
  geom_line() +
  scale_y_log10() +
  labs(y = "time (µs)")
```

```{r custom-bench-plot, echo=FALSE, ref.label="custom-bench-plot-code", fig.width = 10, fig.asp = 0.65}

```

## Exercise

- Benchmark `crossprod()` against `%*%` for computing the inner product between
  a matrix and a vector. --

- Parameterize the benchmark by $n$ and $p$.
- Where do you think the difference comes from?

```{r crossprod-bench, include = FALSE, cache = TRUE}
crossprod_bench <- bench::press(
  n = 10^(1:3),
  p = 10^(1:2),
  {
    x <- matrix(rnorm(n * p), n)
    y <- rnorm(n)

    bench::mark(
      t(x) %*% y,
      crossprod(x, y)
    )
  }
)
```

## Notes About Benchmarking

- Look at absolute values too: does the difference matter (for your use case)?
- Keep use case in mind.
- Your computer may be doing something else at the same time.
- Hardware matters: different computers may give different results.

# Improving Performance

## Many Ways

### General Computational Strategies

- Avoiding copies (passing by reference)
- Storage modes
- Data structures
- Parallelization

. . .

### R-Specific Tricks

- Vectorization
- Using specialized functions
- Check function help files: arguments matter

. . .

### Context-Specific Tricks

- Density destimation: binning (up next)

## Binning in Density Estimation

The line profiler revealed that most time is spent on the kernel evaluation, but
we cannot optimize the kernel further.

. . .

Instead, we use binning: create $B$ bins and evaluate the kernel and replace
$$\hat{f}(x) = \frac{1}{n h} \sum_{i=1}^n K\left( \frac{x - x_i}{h} \right)$$
with
$$\hat{f}(x) = \frac{1}{n h} \sum_{j=1}^B n_j K\left( \frac{x - c_j}{h} \right)$$
where $c_j$ is the center of the $j$th bin and $n_j$ is the number of data
points in the $j$th bin.

. . .

Turns complexity from $O(nm)$ to $O(n) + O(mB)$.

### Binning Procedure

1. Determine the range of the data and divide it into equal-sized bins.
2. Count the number of data points in each bin.
3. For each bin, compute the kernel density estimate using the bin count and the
   kernel function.
4. For a new data point, find the bin it belongs to and use the pre-computed
   kernel density estimate for that bin.

Example of context-specific optimization.

. . .

If $n < B$, then binning will not help

## Binning

```{r}
kern_bin
```

The `kern_bin()` function is a loop along the data vector, and arithmetic is
used to determine which bin center is closest to a data point.

. . .

The `kern_dens_bin()` function (see [the source file](R/kernel.R)) computes bin
weights using `kern_bin()` with grid points as bin centers.

```{r micro3, cache = TRUE, echo=FALSE, message=FALSE, warning = FALSE}
res2 <- bench::press(
  n = 2^(5:13),
  {
    h <- 0.2
    x <- rnorm(n)
    bench::mark(
      base = density(x, h),
      loop = kern_dens(x, h),
      vectorized = kern_dens_vec(x, h),
      binning = kern_dens_bin(x, h),
      check = FALSE
    )
  }
)

mutate(
  res2,
  expr = as.character(expression),
  median = as.numeric(median)
) |>
  ggplot(aes(n, median, color = expr)) +
  geom_point() +
  geom_line() +
  scale_y_log10() +
  labs(y = "time (µs)")
```

The relative benefit of binning increases with the size of the data.

## Testing

```{r plot-kern-dens-benchmark, echo=-c(1:2), fig.width = 7, fig.asp = 0.7}
library(tidyverse)
set.seed(123)
x <- rnorm(1e4) + rnorm(1e4, -3, 0.7)

plot(kern_dens(x, 0.2), type = "l", lwd = 4)
lines(kern_dens_bin(x, 0.2), col = "red", lwd = 2)
```

## Testing

```{r test-kern-dens-bench, echo=FALSE, fig.width=7, fig.asp = 0.65}
x0 <- kern_dens(x, 0.2)$x
plot(
  x0,
  kern_dens(x, 0.2)$y - kern_dens_bin(x, 0.2)$y,
  type = "l",
  ylim = c(-3e-3, 3e-3),
  lwd = 2,
  ylab = "Difference"
)
lines(
  x0,
  kern_dens(x[1:1024], 0.2)$y - kern_dens_bin(x[1:1024], 0.2)$y,
  col = "red",
  lwd = 2
)
lines(
  x0,
  kern_dens(x[1:128], 0.2)$y - kern_dens_bin(x[1:128], 0.2)$y,
  col = "blue",
  lwd = 2
)
```

The absolute errors due to binning are small but increasing with decreasing
length of data sequence. Here $n = 8192$ is black, $n = 1024$ is red and
$n = 128$ is blue.

## Line Profiling

The `kern_dens_bin()` function is so much faster for long sequences that to get
good profiling results we use a 512 times longer data sequence.

```{r eval = FALSE}
x <- rnorm(2^22)
# profvis(kern_dens_bin(x, 0.2))
```

## Vectorization

- Vectorization is the process of replacing loops with vectorized operations.
- These vectorized operations are also loops, but they are written in C instead
  or R.
  - Examples of vectorized functions: `mean()`, `exp()`
  - Examples of non-vectorized functions: `apply()`, `Vectorize()`
- Vectorizing code is often about finding the right function in R.
  - `colSums()` instead of for loop or apply-type of function.

## Exercise on Vectorization

- Vectorize the `kern_bin()` function
- Benchmark the performance of the two functions.
- **Hint:** Use `tabulate()`.

```{r}
kern_bin
```

```{r kern-bin-vec, include = FALSE}
kern_bin_vec <- function(x, lo, hi, m) {
  delta <- (hi - lo) / (m - 1)
  i <- floor((x - lo) / delta + 0.5) + 1
  w <- tabulate(i, nbins = m)
  w / sum(w)
}
```

## Avoiding Copies

### Copy-on-Modify

- In R, objects are passed by reference, but when an object is modified a copy
  is created.
- For instance, when subsetting a matrix, a copy is created. It's not possible
  to access for instance a column by reference.
- Growing vectors (`c()`) and matrices (`rbind()`, `cbind()`) creates copies.

```{r}
x <- rnorm(100)
x <- c(x, 4) # 101 values are allocated
```

## Memory

### Memory in R

In R, everything is typically loaded into memory.

### Garbage Collection

R includes a garbage collector, which intermittently releases unused blocks in
memory.

### Trade-Offs

Storing intermediate objects that are used multiple times will boost performance
at the cost of additional memory storage.

## Exercise

Can you rewrite the following function to avoid creating copies?

```{r}
matrix_vector_dot
```

## Storage Modes

:::: {.columns}

::: {.column width="47%"}

- In R, matrices are stored in column-major order.
- This is generally language-dependent.
- This means that when you access a column of a matrix, you are accessing a
  contiguous block of memory.
- Some operations are faster with column-major order and others with row-major
  order.

:::

::: {.column width="47%"}

:::

::::

## Exercise

Benchmark the following three implementations of matrix-vector multiplication.

1. Take the inner product of each row with the vector and sum up.
2. First transpose the matrix and then take the inner product.
3. Take the elementwise (Hadamard) product of each column of the matrix and
   vector and sum up. --

### Implementation of First Method

```{r}
matvecmul_v1 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- t(x[i, ]) %*% y
  }
  z
}
```

```{r, include = FALSE, cache = TRUE}
n <- 1e3
p <- 1e1

x <- matrix(rnorm(n * p), n)
y <- matrix(rnorm(p), p, 1)

f1 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- t(x[i, ]) %*% y
  }
  z
}

f2 <- function(x, y) {
  n <- NROW(x)
  x_t <- t(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- x_t[, i] %*% y
  }

  z
}

f3 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(NCOL(x))) {
    z <- z + x[, i] * y[i]
  }

  z
}

bench::mark(f1(x, y), f2(x, y), f3(x, y))
```

## Concurrency

- Modern processors today have multiple (physical and virtual) cores
- But unless instructed otherwise, only a single core is going to be used.
- The computer doesn't automatically know that your computations are safe to do
  in parallel.

## Embarassingly Parallel Tasks

Trivial implementation of parallelization

It would be embarassing to miss the opportuity to parallelize the task.

### Examples

- Summing a vector (or matrix): `sum()`
- Linear algebra operations: `%*%` (`crossprod()`)
- Running iterations of a simulation
- Cross-validation

## The foreach Package

:::: {.columns}

::: {.column width="47%"}

```{r foreach-load, message = FALSE, echo = -1}
options(width = 40)
library(foreach)
```

```{r foreach-sqrt}
foreach(i = seq_len(3)) %dopar%
  {
    sqrt(i)
  }
```

:::

::: {.column width="47%"}

### Notes

- Returns a list (so not really a for loop).
- The warning tells us that actually nothing is parallel yet.
- First, we need to register a **backend**.

:::

::::

## Backends

- Multiple backends, installed separately (**doParallel**, **doMC**,
  **doFuture**)
- Load and **register** one before using foreach
- Windows user cannot use forking (which is cheaper)

```{r doparallel, echo = -1, message = FALSE}
options(width = 80)
library(doParallel)

cl <- makeCluster(2) # 2 is the number of threads requested
# cl <- makeForkedCluster(2) # Only on non-Windows

registerDoParallel(cl)
# registerDoParallel(cores = 4) # If not on Windows

res <- foreach(i = seq_len(3)) %dopar%
  {
    cat("Hello from process", Sys.getpid(), "\n")
  }
```

## Combining Results

- `foreach()` always returns a list.
- If you want to reduce your result, use either the `.combine` argument or
  manually reduce the resulting list.

. . .

### `.combine`

```{r}
x <- c(4, 1, 1e2, 2)

foreach(i = seq_len(4), .combine = c) %dopar%
  {
    log(x[[i]])
  }
```

## Exercise

### Part 1

Write a parallelized version of `mean()`. The function should take a vector as
it's first argument and a cluster object as the second argument.

```{r par-mean-intro, eval = FALSE}
par_mean <- function(x) {
  n <- length(x)
  # Your code here
}
```

```{r par-mean, include = FALSE}
par_mean <- function(x) {
  n <- length(x)
  foreach(i = seq_len(n), .combine = sum) %dopar%
    {
      x[[i]] / n
    }
}
```

. . .

### Part 2

Write a parallelized version of `lapply()`. Pass along arguments to the
function.

```{r, include = FALSE}
par_lapply <- function(x, fun, ...) {
  foreach(i = seq_along(x), .combine = sum) %dopar%
    {
      fun(x[[i]], ...)
    }
}

par_lapply(x, sum)
```

## Futures

- An abstraction for a value that may be available at some point in the future.
- Sometimes called promises: "I promise to give you the result later."

. . .

:::: {.columns}

::: {.column width="47%"}

### Without Futures

```{r}
v <- {
  cat("Hello world!\n")
  3.14
}
v
```

:::

::: {.column width="47%"}

### With Futures

```{r}
library(future)
v %<-%
  {
    cat("Hello world!\n")
    3.14
  }
v
```

:::

::::

## Futures Enable Parallelization

- To parallelize a computation, we can use futures.
- A thread is created for each future.
- Other work can be done in the meantime on main thread.
- Once the value is needed, hopefully it is already (or best: just-in-time)
  available.

```{r}
#| eval: false
plan(multisession)

# plan(multicore) # Not on Windows or RStudio!

z %<-%
  {
    # Expensive computation
  }

# Other expensive computations

z # Will block until the value is available
```

## Caveats Regarding Parallelization

- Twice as many cores $\neq$ twice as fast (in practice).
  - Communication overhead
  - Memory overhead
- Thread safety
- Parallelizing functions that are already parallelized internally typicall has
  no (or negative) effect.

## Exercise

Write a program for running ordinary least squares regression in parallel using
the **futures** package.

**Recall:** The OLS estimator is given by $\hat{\beta} = (X^TX)^{-1}X^T y$.

```{r ols-future, include = FALSE}
n <- 100
p <- 10
x <- matrix(rnorm(n * p), n, p)
y <- rnorm(n)

xty %<-%
  {
    crossprod(x, y)
  }
xtx <- crossprod(x)
solve(xtx, xty)
```

## Summary

- First find bottlenecks through profiling
- Benchmark different implementations and use existing solutions
- Vectorize and use domain-specific knowledge to improve performance.

. . .

### Keep in Mind

- Only optimize when it's needed: profile first
- Keep readability in mind: performant code can be hard to read.
- Write tests: performance improvements can introduce bugs.
