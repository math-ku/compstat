---
title: "Likelihood Optimization"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}

error_hook <- knitr::knit_hooks$get("error")
knitr::knit_hooks$set(error = function(x, options) {
  n <- options$linewidth
  if (!is.null(n)) {
    x <- knitr:::split_lines(x)
    if (any(nchar(x) > n)) {
      x <- strwrap(x, width = n)
    }
    x <- paste(x, collapse = "\n\t")
  }
  error_hook(x, options)
})

library(patchwork)

old_options <- options(digits = 4)
```

## Last Time

### Testing

Systematic approach to find errors in code.

. . .

### Debugging

Finding and fixing bugs in code.

## Today

### Optimization for Multinomial Models

We will look at optimization for multinomial models, using an
example on Peppered Moths.

\medskip\pause

We will introduce two different optimization methods for constrained problems:

- General optimization using extended-value functions and zero-order methods.
- Constrained optimization using the barrier method.

# Multinomial Models

## The Peppered Moth

:::: {.columns}

::: {.column width="47%"}

### Alleles

C, Ci, T with frequencies $p_C$, $p_I$, $p_T$ and 
$$
p_C + p_I + p_T = 1.
$$

### Genotypes

CC, CI, CT, II, IT, TT

### Phenotypes

Black, mottled, light-colored

:::

::: {.column width="47%"}

![](../images/peppered-moth.jpg){width=100%}


: Genotype to Phenotype Map

|       | **C** | **I**   | **T**         |
| ----- | ----- | ------- | ------------- |
| **C** | Black | Black   | Black         |
| **I** | Black | Mottled | Mottled       |
| **T** | Black | Mottled | Light-colored |

:::

::::

::: {.notes}

The Peppered Moth is called 'Birkemåler' in Danish. There is a nice collection
of these in different colors in the Zoological Museum. The alleles are ordered
in terms of dominance as C > I > T. Moths with genotype including C are dark.
Moths with genotype TT are light colored. Moths with genotypes II and IT are
mottled.

The peppered moth provided an early demonstration of evolution in the 19th
centure England, where the light colored moth was outnumered by the dark colored
variety. The dark color became advantageous due to the increased polution, where
trees were darkened by soot.

https://en.wikipedia.org/wiki/Peppered_moth_evolution

:::

## The Hardy–Weinberg Equilibrium

According to the
[Hardy–Weinberg equilibrium](https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle),
the genotype frequencies are
$$
p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.
$$

. . .

The complete multinomial log-likelihood is 
$$
\begin{aligned}
  & 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_I) \\ 
  & + 2 n_{II} \log(p*I) + n_{IT} \log(2p_I p_T) + 2 n_{TT} \log(p_T),
\end{aligned}
$$

. . .

We only observe $(n_C, n_I, n_T)$, where
$$
n = \underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} +
\underbrace{n_{IT} + n_{II}}_{=n_I} + \underbrace{n_{TT}}_{=n_T}.
$$

. . .

As a specific data example we have the observation $n_C= 85$, $n_I = 196$, and
$n_T = 341.$

## Multinomial Cell Collapsing

The Peppered Moth example is an example of _cell collapsing_ in a multinomial
model.

\medskip\pause

In general, let $A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K\}$ be a partition
and let
$$
M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}
$$
be the map given by
$$
M((n_1, \ldots, n_K))_j = \sum_{i \in A_j} n_i.
$$

## Multinomial Distribution

If $Y \sim \textrm{Mult}(p, n)$ with $p = (p_1, \ldots, p_K)$ then
$$
X = M(Y) \sim \textrm{Mult}(M(p), n).
$$

. . .

For the peppered moths, $K = 6$ corresponding to the six genotypes, $K_0 = 3$
and the partition corresponding to the phenotypes is
$$\{1, 2, 3\} \cup \{4, 5\} \cup \{6\} = \{1, \ldots, 6\},$$
and $$M(n_1, \ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).$$

## Cell Collapsing

In terms of the $(p_C, p_I)$ parametrization, $p_T = 1 - p_C - p_I$ and
$$p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).$$

. . .

Hence $$M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).$$

. . .

The log-likelihood is,

$$
\begin{aligned} \ell(p_C, p_I) & = n_C \log(p_C^2 +
2p_Cp_I + 2p_Cp_T) \\ & \phantom{={}}+ n_I \log(p_I^2 +2p_Ip_T) + n_T \log
(p_T^2) \end{aligned}
$$

## Standard Form

This is an optimization problem of the following standard form:

$$
\begin{aligned}&\operatorname*{minimize}_{p_C,p_I} && -\ell(p_C, p_I) \\
  &\textrm{subject to} && p_C + p_C - 1 \leq 0\\
  &                    && -p_C \leq 0\\
  &                    && -p_I \leq 0\\
  &                    && p_I - 1 \leq 0\\
  &                    && p_C -1 \leq 0.
\end{aligned}
$$

. . .

### Convex?

. . .

**Yes**, because the negative log-likelihood is convex and so are the
constraints (affine).

### Can We Solve it Using Gradient Descent or Newton's Method?

. . .

**No** (not directly), since the problem is constrained.

```{r likelihood}
#| echo: false
neg_loglik_pep <- function(par, x) {
  pC <- par[1]
  pI <- par[2]
  pT <- 1 - pC - pI

  if (pC > 1 || pC < 0 || pI > 1 || pI < 0 || pT < 0) {
    return(Inf)
  }

  p_dar <- pC^2 + 2 * pC * pI + 2 * pC * pT
  p_mot <- pI^2 + 2 * pI * pT
  p_lig <- pT^2

  -(x[1] * log(p_dar) + x[2] * log(p_mot) + x[3] * log(p_lig))
}
```

##

```{r surface, echo = FALSE}
#| fig-width: 3.9
#| fig-height: 3.9
#| fig-cap: "Contour plot of the negative log-likelihood for the peppered moths
#|   example."
#| warning: false
n <- 100

pC <- seq(0, 1, length.out = n)
pI <- seq(0, 1, length.out = n)
z <- matrix(NA, n, n)

x <- c(85, 196, 341)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- neg_loglik_pep(c(pC[i], pI[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

levels <- exp(seq(log(min_z), log(3000), length.out = 25))

contour(pC, pI, z, levels = levels, col = pal(30), asp = 1, drawlabels = FALSE)
```

## Change of Variables

:::: {.columns}

::: {.column width="47%"}

Let $$p_j = \frac{\exp{(\theta_j)}}{\sum_{i = 1}^K \exp(\theta_i)}.$$

\medskip\pause

Seems like a good idea! Constraints are automatically satisfied.

. . .

### But is it Convex?

**No**, $f$ is no longer convex.

:::

::: {.column width="47%"}

```{r, echo = FALSE, warning = FALSE}
#| fig-height: 3
other_negloglik <- function(theta, x) {
  theta <- c(theta, 0)

  pC <- exp(theta[1]) / sum(exp(theta))
  pI <- exp(theta[2]) / sum(exp(theta))
  pT <- 1 / sum(exp(theta))

  p_dark <- pC * (pC + 2 * pI + 2 * pT)
  p_mottled <- pI * (pI + 2 * pT)
  p_light <- pT^2

  -(x[1] * log(p_dark) + x[2] * log(p_mottled) + x[3] * log(p_light))
}

n <- 100

theta1 <- seq(-19, 19, length.out = n)
theta2 <- seq(-15, 15, length.out = n)
z <- matrix(NA, n, n)

x <- c(40, 196, 50)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- other_negloglik(c(theta1[i], theta2[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

contour(theta1, theta2, z, col = pal(30), asp = 1, drawlabels = FALSE)
x1 <- c(2.25, 8.9)
x2 <- c(-5, 6)
points(x1, x2, pch = 19)
lines(x1, x2, lty = "dotted", lwd = 2)
```

:::

::::

## Optimization

So, we cannot use our existing toolbox. What to do?

\medskip\pause

We will try two different options.

. . .

### General Optimization

Define **extended-value** function, and use general (zero-order) optimization

. . .

### Constrained Optimization (Barrier Method)

Use the barrier method to directly solve the **constrained** optimization
problem.

##

We can code a problem-specific (extended-value) version of the negative
log-likelihood.

```{r likelihood-again}
#| ref-label: likelihood
```

## Zero-Order Nelder-Mead

:::: {.columns align="center"}

::: {.column width="47%"}

```{r moth-optim-NM}
x <- c(85, 196, 341)
x0 <- c(1 / 3, 1 / 3)
optim(x0, neg_loglik_pep, x = x)
```

:::

. . .

::: {.column width="47%"}

### Feasible Starting Point

Some thought has to go into the initial parameter choice.

. . .

```{r moth-error, error = TRUE, linewidth = 38}
optim(
  c(0, 0),
  neg_loglik_pep,
  x = x
)
```

:::

::::

## Log-Likelihood Function Factory

`M()` sums the cells that are collapsed, which we specify in `group` argument.

```{r M-collapse}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}
```

##

Compute the negative log-likelihood for general multinomial cell collapsing
problems.

```{r mult-likelihood-factory}
mult_likelihood <- function(
  x,
  group,
  prob,
  constraint = function(par) TRUE
) {
  force(x)
  force(group)
  force(prob)

  function(params) {
    pr <- prob(params)
    if (!constraint(params) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}
```

## Peppered Moths Specifics

:::: {.columns}

::: {.column width="47%"}

### Multinomial Probabilities

`prob()` maps parameters to the multinomial probability vector.

```{r prob}
prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}
```

:::

. . .

::: {.column width="47%"}

### Constraints

Check of constraints.

```{r constraint}
constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}
```

:::

::::

## Putting it All Together

```{r moth_likelihood}
neg_mult_loglik <- mult_likelihood(
  x = x,
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)
```

. . .

```{r moth-optim}
moth_optim <- optim(x0, neg_mult_loglik)
moth_optim
```


## Barrier Function

Another option is to use the \alert{barrier method}.
 
\medskip\pause

Transform
$$
\begin{aligned}
  & \textrm{minimize}   &  & f(x)                                 \\
  & \textrm{subject to} &  & g_i(x) \leq 0, & \quad i =1,\dots,m. \\
\end{aligned}
$$ \pause
into
$$
\begin{aligned}
  \textrm{minimize}   &  & tf(x) + \phi(x)
\end{aligned}
$$
with the barrier function
$$
\phi(z) = -\sum_{i=1}^m \log(-g_i(z)).
$$

\medskip\pause

For $t$ small, the barrier term dominates and the solution is far from
the boundary.

\medskip\pause

Increasing $t$ puts more weight on the original objective and the solution
approaches the constrained optimum.


```{r}
#| echo: false
#| include: false
f <- function(x1, x2) {
  ((x1 - 2)^2 + (x2 - 1)^2)
}

g <- function(x1, x2) {
  x1 - 1
}


log_barrier <- function(x1, x2) {
  out <- -(log(-g(x1, x2)))
  out[!is.finite(out)] <- Inf
  out[is.nan(out)] <- Inf
  out
}

x1_grid <- seq(-1.5, 1.2, length.out = 101)
x2_grid <- seq(-0.5, 2.5, length.out = 101)
z1 <- outer(x1_grid, x2_grid, f)
z2 <- outer(x1_grid, x2_grid, log_barrier)

plot_barrier <- function(t) {
  z <- t * z1 + z2
  min_z <- min(z, na.rm = TRUE)
  levels <- exp(seq(log(min_z), log(20 + min_z), length.out = 25))
  cols <- hcl.colors(30, "viridis", rev = FALSE)
  argmin <- which(z == min_z, arr.ind = TRUE)
  contour(
    x1_grid,
    x2_grid,
    z,
    levels = levels,
    drawlabels = FALSE,
    col = cols,
    asp = 1,
    xlab = expression(x[1]),
    ylab = expression(x[2])
  )
  abline(v = 1, lty = "dashed", col = "black", lwd = 2)
  points(
    x1_grid[argmin[1, 1]],
    x2_grid[argmin[1, 2]],
    pch = 4,
    col = "darkorange",
    cex = 1.5
  )
  legend(
    "topleft",
    legend = c("Constraint", "Optimum", paste0("t = ", t)),
    col = c("black", "darkorange", NA),
    lty = c("dashed", NA, NA),
    pch = c(NA, 4, NA),
    bg = "white"
  )
}
plot_barrier(0.085)
```

## Example

:::: {.columns align="center"}

::: {.column width="47%"}

Let's assume we are solving
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\
  &\textrm{subject to} && x_1 - 1 \leq 0.
\end{aligned}
$$\pause\bigskip

The \alert{unconstrained} optimum would be at $(2, 1)$, but this is not feasible.

\medskip\pause

Using the barrier method, we solve a sequence of unconstrained problems
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && tf(x) - \log(1 - x_1)
\end{aligned}
$$

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.085)
```

:::

::::

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.1)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.2)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.5)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(10)
```

## Barrier Method Algorithm

\begin{algorithm}[H]
  \caption{Barrier Method}
  \KwData{$t > 0$, $x$ strictly feasible, $\mu > 1$,, $\varepsilon > 0$, $m$ number of constraints}
  \Repeat{$m/t < \varepsilon$ }{
    $x \gets \operatorname*{argmin}_x tf(x) + \phi(x)$\;
    $t \gets \mu t$\;
  }
  \Return{$x$}
\end{algorithm}

\medskip\pause

Guarantees convergence to a $\varepsilon$-suboptimal solution.

## `constrOptim()`

The barrier method can be used via `constrOptim()`.

\medskip\pause

Solves problems with affine/linear inequality constraints of the form
$$Ax \succeq b$$ or, in terms of arguments: `ui %*% theta >= ci`.

. . .

:::: {.columns}

::: {.column width="47%"}

### Moths Example

$$
A = 
\begin{bmatrix}
  1 & 0\\-1 & 0\\0 & 1\\0 & -1\\ -1 & -1
\end{bmatrix},
\quad b = 
\begin{bmatrix}
  0\\-1\\0\\-1\\-1
\end{bmatrix}.
$$

:::

. . .

::: {.column width="47%"}

### Optimization Problem

$$
\begin{aligned}
  &\operatorname*{minimize}_{p} && -\ell(p)       \\
  &\textrm{subject to}          && Ap  \succeq b.
\end{aligned}
$$
with $p = (p_C, p_I)$.

:::

::::

. . .

**Not** standard form---but what `constrOptim()` expects.

## `constrOptim()`

```{r constr-optim-ex}
A <- rbind(
  c(1, 0),
  c(-1, 0),
  c(0, 1),
  c(0, -1),
  c(-1, -1)
)

b <- c(0, -1, 0, -1, -1)

constrOptim(x0, neg_loglik_pep, NULL, ui = A, ci = b, x = x)$par
```

. . .

## Still Requires Feasible Initial Point

```{r const-error, error = TRUE, linewidth = 80}
constrOptim(
  c(0.0, 0.3),
  neg_loglik_pep,
  NULL,
  ui = A,
  ci = b,
  x = x
)
```

## Interior-Point Method

The barrier method is the basis of the interior-point method.

. . .

### Phase I: Initialization

Find a feasible point $x^{(0)}$ by solving the optimization problem

$$
\begin{aligned}&\operatorname*{minimize}_{(x,s)} && s \\
&\textrm{subject to} && g_i(x) \leq s, & \quad i = 1, \dots, m.\end{aligned}
$$

. . .

If $s^* \leq 0$, then $x^* = x^{(0)}$ is feasible.

\medskip\pause

Easy in the peppered moths case! But this is not always the case.

. . .

### Phase II: Barrier Method

Use a barrier method to solve the constrained problem, starting at $x^{(0)}$.

## Multinomial Conditional Distributions

Distribution of $Y_{A_j} = (Y_i)_{i \in A_j}$ conditional on $X$ can be found
too:
$$Y_{A_j} \mid X = x \sim \textrm{Mult}\left( \frac{p_{A_j}}{M(p)_j}, x_j \right).$$

. . .

### Expected Genotype Frequencies

Hence for $k \in A_j$,
$$\operatorname{E} (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.$$

. . .

```{r moth_cond_exp, echo=2:4}
old_options <- options(digits = 3)
group <- c(1, 1, 1, 2, 2, 3)
p <- prob(moth_optim$par)
x[group] * p / M(p, group)[group]
options(digits = old_options$digits)
```

## Summary

- Peppered moths example is simple and the log-likelihood for the observed data
  can easily be computed.
- Constrained optmimization can be solved using the barrier method.
- `optim()` in R is general interface to optimization methods.
- `constrOptim()` is used for linearly constrained optimization.

## Exercise: Absolute-Value Constraints

Solve the following optimization problem:
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && \frac{1}{2}(x - 2)^2 \\
  &\textrm{subject to} && |x| \leq 1.
\end{aligned}
$$

### Steps

1. Rewrite the problem in standard form.
2. Write up the constraints in the form `ui %*% theta >= ci`.
3. Use `constrOptim()` with `ui` and `ci` to solve the problem. You need
   to provide the objective and gradient functions too.

```{r exercise-optim, include = FALSE}
a <- 2

f <- function(x, a) {
  0.5 * (x - a)^2
}

g <- function(x, a) {
  x - a
}

A <- rbind(1, -1)
b <- c(-1, -1)

constrOptim(c(0.5), f, g, ui = A, ci = b, a = a)
```
