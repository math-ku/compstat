---
title: "Likelihood Optimization and the EM Algorithm"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}

error_hook <- knitr::knit_hooks$get("error")
knitr::knit_hooks$set(error = function(x, options) {
  n <- options$linewidth
  if (!is.null(n)) {
    x <- knitr:::split_lines(x)
    if (any(nchar(x) > n)) {
      x <- strwrap(x, width = n)
    }
    x <- paste(x, collapse = "\n\t")
  }
  error_hook(x, options)
})

library(patchwork)

old_options <- options(digits = 4)
```

## Last Time

### Testing

Systematic approach to find errors in code.

. . .

### Debugging

Finding and fixing bugs in code.

## Today

### Optimization for Multinomial Models

We will look at optimization for multinomial models, using an example on
Peppered Moths.

\medskip\pause

We will introduce two different optimization methods for constrained problems:

- General optimization using extended-value functions and zero-order methods.
- Constrained optimization using the barrier method.

. . .

### The EM Algorithm

We will introduce the EM algorithm. A useful method for likelihood optimization
in the presence of missing data or latent variables.

# Multinomial Models

## The Peppered Moth

:::: {.columns}

::: {.column width="47%"}

### Alleles

C, Ci, T with frequencies $p_C$, $p_I$, $p_T$ and
$$
p_C + p_I + p_T = 1.
$$

### Genotypes

CC, CI, CT, II, IT, TT

### Phenotypes

Black, mottled, light-colored

:::

::: {.column width="47%"}

![](../images/peppered-moth.jpg){width=100%}


: Genotype to Phenotype Map

|       | **C** | **I**   | **T**         |
| ----- | ----- | ------- | ------------- |
| **C** | Black | Black   | Black         |
| **I** | Black | Mottled | Mottled       |
| **T** | Black | Mottled | Light-colored |

:::

::::

::: {.notes}

The Peppered Moth is called 'Birkemåler' in Danish. There is a nice collection
of these in different colors in the Zoological Museum. The alleles are ordered
in terms of dominance as C > I > T. Moths with genotype including C are dark.
Moths with genotype TT are light colored. Moths with genotypes II and IT are
mottled.

The peppered moth provided an early demonstration of evolution in the 19th
centure England, where the light colored moth was outnumered by the dark colored
variety. The dark color became advantageous due to the increased polution, where
trees were darkened by soot.

https://en.wikipedia.org/wiki/Peppered_moth_evolution

:::

## The Hardy–Weinberg Equilibrium

According to the
[Hardy–Weinberg equilibrium](https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle),
the genotype frequencies are
$$
p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.
$$

. . .

The complete multinomial log-likelihood is
$$
\begin{aligned}
  & 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_I) \\
  & + 2 n_{II} \log(p*I) + n_{IT} \log(2p_I p_T) + 2 n_{TT} \log(p_T),
\end{aligned}
$$

. . .

We only observe $(n_C, n_I, n_T)$, where
$$
n = \underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} +
\underbrace{n_{IT} + n_{II}}_{=n_I} + \underbrace{n_{TT}}_{=n_T}.
$$

. . .

As a specific data example we have the observation $n_C= 85$, $n_I = 196$, and
$n_T = 341.$

## Multinomial Cell Collapsing

The Peppered Moth example is an example of _cell collapsing_ in a multinomial
model.

\medskip\pause

In general, let $A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K\}$ be a partition
and let
$$
M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}
$$
be the map given by
$$
M((n_1, \ldots, n_K))_j = \sum_{i \in A_j} n_i.
$$

## Multinomial Distribution

If $Y \sim \textrm{Mult}(p, n)$ with $p = (p_1, \ldots, p_K)$ then
$$
X = M(Y) \sim \textrm{Mult}(M(p), n).
$$

. . .

For the peppered moths, $K = 6$ corresponding to the six genotypes, $K_0 = 3$
and the partition corresponding to the phenotypes is
$$
\{1, 2, 3\} \cup \{4, 5\} \cup \{6\} = \{1, \ldots, 6\},
$$
and
$$
M(n_1, \ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).
$$

## Cell Collapsing

In terms of the $(p_C, p_I)$ parametrization, $p_T = 1 - p_C - p_I$ and
$$
p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).
$$

. . .

Hence
$$
M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).
$$

. . .

The log-likelihood is
$$
\begin{aligned} \ell(p_C, p_I) & = n_C \log(p_C^2 +
2p_Cp_I + 2p_Cp_T) \\ & \phantom{={}}+ n_I \log(p_I^2 +2p_Ip_T) + n_T \log
(p_T^2) \end{aligned}
$$

## Standard Form

This is an optimization problem of the following standard form:
$$
\begin{aligned}&\operatorname*{minimize}_{p_C,p_I} && -\ell(p_C, p_I) \\
  &\textrm{subject to} && p_C + p_C - 1 \leq 0\\
  &                    && -p_C \leq 0\\
  &                    && -p_I \leq 0\\
  &                    && p_I - 1 \leq 0\\
  &                    && p_C -1 \leq 0.
\end{aligned}
$$

. . .

### Convex?

. . .

**Yes**, because the negative log-likelihood is convex and so are the
constraints (affine).

### Can We Solve it Using Gradient Descent or Newton's Method?

. . .

**No** (not directly), since the problem is constrained.

```{r likelihood}
#| echo: false
neg_loglik_pep <- function(par, x) {
  pC <- par[1]
  pI <- par[2]
  pT <- 1 - pC - pI

  if (pC > 1 || pC < 0 || pI > 1 || pI < 0 || pT < 0) {
    return(Inf)
  }

  p_dar <- pC^2 + 2 * pC * pI + 2 * pC * pT
  p_mot <- pI^2 + 2 * pI * pT
  p_lig <- pT^2

  -(x[1] * log(p_dar) + x[2] * log(p_mot) + x[3] * log(p_lig))
}
```

##

```{r surface, echo = FALSE}
#| fig-width: 3.9
#| fig-height: 3.9
#| fig-cap: "Contour plot of the negative log-likelihood for the peppered moths
#|   example."
#| warning: false
n <- 100

pC <- seq(0, 1, length.out = n)
pI <- seq(0, 1, length.out = n)
z <- matrix(NA, n, n)

x <- c(85, 196, 341)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- neg_loglik_pep(c(pC[i], pI[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

levels <- exp(seq(log(min_z), log(3000), length.out = 25))

contour(pC, pI, z, levels = levels, col = pal(30), asp = 1, drawlabels = FALSE)
```

## Change of Variables

:::: {.columns}

::: {.column width="47%"}

Let
$$
p_j = \frac{\exp{(\theta_j)}}{\sum_{i = 1}^K \exp(\theta_i)}.
$$

\medskip\pause

Seems like a good idea! Constraints are automatically satisfied.

. . .

### But is it Convex?

**No**, $f$ is no longer convex.

:::

::: {.column width="47%"}

```{r, echo = FALSE, warning = FALSE}
#| fig-height: 3
other_negloglik <- function(theta, x) {
  theta <- c(theta, 0)

  pC <- exp(theta[1]) / sum(exp(theta))
  pI <- exp(theta[2]) / sum(exp(theta))
  pT <- 1 / sum(exp(theta))

  p_dark <- pC * (pC + 2 * pI + 2 * pT)
  p_mottled <- pI * (pI + 2 * pT)
  p_light <- pT^2

  -(x[1] * log(p_dark) + x[2] * log(p_mottled) + x[3] * log(p_light))
}

n <- 100

theta1 <- seq(-19, 19, length.out = n)
theta2 <- seq(-15, 15, length.out = n)
z <- matrix(NA, n, n)

x <- c(40, 196, 50)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- other_negloglik(c(theta1[i], theta2[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

contour(theta1, theta2, z, col = pal(30), asp = 1, drawlabels = FALSE)
x1 <- c(2.25, 8.9)
x2 <- c(-5, 6)
points(x1, x2, pch = 19)
lines(x1, x2, lty = "dotted", lwd = 2)
```

:::

::::

## Optimization

So, we cannot use our existing toolbox. What to do?

\medskip\pause

We will try two different options.

. . .

### General Optimization

Define **extended-value** function, and use general (zero-order) optimization

. . .

### Constrained Optimization (Barrier Method)

Use the barrier method to directly solve the **constrained** optimization
problem.

##

We can code a problem-specific (extended-value) version of the negative
log-likelihood.

```{r likelihood-again}
#| ref-label: likelihood
```

## Zero-Order Nelder-Mead

:::: {.columns align="center"}

::: {.column width="47%"}

```{r moth-optim-NM}
x <- c(85, 196, 341)
x0 <- c(1 / 3, 1 / 3)
optim(x0, neg_loglik_pep, x = x)
```

:::

. . .

::: {.column width="47%"}

### Feasible Starting Point

Some thought has to go into the initial parameter choice.

. . .

```{r moth-error, error = TRUE, linewidth = 38}
optim(
  c(0, 0),
  neg_loglik_pep,
  x = x
)
```

:::

::::

## Log-Likelihood Function Factory

`M()` sums the cells that are collapsed, which we specify in `group` argument.

```{r M-collapse}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}
```

##

Compute the negative log-likelihood for general multinomial cell collapsing
problems.

```{r mult-likelihood-factory}
mult_likelihood <- function(
  x,
  group,
  prob,
  constraint = function(par) TRUE
) {
  force(x)
  force(group)
  force(prob)

  function(params) {
    pr <- prob(params)
    if (!constraint(params) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}
```

## Peppered Moths Specifics

:::: {.columns}

::: {.column width="47%"}

### Multinomial Probabilities

`prob()` maps parameters to the multinomial probability vector.

```{r prob}
prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}
```

:::

. . .

::: {.column width="47%"}

### Constraints

Check of constraints.

```{r constraint}
constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}
```

:::

::::

## Putting it All Together

```{r moth_likelihood}
neg_mult_loglik <- mult_likelihood(
  x = x,
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)
```

. . .

```{r moth-optim}
moth_optim <- optim(x0, neg_mult_loglik)
moth_optim
```


## Barrier Function

Another option is to use the \alert{barrier method}.

\medskip\pause

Transform
$$
\begin{aligned}
  & \textrm{minimize}   &  & f(x)                                 \\
  & \textrm{subject to} &  & g_i(x) \leq 0, & \quad i =1,\dots,m. \\
\end{aligned}
$$
\pause into
$$
\begin{aligned}
  \textrm{minimize}   &  & tf(x) + \phi(x)
\end{aligned}
$$
with the barrier function
$$
\phi(z) = -\sum_{i=1}^m \log(-g_i(z)).
$$

\medskip\pause

For $t$ small, the barrier term dominates and the solution is far from the
boundary.

\medskip\pause

Increasing $t$ puts more weight on the original objective and the solution
approaches the constrained optimum.


```{r}
#| echo: false
#| include: false
f <- function(x1, x2) {
  ((x1 - 2)^2 + (x2 - 1)^2)
}

g <- function(x1, x2) {
  x1 - 1
}


log_barrier <- function(x1, x2) {
  out <- -(log(-g(x1, x2)))
  out[!is.finite(out)] <- Inf
  out[is.nan(out)] <- Inf
  out
}

x1_grid <- seq(-1.5, 1.2, length.out = 101)
x2_grid <- seq(-0.5, 2.5, length.out = 101)
z1 <- outer(x1_grid, x2_grid, f)
z2 <- outer(x1_grid, x2_grid, log_barrier)

plot_barrier <- function(t) {
  z <- t * z1 + z2
  min_z <- min(z, na.rm = TRUE)
  levels <- exp(seq(log(min_z), log(20 + min_z), length.out = 25))
  cols <- hcl.colors(30, "viridis", rev = FALSE)
  argmin <- which(z == min_z, arr.ind = TRUE)
  contour(
    x1_grid,
    x2_grid,
    z,
    levels = levels,
    drawlabels = FALSE,
    col = cols,
    asp = 1,
    xlab = expression(x[1]),
    ylab = expression(x[2])
  )
  abline(v = 1, lty = "dashed", col = "black", lwd = 2)
  points(
    x1_grid[argmin[1, 1]],
    x2_grid[argmin[1, 2]],
    pch = 4,
    col = "darkorange",
    cex = 1.5
  )
  legend(
    "topleft",
    legend = c("Constraint", "Optimum", paste0("t = ", t)),
    col = c("black", "darkorange", NA),
    lty = c("dashed", NA, NA),
    pch = c(NA, 4, NA),
    bg = "white"
  )
}
plot_barrier(0.085)
```

## Example

:::: {.columns align="center"}

::: {.column width="47%"}

Let's assume we are solving
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\
  &\textrm{subject to} && x_1 - 1 \leq 0.
\end{aligned}
$$
\pause\bigskip

The \alert{unconstrained} optimum would be at $(2, 1)$, but this is not
feasible.

\medskip\pause

Using the barrier method, we solve a sequence of unconstrained problems
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && tf(x) - \log(1 - x_1)
\end{aligned}
$$

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.085)
```

:::

::::

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.1)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.2)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.5)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(10)
```

## Barrier Method Algorithm

\begin{algorithm}[H]
  \caption{Barrier Method}
  \KwData{$t > 0$, $x$ strictly feasible, $\mu > 1$,, $\varepsilon > 0$, $m$ number of constraints}
  \Repeat{$m/t < \varepsilon$ }{
    $x \gets \operatorname*{argmin}_x tf(x) + \phi(x)$\;
    $t \gets \mu t$\;
  }
  \Return{$x$}
\end{algorithm}

\medskip\pause

Guarantees convergence to a $\varepsilon$-suboptimal solution.

## `constrOptim()`

The barrier method can be used via `constrOptim()`.

\medskip\pause

Solves problems with affine/linear inequality constraints of the form
$$
Ax \succeq b
$$
or, in terms of arguments: `ui %*% theta >= ci`.

. . .

:::: {.columns}

::: {.column width="47%"}

### Moths Example

$$
A =
\begin{bmatrix}
  1 & 0\\-1 & 0\\0 & 1\\0 & -1\\ -1 & -1
\end{bmatrix},
\quad b =
\begin{bmatrix}
  0\\-1\\0\\-1\\-1
\end{bmatrix}.
$$

:::

. . .

::: {.column width="47%"}

### Optimization Problem

$$
\begin{aligned}
  &\operatorname*{minimize}_{p} && -\ell(p)       \\
  &\textrm{subject to}          && Ap  \succeq b.
\end{aligned}
$$
with $p = (p_C, p_I)$.

:::

::::

. . .

**Not** standard form---but what `constrOptim()` expects.

## `constrOptim()`

```{r constr-optim-ex}
A <- rbind(
  c(1, 0),
  c(-1, 0),
  c(0, 1),
  c(0, -1),
  c(-1, -1)
)

b <- c(0, -1, 0, -1, -1)

constrOptim(x0, neg_loglik_pep, NULL, ui = A, ci = b, x = x)$par
```

## Still Requires Feasible Initial Point

```{r const-error, error = TRUE, linewidth = 80}
constrOptim(
  c(0.0, 0.3),
  neg_loglik_pep,
  NULL,
  ui = A,
  ci = b,
  x = x
)
```

## Interior-Point Method

The barrier method is the basis of the interior-point method.

. . .

### Phase I: Initialization

Find a feasible point $x^{(0)}$ by solving the optimization problem

$$
\begin{aligned}&\operatorname*{minimize}_{(x,s)} && s \\
&\textrm{subject to} && g_i(x) \leq s, & \quad i = 1, \dots, m.\end{aligned}
$$

. . .

If $s^* \leq 0$, then $x^* = x^{(0)}$ is feasible.

\medskip\pause

Easy in the peppered moths case! But this is not always the case.

. . .

### Phase II: Barrier Method

Use a barrier method to solve the constrained problem, starting at $x^{(0)}$.

## Multinomial Conditional Distributions

Distribution of $Y_{A_j} = (Y_i)_{i \in A_j}$ conditional on $X$ can be found
too:
$$
Y_{A_j} \mid X = x \sim \textrm{Mult}\left( \frac{p_{A_j}}{M(p)_j}, x_j \right).
$$

. . .

### Expected Genotype Frequencies

Hence for $k \in A_j$,
$$
\operatorname{E} (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.
$$

. . .

```{r moth_cond_exp, echo=2:4}
old_options <- options(digits = 3)
group <- c(1, 1, 1, 2, 2, 3)
p <- prob(moth_optim$par)
x[group] * p / M(p, group)[group]
options(digits = old_options$digits)
```

## Summary

- Peppered moths example is simple and the log-likelihood for the observed data
  can easily be computed.
- Constrained optmimization can be solved using the barrier method.
- `optim()` in R is general interface to optimization methods.
- `constrOptim()` is used for linearly constrained optimization.

# The EM Algorithm

```{r moth_likelihood2, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, prob, constraint = function(par) TRUE) {
  function(par) {
    pr <- prob(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}

loglik <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), loglik)
```

## Incomplete Data Likelihood

We observe data $X$ but imagine (unobserved) complete data $Y = (X, Z)$ where $Z$ are latent / missing variables.

\medskip\pause

Let the joint density/pmf be
$$
f(x, z \mid \theta) = p(z \mid \theta)\, p(x \mid z, \theta).
$$

\medskip\pause

The (observed) or marginal likelihood is
$$
g(x \mid \theta) =
\begin{cases}
\sum_z f(x, z \mid \theta), & \text{(discrete } Z)\\[4pt]
\int f(x, z \mid \theta)\, \mathrm{d}z, & \text{(continuous } Z).
\end{cases}
$$

Often this sum/integral is \alert{hard} to maximize directly.

## Complete vs Observed Log-Likelihood

Complete log-likelihood:
$$
\ell_c(\theta; x, z) = \log f(x, z \mid \theta).
$$

Observed (incomplete) log-likelihood:
$$
\ell(\theta) = \log g(x \mid \theta) = \log \sum_z f(x, z \mid \theta).
$$

Strategy: Replace the unobserved $z$ by its conditional distribution given the observed $x$ and current parameter iterate.

## The Q Function

Define
$$
Q(\theta \mid \theta') :=
\operatorname{E}_{Z \mid X = x,\, \theta'}\big[\ell_c(\theta; X, Z)\big]
= \sum_z \big(\log f(x, z \mid \theta)\big)\, \Pr(Z = z \mid X = x, \theta')
$$
(discrete case; integral analog in continuous case).

\medskip\pause

We can compute $Q$ if:

1. We can evaluate the complete log-likelihood.
2. We can compute (or at least work with) the conditional distribution $Z \mid X = x, \theta'$.

## EM Algorithm (Definition)

Given an initial $\theta^{(0)}$:

- E-step: Compute (or update a representation of) $Q(\theta \mid \theta^{(n)})$.
- M-step: $\theta^{(n+1)} = \arg\max_{\theta} Q(\theta \mid \theta^{(n)})$.

Repeat until convergence.

## Monotonicity (Sketch)

Key inequality:
$$
\ell(\theta) - \ell(\theta') \;\ge\; Q(\theta \mid \theta') - Q(\theta' \mid \theta).
$$

Reason (discrete case sketch): Write
$$
\ell(\theta) = \log \sum_z f(x, z \mid \theta') \frac{f(x, z \mid \theta)}{f(x, z \mid \theta')}
= \log \operatorname{E}_{Z \mid X=x,\theta'}\!\left[\frac{f(x, Z \mid \theta)}{f(x, Z \mid \theta')}\right]
\ge \operatorname{E}_{Z \mid X=x,\theta'}\!\left[\log \frac{f(x, Z \mid \theta)}{f(x, Z \mid \theta')}\right],
$$
(Jensen) which rearranges to the inequality above. Thus maximizing $Q(\theta \mid \theta^{(n)})$ gives $\ell(\theta^{(n+1)}) \ge \ell(\theta^{(n)})$. Strict increase if $Q$ increases strictly.

## Practical Pattern

1. Specify latent structure $Z$.
2. Write complete log-likelihood $\ell_c$.
3. Derive conditional expectations needed for $Q$ (E-step).
4. Maximize $Q$ in $\theta$ (M-step) — often closed form.
5. Iterate until parameter change or log-likelihood change is small.

## Conditional Distribution (Needed for E-Step)

We only need $\Pr(Z = z \mid X = x, \theta^{(n)}) \propto f(x, z \mid \theta^{(n)})$ (normalize over $z$). Use it to compute required conditional expectations.

## EM Iterations (Illustration)

![Four iterations of the EM algorithm.](../images/em-iterations.png){width=87%}

## Multinomial Complete Data Likelihood

If $Y \sim \textrm{Mult}(p, n)$ the complete data log-likelihood is
$$
\ell_{\textrm{complete}}(p) = \sum_{i=1}^K Y_i \log(p_i).
$$

. . .

Thus
$$
Q(p \mid p') = \operatorname{E}_{p'}( \ell_{\textrm{complete}}(p) \mid X = x) = \sum_{i=1}^K \operatorname{E}_{p'}( Y_i \mid X = x) \log(p_i)
$$
for any $X = M(Y)$.

## E-step for Multinomial Model

For the multinomial model with $M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}$
the cell collapsing map corresponding to the partition
$A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K \}$,
$$
\operatorname{E}_p (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.
$$
for $k \in A_j$.

. . .

### Abstract E-Step Implementation

```{r}
EStep_mult <- function(p, x, group) {
  x[group] * p / M(p, group)[group]
}
```

## Multinomial MLE for Moths

With $y = (n_{CC}, n_{CI}, n_{CT}, n_{II}, n_{IT}, n_{TT})^T$ a complete
observation, it can be shown that the MLE is
$$
\begin{aligned}
  \hat{p}_C & = (n_{CC} + (n_{CI} + n_{CT}) / 2) / n \\
  \hat{p}_I & = ((n_{CI} + n_{IT}) / 2 + n_{II}) / n
\end{aligned}
$$
where $n = n_{CC} + n_{CI} + n_{CT} + n_{II} + n_{IT} + n_{TT}$.

\medskip\pause

For us $\hat{p} = \frac{1}{n} \symbf{X} y$.

:::: {.columns}

::: {.column width="47%"}

```{r x-matrix}
X <- matrix(
  c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2,
  2,
  6,
  byrow = TRUE
)
```

:::

::: {.column width="47%"}

```{r x-matrix-eval}
X
```

:::

::::

## Abstract M-Step

MLE of complete log-likelihood is linear estimator in $y / n$

```{r mstep-mult}
MStep_mult <- function(n, X) {
  as.vector(X %*% n / (sum(n)))
}
```

\medskip\pause

`EStep_mult()` and `MStep_mult()` are abstract implementations. Requires
arguments `group` and `X`.

\medskip\pause

M-step only implemented when complete-data MLE is _linear estimator_.

## EM Factory for Multinomial Models

```{r EM-factory}
EM_multinomial <- function(x, group, prob, X) {
  force(x)
  force(group)
  force(prob)
  force(X)

  EStep <- function(p) EStep_mult(prob(p), x, group)
  MStep <- function(n) MStep_mult(n, X)

  function(par, epsilon = 1e-6, maxit = 20, cb = NULL) {
    for (i in seq_len(maxit)) {
      par0 <- par
      par <- MStep(EStep(par))
      if (!is.null(cb)) {
        cb()
      }
      if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {
        break
      }
    }
    par
  }
}
```

## Peppered Moths EM Algorithm

```{r, results='hold'}
EM <- EM_multinomial(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  X = matrix(
    c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2,
    2,
    6,
    byrow = TRUE
  )
)

EM(c(0.3, 0.3))
moth_optim$par
```

## Inside the EM Algorithm

```{r}
library(CSwR)
EM_tracer <- tracer("par")
EM(c(0.3, 0.3), cb = EM_tracer$tracer)

summary(EM_tracer)
```

## Tracing the EM Algorithm

```{r em-tracer}
EM_tracer <- tracer(c("par0", "par"), Delta = 0)
phat <- EM(c(0.3, 0.3), epsilon = 0, cb = EM_tracer$tracer)
phat
EM_trace <- summary(EM_tracer)
tail(EM_trace)
```

## Adding Computed Values

```{r}
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))
EM_trace <- transform(
  EM_trace,
  n = seq_len(nrow(EM_trace)),
  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2),
  loglik = loglik_pep(par.1, par.2)
)
```

##

```{r plot-curves, echo = FALSE, warning = FALSE}
#| fig-width: 5.4
#| fig-height: 4.5
loglik_pep <- Vectorize(function(p1, p2) loglik(c(p1, p2)))

group <- c(1, 1, 1, 2, 2, 3)

EStep <- function(p) EStep_mult(prob(p), x, group)
MStep <- function(n) MStep_mult(n, X)

epsilon <- 1e-6
maxit <- 5

exp_vals <- matrix(NA, nrow = maxit, ncol = length(group))

par <- c(0.3, 0.3)

level_curves <- vector("list", length = maxit)

ll <- double(maxit)
x <- c(85, 196, 341)

n_mat <- 100
pC <- seq(0, 1, length.out = n_mat)
pI <- seq(0, 1, length.out = n_mat)

pars <- vector("list", length = maxit)

for (i in seq_len(maxit)) {
  par0 <- par
  y <- EStep(par)

  x_new <- M(y, group)
  z <- matrix(NA, n_mat, n_mat)

  x <- c(85, 196, 341)

  for (k in seq_len(n_mat)) {
    for (j in seq_len(n_mat)) {
      # z[k, j] <- neg_loglik_pep(c(pC[k], pI[j]), x)
      val <- -sum(y * log(prob(c(pC[k], pI[j]))))
      if (is.finite(val)) {
        z[k, j] <- val
      } else {
        z[k, j] <- -Inf
      }
    }
  }

  level_curves[[i]] <- z

  ll[i] <- -sum(y * log(prob(par)))

  exp_vals[i, ] <- y
  par <- MStep(y)
  pars[[i]] <- par
}

# min_z <- min(sapply(level_curves, min))
# levels <- exp(seq(log(min_z), log(3000), length.out = 25))

pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

par_optim <- c(0.07084, 0.18874)

old_par <- par(no.readonly = TRUE)

par(mfrow = c(2, 2), mai = c(0.5, 0.5, 0.1, 0.1))
for (i in 1:4) {
  contour(
    pC,
    pI,
    level_curves[[i]],
    nlevels = 25,
    col = pal(25),
    asp = 1,
    drawlabels = FALSE
  )
  text(0.9, 0.9, labels = paste0("k = ", i))

  points(par_optim[1], par_optim[2], pch = 19, col = "dark orange")
  points(pars[[i]][1], pars[[i]][2], pch = 19)
}
par(old_par)
```

##

```{r tracer-plot1, echo = FALSE, fig.height = 5.5}
ggplot(EM_trace, aes(n, par_norm_diff)) +
  geom_point() +
  geom_line() +
  labs(
    y = expression(paste(
      "||",
      theta^{
        (n)
      } -
        theta^{
          (n - 1)
        },
      "||"
    ))
  ) +
  scale_y_log10()
```

Note the log-axis. The EM-algorithm converges linearly.

## Linear Convergence

The log-rate of the convergence can be estimated by least-squares.

```{r linear-convergence}
log_lm <- lm(log(par_norm_diff) ~ n, data = EM_trace)
exp(coefficients(log_lm)["n"])
```

\medskip\pause

It is very small in this case implying fast convergence.

\medskip\pause

This is not always the case. If the log-likelihood is flat, the EM-algorithm can
become quite slow with a rate close to 1.

##

```{r loglikelihood-convergence, warning=FALSE, echo = FALSE, fig.cap="Log-likelihood convergence"}
#| fig-width: 4
mutate(EM_trace, diff_loglik = loglik - min(loglik)) |>
  ggplot(aes(n, diff_loglik)) +
  labs(y = expression(l[theta] - min(l))) +
  geom_point() +
  geom_line() +
  scale_y_log10()
```

## Optimization and Statistics

The EM-algorithm is a general algorithm for numerical optimization of a
log-likelihood function. It works by iteratively optimizing
$$
Q(\theta \mid \theta^{(n)}) = E_{\theta^{(n)}}(\log f(Y \mid \theta) \mid X = x).
$$

\medskip\pause

For numerical optimization of $Q$ or variants of EM (generalized EM algorithms)
the gradient and Hessian of $Q$ can be useful.

\medskip\pause

For statistics we need the observed Fisher information (Hessian of the negative
log-likelihood for the observed data).

## Exercise: Absolute-Value Constraints

Solve the following optimization problem:
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && \frac{1}{2}(x - 2)^2 \\
  &\textrm{subject to} && |x| \leq 1.
\end{aligned}
$$

### Steps

1. Rewrite the problem in standard form.
2. Write up the constraints in the form `ui %*% theta >= ci`.
3. Use `constrOptim()` with `ui` and `ci` to solve the problem. You need to
   provide the objective and gradient functions too.

```{r exercise-optim, include = FALSE}
a <- 2

f <- function(x, a) {
  0.5 * (x - a)^2
}

g <- function(x, a) {
  x - a
}

A <- rbind(1, -1)
b <- c(-1, -1)

constrOptim(c(0.5), f, g, ui = A, ci = b, a = a)
```
