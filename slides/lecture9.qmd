---
title: "Likelihood Optimization and the EM Algorithm"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}

error_hook <- knitr::knit_hooks$get("error")
knitr::knit_hooks$set(error = function(x, options) {
  n <- options$linewidth
  if (!is.null(n)) {
    x <- knitr:::split_lines(x)
    if (any(nchar(x) > n)) {
      x <- strwrap(x, width = n)
    }
    x <- paste(x, collapse = "\n\t")
  }
  error_hook(x, options)
})

library(patchwork)

old_options <- options(digits = 4)
```

```{r moth_likelihood2, echo = FALSE}
M <- function(y, group) {
  as.vector(tapply(y, group, sum))
}

mult_likelihood <- function(x, group, prob, constraint = function(par) TRUE) {
  function(par) {
    pr <- prob(par)
    if (!constraint(par) || any(pr > 1) || any(pr < 0)) {
      return(Inf)
    }
    -sum(x * log(M(pr, group)))
  }
}

prob <- function(p) {
  p[3] <- 1 - p[1] - p[2]
  c(
    p[1]^2,
    2 * p[1] * p[2],
    2 * p[1] * p[3],
    p[2]^2,
    2 * p[2] * p[3],
    p[3]^2
  )
}

constraint <- function(par) {
  par[1] <= 1 &&
    par[1] >= 0 &&
    par[2] <= 1 &&
    par[2] >= 0 &&
    1 - par[1] - par[2] >= 0
}

loglik <- mult_likelihood(
  x = c(85, 196, 341),
  group = c(1, 1, 1, 2, 2, 3),
  prob = prob,
  constraint = constraint
)

moth_optim <- optim(c(0.3, 0.3), loglik)
```

## Last Time

### Testing

Systematic approach to find errors in code.

. . .

### Debugging

Finding and fixing bugs in code.

## Today

### Optimization for Multinomial Models

We will look at optimization for multinomial models, using an example on
Peppered Moths.

\medskip\pause

We will introduce two different optimization methods for constrained problems:

- General optimization using extended-value functions and zero-order methods.
- Constrained optimization using the barrier method.

. . .

### The EM Algorithm

We will introduce the EM algorithm. A useful method for likelihood optimization
in the presence of missing data or latent variables.

# Multinomial Models

## The Peppered Moth

:::: {.columns}

::: {.column width="47%"}

### Alleles

C, Ci, T with frequencies $p_C$, $p_I$, $p_T$ and
$$
p_C + p_I + p_T = 1.
$$

### Genotypes

CC, CI, CT, II, IT, TT

### Phenotypes

Black, mottled, light-colored

:::

::: {.column width="47%"}

![](../images/peppered-moth.jpg){width=100%}


: Genotype to Phenotype Map

|       | **C** | **I**   | **T**         |
| ----- | ----- | ------- | ------------- |
| **C** | Black | Black   | Black         |
| **I** | Black | Mottled | Mottled       |
| **T** | Black | Mottled | Light-colored |

:::

::::

::: {.notes}

The Peppered Moth is called 'Birkemåler' in Danish. There is a nice collection
of these in different colors in the Zoological Museum. The alleles are ordered
in terms of dominance as C > I > T. Moths with genotype including C are dark.
Moths with genotype TT are light colored. Moths with genotypes II and IT are
mottled.

The peppered moth provided an early demonstration of evolution in the 19th
century England, where the light colored moth was outnumered by the dark colored
variety. The dark color became advantageous due to the increased polution, where
trees were darkened by soot.

https://en.wikipedia.org/wiki/Peppered_moth_evolution

:::

## The Hardy–Weinberg Equilibrium

According to the
[Hardy–Weinberg equilibrium](https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle),
the genotype frequencies are
$$
p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.
$$

. . .

The complete multinomial log-likelihood is
$$
\begin{aligned}
  & 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_T) \\
  & + 2 n_{II} \log(p_I) + n_{IT} \log(2p_I p_T) + 2 n_{TT} \log(p_T),
\end{aligned}
$$

. . .

We only observe $(n_C, n_I, n_T)$, where
$$
n = \underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} +
\underbrace{n_{IT} + n_{II}}_{=n_I} + \underbrace{n_{TT}}_{=n_T}.
$$

. . .

As a specific data example we have the observation $n_C= 85$, $n_I = 196$, and
$n_T = 341.$

## Multinomial Cell Collapsing

The Peppered Moth example is an example of _cell collapsing_ in a multinomial
model.

\medskip\pause

In general, let $A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K\}$ be a partition
and let
$$
M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}
$$
be the map given by
$$
M((n_1, \ldots, n_K))_j = \sum_{i \in A_j} n_i.
$$

## Multinomial Distribution

If $Y \sim \textrm{Mult}(p, n)$ with $p = (p_1, \ldots, p_K)$ then
$$
X = M(Y) \sim \textrm{Mult}(M(p), n).
$$

. . .

For the peppered moths, $K = 6$ corresponding to the six genotypes, $K_0 = 3$
and the partition corresponding to the phenotypes is
$$
\{1, 2, 3\} \cup \{4, 5\} \cup \{6\} = \{1, \ldots, 6\},
$$
and
$$
M(n_1, \ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).
$$

## Cell Collapsing

In terms of the $(p_C, p_I)$ parametrization, $p_T = 1 - p_C - p_I$ and
$$
p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).
$$

. . .

Hence
$$
M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).
$$

. . .

The log-likelihood is
$$
\begin{aligned}
  \ell(p_C, p_I) & = n_C \log(p_C^2 + 2p_Cp_I + 2p_Cp_T) \\
                 & \phantom{={}}+ n_I \log(p_I^2 +2p_Ip_T) + n_T \log (p_T^2)
\end{aligned}
$$

## Standard Form

This is an optimization problem of the following standard form:
$$
\begin{aligned}&\operatorname*{minimize}_{p_C,p_I} && -\ell(p_C, p_I) \\
  &\textrm{subject to} && p_C + p_C - 1 \leq 0\\
  &                    && -p_C \leq 0\\
  &                    && -p_I \leq 0\\
  &                    && p_I - 1 \leq 0\\
  &                    && p_C -1 \leq 0.
\end{aligned}
$$

. . .

### Convex?

. . .

**Yes**, because the negative log-likelihood is convex and so are the
constraints (affine).

### Can We Solve it Using Gradient Descent or Newton's Method?

. . .

**No** (not directly), since the problem is constrained.

```{r likelihood}
#| echo: false
neg_loglik_pep <- function(par, x) {
  pC <- par[1]
  pI <- par[2]
  pT <- 1 - pC - pI

  if (pC > 1 || pC < 0 || pI > 1 || pI < 0 || pT < 0) {
    return(Inf)
  }

  p_dar <- pC^2 + 2 * pC * pI + 2 * pC * pT
  p_mot <- pI^2 + 2 * pI * pT
  p_lig <- pT^2

  -(x[1] * log(p_dar) + x[2] * log(p_mot) + x[3] * log(p_lig))
}
```

##

```{r surface, echo = FALSE}
#| fig-width: 3.9
#| fig-height: 3.9
#| fig-cap: "Contour plot of the negative log-likelihood for the peppered moths
#|   example."
#| warning: false
n <- 100

pC <- seq(0, 1, length.out = n)
pI <- seq(0, 1, length.out = n)
z <- matrix(NA, n, n)

x <- c(85, 196, 341)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- neg_loglik_pep(c(pC[i], pI[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

levels <- exp(seq(log(min_z), log(3000), length.out = 25))

contour(pC, pI, z, levels = levels, col = pal(30), asp = 1, drawlabels = FALSE)
```

## Change of Variables

:::: {.columns}

::: {.column width="47%"}

Let
$$
p_j = \frac{\exp{(\theta_j)}}{\sum_{i = 1}^K \exp(\theta_i)}.
$$

\medskip\pause

Seems like a good idea! Constraints are automatically satisfied.

. . .

### But is it Convex?

**No**, $f$ is no longer convex.

:::

::: {.column width="47%"}

```{r, echo = FALSE, warning = FALSE}
#| fig-height: 3
other_negloglik <- function(theta, x) {
  theta <- c(theta, 0)

  pC <- exp(theta[1]) / sum(exp(theta))
  pI <- exp(theta[2]) / sum(exp(theta))
  pT <- 1 / sum(exp(theta))

  p_dark <- pC * (pC + 2 * pI + 2 * pT)
  p_mottled <- pI * (pI + 2 * pT)
  p_light <- pT^2

  -(x[1] * log(p_dark) + x[2] * log(p_mottled) + x[3] * log(p_light))
}

n <- 100

theta1 <- seq(-19, 19, length.out = n)
theta2 <- seq(-15, 15, length.out = n)
z <- matrix(NA, n, n)

x <- c(40, 196, 50)

for (i in seq_len(n)) {
  for (j in seq_len(n)) {
    z[i, j] <- other_negloglik(c(theta1[i], theta2[j]), x)
  }
}
pal <- function(n) hcl.colors(n, "viridis", rev = FALSE)

min_z <- min(z)

contour(theta1, theta2, z, col = pal(30), asp = 1, drawlabels = FALSE)
x1 <- c(2.25, 8.9)
x2 <- c(-5, 6)
points(x1, x2, pch = 19)
lines(x1, x2, lty = "dotted", lwd = 2)
```

:::

::::

## Optimization

So, we cannot use our existing toolbox. What to do?

\medskip\pause

We will try two different options.

. . .

### General Optimization

Define **extended-value extension**, and use general (zero-order) optimization.

. . .

### Constrained Optimization (Barrier Method)

Use the barrier method to directly solve the **constrained** optimization
problem.

## Extended-Value Extension

:::: {.columns}

::: {.column width="47%"}

If $f$ is convex, its extended-value extension is
$$
\tilde{f}(x) =
  \begin{cases}
    f(x), & x \in \operatorname{dom} f, \\
    +\infty, & \text{otherwise}.
  \end{cases}
$$

. . .

### Example {.example}

Consider the function
$$
f(p) = p (p - 1), \quad p \in [0, 1].
$$

. . .

Its extended-value extension is
$$
\tilde{f}(p) =
  \begin{cases}
    p (p - 1), & p \in [0, 1], \\
    +\infty, & \text{otherwise}.
  \end{cases}
$$

:::

::: {.column width="47%"}


```{r}
#| echo: false
#| fig-height: 3.4
#| fig-cap: Extended-value extension of a simple function.
p <- seq(-0.5, 1.5, length.out = 101)
f <- function(p) {
  ifelse(p < 0 | p > 1, 10, p * (p - 1))
}
plot(
  p,
  f(p),
  type = "l",
  ylim = c(-0.3, 0.1),
  xlab = "p",
  ylab = expression(tilde(f)(p))
)
```

:::

::::


##

We can code a problem-specific (extended-value) version of the negative
log-likelihood.

```{r likelihood-again}
#| ref-label: likelihood
```

## `optim()`

R provides a general-purpose optimization function `optim()`.

\medskip\pause

Supports several optimization methods, including:

- Nelder-Mead (default)
- BFGS (quasi-Newton)
- CG (conjugate gradient)
- L-BFGS-B (box-constrained quasi-Newton)
- SANN (simulated annealing)

\medskip\pause

Can supply gradient, but also compute it numerically (default).

## `optim()` Example

```{r optim-ex}
obj <- function(x) {
  (x[1] - 1)^2 + (x[2] - 2.5)^2
}
optim(c(0, 0), obj, method = "BFGS")
```

. . .

### Alternatives

- `nlm()` for unconstrained optimization\pause
- The **optimx** package for more methods\pause

See the
[CRAN task view on optimization](https://cran.r-project.org/web/views/Optimization.html)

## Zero-Order Nelder-Mead

:::: {.columns align="center"}

::: {.column width="47%"}

```{r moth-optim-NM}
x <- c(85, 196, 341)
x0 <- c(1 / 3, 1 / 3)
optim(x0, neg_loglik_pep, x = x)
```

:::

. . .

::: {.column width="47%"}

### Feasible Starting Point

Some thought has to go into the initial parameter choice.

. . .

```{r moth-error, error = TRUE, linewidth = 38}
optim(
  c(0, 0),
  neg_loglik_pep,
  x = x
)
```

:::

::::

## Barrier Function

Another option is to use the \alert{barrier method}.

\medskip\pause

Transform
$$
\begin{aligned}
  & \textrm{minimize}   &  & f(x)                                 \\
  & \textrm{subject to} &  & g_i(x) \leq 0, & \quad i =1,\dots,m. \\
\end{aligned}
$$
\pause into
$$
\begin{aligned}
  \textrm{minimize}   &  & tf(x) + \phi(x)
\end{aligned}
$$
with the barrier function
$$
\phi(z) = -\sum_{i=1}^m \log(-g_i(z)).
$$

\medskip\pause

For $t$ small, the barrier term dominates and the solution is far from the
boundary.

\medskip\pause

Increasing $t$ puts more weight on the original objective and the solution
approaches the constrained optimum.


```{r}
#| echo: false
#| include: false
f <- function(x1, x2) {
  ((x1 - 2)^2 + (x2 - 1)^2)
}

g <- function(x1, x2) {
  x1 - 1
}


log_barrier <- function(x1, x2) {
  out <- -(log(-g(x1, x2)))
  out[!is.finite(out)] <- Inf
  out[is.nan(out)] <- Inf
  out
}

x1_grid <- seq(-1.5, 1.2, length.out = 101)
x2_grid <- seq(-0.5, 2.5, length.out = 101)
z1 <- outer(x1_grid, x2_grid, f)
z2 <- outer(x1_grid, x2_grid, log_barrier)

plot_barrier <- function(t) {
  z <- t * z1 + z2
  min_z <- min(z, na.rm = TRUE)
  levels <- exp(seq(log(min_z), log(20 + min_z), length.out = 25))
  cols <- hcl.colors(30, "viridis", rev = FALSE)
  argmin <- which(z == min_z, arr.ind = TRUE)
  contour(
    x1_grid,
    x2_grid,
    z,
    levels = levels,
    drawlabels = FALSE,
    col = cols,
    asp = 1,
    xlab = expression(x[1]),
    ylab = expression(x[2])
  )
  abline(v = 1, lty = "dashed", col = "black", lwd = 2)
  points(
    x1_grid[argmin[1, 1]],
    x2_grid[argmin[1, 2]],
    pch = 4,
    col = "darkorange",
    cex = 1.5
  )
  legend(
    "topleft",
    legend = c("Constraint", "Optimum", paste0("t = ", t)),
    col = c("black", "darkorange", NA),
    lty = c("dashed", NA, NA),
    pch = c(NA, 4, NA),
    bg = "white"
  )
}
plot_barrier(0.085)
```

## Example

:::: {.columns align="center"}

::: {.column width="47%"}

Let's assume we are solving
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && f(x) = (x_1 - 2)^2 + (x_2 - 1)^2 \\
  &\textrm{subject to} && x_1 - 1 \leq 0.
\end{aligned}
$$
\pause\bigskip

The \alert{unconstrained} optimum would be at $(2, 1)$, but this is not
feasible.

\medskip\pause

Using the barrier method, we solve a sequence of unconstrained problems
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && tf(x) - \log(1 - x_1)
\end{aligned}
$$

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.085)
```

:::

::::

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.1)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.2)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(0.5)
```

## 

```{r}
#| echo: false
#| fig-cap: "Contour plots of the barrier-penalized objective."
#| fig-height: 3.6
#| fig-width: 3.2
plot_barrier(10)
```

## Barrier Method Algorithm

\begin{algorithm}[H]
  \caption{Barrier Method}
  \KwData{$t > 0$, $x$ strictly feasible, $\mu > 1$,, $\varepsilon > 0$, $m$ number of constraints}
  \Repeat{$m/t < \varepsilon$ }{
    $x \gets \operatorname*{argmin}_x tf(x) + \phi(x)$\;
    $t \gets \mu t$\;
  }
  \Return{$x$}
\end{algorithm}

\medskip\pause

Guarantees convergence to a $\varepsilon$-suboptimal solution.

## `constrOptim()`

The barrier method can be used via `constrOptim()`.

\medskip\pause

Solves problems with affine/linear inequality constraints of the form
$$
Ax \succeq b
$$
or, in terms of arguments: `ui %*% theta >= ci`.

. . .

:::: {.columns}

::: {.column width="47%"}

### Moths Example

$$
A =
\begin{bmatrix}
  1 & 0\\-1 & 0\\0 & 1\\0 & -1\\ -1 & -1
\end{bmatrix},
\quad b =
\begin{bmatrix}
  0\\-1\\0\\-1\\-1
\end{bmatrix}.
$$

:::

. . .

::: {.column width="47%"}

### Optimization Problem

$$
\begin{aligned}
  &\operatorname*{minimize}_{p} && -\ell(p)       \\
  &\textrm{subject to}          && Ap  \succeq b.
\end{aligned}
$$
with $p = (p_C, p_I)$.

:::

::::

. . .

**Not** standard form---but what `constrOptim()` expects.

## `constrOptim()`

```{r constr-optim-ex}
A <- rbind(
  c(1, 0),
  c(-1, 0),
  c(0, 1),
  c(0, -1),
  c(-1, -1)
)

b <- c(0, -1, 0, -1, -1)

constrOptim(x0, neg_loglik_pep, NULL, ui = A, ci = b, x = x)$par
```

## Still Requires Feasible Initial Point

```{r const-error, error = TRUE, linewidth = 80}
constrOptim(
  c(0.0, 0.3),
  neg_loglik_pep,
  NULL,
  ui = A,
  ci = b,
  x = x
)
```

## Interior-Point Method

The barrier method is the basis of the interior-point method.

. . .

### Phase I: Initialization

Find a feasible point $x^{(0)}$ by solving the optimization problem

$$
\begin{aligned}&\operatorname*{minimize}_{(x,s)} && s \\
&\textrm{subject to} && g_i(x) \leq s, & \quad i = 1, \dots, m.\end{aligned}
$$

. . .

If $s^* \leq 0$, then $x^* = x^{(0)}$ is feasible.

\medskip\pause

Easy in the peppered moths case! But this is not always the case.

. . .

### Phase II: Barrier Method

Use a barrier method to solve the constrained problem, starting at $x^{(0)}$.

## Multinomial Conditional Distributions

Distribution of $Y_{A_j} = (Y_i)_{i \in A_j}$ conditional on $X$ can be found
too:
$$
Y_{A_j} \mid X = x \sim \textrm{Mult}\left( \frac{p_{A_j}}{M(p)_j}, x_j \right).
$$

. . .

### Expected Genotype Frequencies

Hence for $k \in A_j$,
$$
\operatorname{E} (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}.
$$

. . .

```{r moth_cond_exp, echo=2:4}
old_options <- options(digits = 3)
group <- c(1, 1, 1, 2, 2, 3)
p <- prob(moth_optim$par)
x[group] * p / M(p, group)[group]
options(digits = old_options$digits)
```

# The EM Algorithm

## The Expectation-Maximization (EM) Algorithm

:::: {.columns}

::: {.column width="47%"}

### Goal

What we want is the MLE
$$
\hat{\theta} = \arg\max_\theta \ell(\theta \mid X)
$$
with $\ell(\theta \mid X) = \log p(X \mid \theta)$.

. . .

### Problem {.alert}

The likelihood requires summing/integrating over unobserved variables Z:
$$
p(X \mid \theta) = \sum_Z p(X, Z \mid \theta).
$$

\medskip\pause

This is often computationally infeasible.

:::

::: {.column width="47%"}

### Key Idea {.example}

Work instead with the complete-data likelihood:
$$
p(X, Z \mid \theta),
$$
which is often easier to handle.

:::

::::

## The Algorithm

### E-step

Compute expected complete log-likelihood:
$$
Q(\theta\mid\theta^{(t)})=\E_{Z \mid X,\theta^{(t)}}\left( \log p(X, Z \mid \theta) \right)
$$

\medskip\pause

### M-Step

Maximize expected complete log-likelihood:
$$
\theta^{(t+1)}=\arg\max_\theta Q(\theta\mid\theta^{(t)}).
$$

### Repeat

Until convergence.

## Why EM Works

### Key Inequality

$$
\ell(\theta) = \log p(X \mid \theta) \geq \E_{q(Z)}[\log p(X,Z \mid \theta)] - \E_{q(Z)}[\log q(Z)].
$$

. . .

### E-Step

Choose
$$
q^{(t)}(Z) = p(Z \mid X, \theta^{(t)}).
$$

\medskip\pause

Then inequality is tight, since
$$
\kl(q^{(t)} || p(Z \mid X, \theta^{(t)})) = 0.
$$

. . .

### M-Step

Update $\theta$ by maximizing the bound. This guarantees non-decreasing
log-likelihood:
$$
\ell(\theta^{(t+1)}) \geq \ell(\theta^{(t)}).
$$

## From the Lower Bound to $Q$

### Variational Bound

$$
F(q,\theta) = \E_q\big(\log p(X,Z \mid \theta)\big) - \E_q\big[\log q(Z)\big].
$$

\medskip\pause

With
$$
q^{(t)}(Z) = p(Z \mid X, \theta^{(t)})
$$
we have
$$
F(q^{(t)},\theta) = Q(\theta \mid \theta^{(t)}) + H(q^{(t)}),
$$
where $H(q^{(t)})$ is entropy, independent of $\theta$.

\medskip\pause

### Consequence

M-step:
$$
\theta^{(t+1)} = \arg\max_\theta Q(\theta \mid \theta^{(t)}).
$$

. . .

So EM maximizes the variational lower bound.

##

![Four iterations of the EM algorithm.](../images/em-iterations.png){width=87%}

## EM Example: Two Hidden Coins

### Setup

- Two coins, A and B, unknown biases $\theta_A$ and $\theta_B$.\pause
- Each experiment: pick a coin at random (latent variable $Z_i$) and flip it
  10 times.\pause
- Observed data: $X_i$: number of heads in each experiment.\pause

. . .

### Goal

Estimate $\theta_A$ and $\theta_B$ (MLE) using EM.

. . .

### Complete Likelihood

If we knew which coin was used in each experiment, the complete-data
likelihood would be
$$
p(X,Z \mid \theta) = \sum_i \left( \symbf{1}_{Z_i = A} \text{Binomial}(X_i \mid 10, \theta_A)
+ \symbf{1}_{Z_i = B} \text{Binomial}(X_i \mid 10, \theta_B) \right).
$$

## The E-Step: The Q-Function

The log-likelihood of the complete data (for a single experiment) is
$$
\log p(X_i, Z_i \mid \theta) =
\symbf{1}_{Z_i = A} \log \text{Bin}(X_i \mid 10, \theta_A)
+ \symbf{1}_{Z_i = B} \log \text{Bin}(X_i \mid 10, \theta_B).
$$

\medskip\pause

Take expection over $Z_i$ given $X_i$ and current parameters:
$$
Q(\theta \mid \theta^{(t)}) = \sum_i \gamma_{i}^{(t)} \log \text{Bin}(X_i \mid 10, \theta_A)  + \sum_i (1 - \gamma_{i}^{(t)}) \log \text{Bin}(X_i \mid 10, \theta_B).
$$

\medskip\pause

$$
\begin{aligned}
\gamma_i^{(t)} &= \E_Z\left(\symbf{1}_{Z_i = A} \mid X_i, \theta_A^{(t)}, \theta_B^{(t)}\right) \\
               &= \Pr(Z_i = A \mid X_i, \theta_A^{(t)}, \theta_B^{(t)} )\\
               &= \frac{\text{Binomial}(X_i \mid 10, \theta_A^{(t)})}{\text{Binomial}(X_i \mid 10, \theta_A^{(t)}) + \text{Binomial}(X_i \mid 10, \theta_B^{(t)})}.
\end{aligned}
$$

\medskip\pause

This called the **responsibility** of coin $A$ for experiment $i$.

##

: Example After One Iteration

| Experiment | Heads | $\gamma$ | 
|------------|-------|----------|
| 1          |     7 |     0.78 |
| 2          |     4 |     0.35 |
| 3          |     9 |     0.90 |

## M-Step: Update Coin Biases

Maximizing $Q$ with respect to $\theta_A$ and $\theta_B$ gives
$$
\theta_A^{(t+1)} = \frac{\sum_i \gamma_{i}^{(t)} X_i}{\sum_i 10 \cdot \gamma_{i}^{(t)}}, \quad
\theta_B^{(t+1)} = \frac{\sum_i (1 - \gamma_{i}^{(t)}) X_i}{\sum_i 10 \cdot (1 - \gamma_{i}^{(t)})}.
$$

. . .

### Intuition

Compute expected # of heads for each coin and divide by expected # of flips.

\medskip\pause

: Example After One Iteration

| Coin | Updated $\theta$ |
|------|------------------|
| A    |             0.80 |
| B    |             0.42 |

##

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 3.9
#| fig-cap: "EM algorithm iterations for the two-coin example."
set.seed(1)
n <- 1000
heads <- {
  ind <- sample(c(TRUE, FALSE), n, TRUE)
  rbinom(n, 10, ifelse(ind, 0.8, 0.4))
}
maxit <- 20
theta <- c(0.6, 0.5) # start
theta_hist <- matrix(NA, nrow = maxit, ncol = 2)
colnames(theta_hist) <- c("thetaA", "thetaB")
loglik <- numeric(maxit)

for (t in 1:maxit) {
  # record before update
  theta_hist[t, ] <- theta
  pA <- dbinom(heads, 10, theta[1])
  pB <- dbinom(heads, 10, theta[2])
  mix <- 0.5 * pA + 0.5 * pB
  loglik[t] <- sum(log(mix))
  if (t == maxit) {
    break
  }
  gamma <- pA / (pA + pB) # responsibilities
  theta[1] <- sum(gamma * heads) / (10 * sum(gamma))
  theta[2] <- sum((1 - gamma) * heads) / (10 * sum(1 - gamma))
}

matplot(
  1:maxit,
  theta_hist,
  type = "b",
  pch = 19,
  col = c("blue", "red"),
  lty = 1,
  xlab = "Iteration",
  ylab = expression(hat(theta)),
  ylim = c(0, 1)
)
abline(
  h = c(0.8, 0.4),
  col = c("blue", "red"),
  lty = 2
)
legend(
  "bottomright",
  legend = c(expression(hat(theta)[A]), expression(hat(theta)[B])),
  col = c("blue", "red"),
  lty = c(1, 1),
  pch = c(19, 19)
)
```

## 

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 3.5
#| fig-cap: "Convergence of the log-likelihood in the two-coin EM example."
plot(
  1:maxit,
  max(loglik) - loglik + 1e-10,
  type = "b",
  pch = 19,
  xlab = "Iteration",
  ylab = expression(max(l) - l),
  log = "y"
)
```

## Connection to $Q$-Function

Recall:
$$
Q(\theta \mid \theta^{(t)}) = \E_{Z \mid X, \theta^{(t)}}[\log p(X,Z \mid \theta)]
$$

\medskip\pause

In the coin-flip example:
$$
Q(\theta_A, \theta_B \mid \theta^{(t)}_A, \theta^{(t)}_B)
= \sum_i \gamma_{i}^{(t)} \log \text{Bin}(X_i \mid 10, \theta_A)
+ \sum_i (1 - \gamma_{i}^{(t)}) \log \text{Bin}(X_i \mid 10, \theta_B)
$$

. . .

- **E-step:** compute $\gamma_{i}^{(t)}$, the expected values (responsibilities)
  of the latent variables.
- **M-step:** maximize $Q$ with respect to $\theta_A$ and $\theta_B$: weighted
  averages of heads.

\medskip\pause

Intuition: $Q$ is the expected complete-data log-likelihood; EM alternates
between **expectation** (filling in latent variables) and **maximization**
(updating parameters).

## Summary

### Likelihood Optimization

Peppered moths example is simple and the log-likelihood for the observed data
can easily be computed.

\medskip\pause

Constrained optimization can be solved using the barrier method.

. . .

### The EM Algorithm

The EM algorithm is a general approach to likelihood optimization in the
presence of missing data or latent variables.

\medskip\pause

Each iteration is guaranteed to increase the observed data log-likelihood.


## Exercise: Absolute-Value Constraints

Solve the following optimization problem:
$$
\begin{aligned}
  &\operatorname*{minimize}_{x}              && \frac{1}{2}(x - 2)^2 \\
  &\textrm{subject to} && |x| \leq 1.
\end{aligned}
$$

### Steps

1. Rewrite the problem in standard form.
2. Write up the constraints in the form `ui %*% theta >= ci`.
3. Use `constrOptim()` with `ui` and `ci` to solve the problem. You need to
   provide the objective and gradient functions too.

```{r exercise-optim, include = FALSE}
a <- 2

f <- function(x, a) {
  0.5 * (x - a)^2
}

g <- function(x, a) {
  x - a
}

A <- rbind(1, -1)
b <- c(-1, -1)

constrOptim(c(0.5), f, g, ui = A, ci = b, a = a)
```

## Next Time

We will continue with more examples of the EM algorithm.

\medskip\pause

We will show how to solve the peppered moths example using EM.

\medskip\pause

We will also talk about convergence and the **generalized** EM algorithm.
