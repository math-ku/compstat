---
title: "Optimization"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
library(patchwork)
library(tidyverse)

old_options <- options(digits = 4)
```

```{r data, echo=FALSE, cache = TRUE}
vegetables <- read.table(
  here::here("data", "vegetables.txt"),
  header = TRUE,
  colClasses = c(
    "numeric",
    "numeric",
    "factor",
    "factor",
    "numeric",
    "numeric",
    "numeric"
  )
)[, c(1, 2, 3)]
```

## Recap: Monte Carlo Methods

### Standard Monte Carlo

Estimate $\E h(X)$ with $X \sim f$ by
$$
\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n h(X_i) \quad \text{with } X_i \overset{iid}{\sim} f.
$$
with

. . .

### Importance Sampling

Use the "trick" that
$$
\mu = \int h(x) f(x) \ dx = \int h(x) \frac{f(x)}{g(x)} g(x) \ dx = \int h(x) w^*(x) g(x) \ dx
$$
and estimate
$$
\hat{\mu}_n^{IS} = \frac{1}{n} \sum_{i=1}^n h(X_i) w^*(X_i) \quad \text{with } X_i \overset{iid}{\sim} g.
$$

## Recap: Importance Sampling

### Self-Normalized Importance Sampling

When $f$ is known only up to a normalizing constant, we can still use importance
sampling by estimating normalized weights.

. . .

### Picking a Good Importance Distribution

Pick a importance distribution $g$ that covers the important parts of $f$, with
heavy tails, and from which we can easily sample.


## Today

### Optimization

What is an optimization problem?

\medskip\pause

We learn about different kinds of optimization problems.

. . .

### Convex Optimization

Convex optimization problems are a special class of optimization problems that
are easier to solve.

. . .

### Algorithms

We talk about zero, first, and second order methods.

\medskip\pause

We learn about gradient descent and Newton's method.

# Optimization

## Optimization

The process of trying to optimize an objective function $f$.


\medskip\pause

Typically, we are interested in either **minimizing** or **maximizing** the
function.

\medskip\pause

Since minimizing $f$ is equivalent to maximizing $-f$, we will focus on
minimization.

. . .

### Examples {.example}

- Fitting a generalized linear model by minimizing the negative log-likelihood
  \pause
- Finding the shortest path in a network \pause
- Minimizing variance in a portfolio of financial assets.

## Standard Form

In _standard form_, an optimization problem is written as
$$
\begin{aligned}
  &\operatorname*{minimize}_x  && f(x) \\
  &\textrm{subject to}         && g_i(x) \leq 0, \quad i = 1, \ldots, m \\
  &                            && h_j(x) = 0, \quad j = 1, \ldots, k.
  \end{aligned}
$$
where

- $f$ is the objective function,
- $g_i$ are inequality constraints, and
- $h_j$ are equality constraints.

. . .

### Solution

We write the solution as
$$
x^* = \operatorname{argmin}_x f(x)
$$

## Example: Ordinary Least Squares Regression (OLS)

OLS regression is an optimization problem.

\medskip\pause

In standard form, it is
$$
\begin{aligned}
  &\operatorname*{minimize}_{\beta \in \mathbb{R}^p} &&f(\beta) = \frac{1}{2} \lVert y - X\beta\rVert_2^2.
\end{aligned}
$$

. . .

That is, we have no constraints.

. . .

### Solution

The solution is
$$
\beta^* = \operatorname{argmin}_{\beta \in \mathbb{R}^p} f(\beta).
$$

## Types of Optimization Problems

### Convexity

- **Convex**
- Quasiconvex
- Nonconvex

. . .

### Smoothness

- **Smooth**
- Nonsmooth

. . .

### Constraints

- **Unconstrained**
- Constrained

## Convex Optimization Problems

Standard form:

$$
\begin{aligned}
  &\operatorname*{minimize}_x     && f(x) \\
  &\textrm{subject to}            && g_i(x) \leq 0, \quad i = 1, \ldots, m, \\
  &                               && a_i^T x = b_i, \quad j = 1, \ldots, k,
  \end{aligned}
$$

where $f$ and $g_i$ are convex.

## Convex Functions

A function $f$ is convex iff for all $x_1, x_2 \in \operatorname{dom} f$ and
$\lambda \in [0, 1]$
$$
f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2).
$$

This is known as **Jensen's inequality**.

\medskip\pause

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 2
#| fig-cap: "Jensen's inequality for a convex function."
library(ggplot2)

# Convex function
f <- function(x) x^2

# Points for Jensen's inequality
x1 <- -0.5
x2 <- 1
lambda <- 0.3
x_mid <- lambda * x1 + (1 - lambda) * x2

df <- data.frame(
  x = c(seq(-2, 2, length.out = 100), x1, x2),
  y = c(f(seq(-2, 2, length.out = 100)), f(x1), f(x2)),
  type = c(rep("f", 100), rep("chord", 2))
)

pts <- data.frame(
  x = c(x1, x2),
  y = c(f(x1), f(x2))
)

labels <- c(
  f = expression(f(x)),
  chord = expression(lambda * f(x[1]) + (1 - lambda) * f(x[2]))
)

ggplot(df, aes(x, y)) +
  geom_line(aes(linetype = type, color = type)) +
  geom_point(data = pts, aes(x, y)) +
  scale_linetype_manual(
    values = c(f = "solid", chord = "dashed"),
    labels = labels
  ) +
  scale_color_manual(
    values = c(f = "black", chord = "steelblue"),
    labels = labels
  ) +
  geom_text(
    aes(x, y),
    data = pts,
    label = c(expression(x[1]), expression(x[2])),
    nudge_x = c(-0.3, 0.3)
  ) +
  labs(color = NULL, linetype = NULL) +
  labs(y = expression(f(x)), x = "x")
```

## First-Order Condition for Convexity

If $f$ is **differentiable**, then the condition is equivalent to
$$
f(x_1) \geq f(x_2) + \nabla f(x_2)^T(x_1 - x_2).
$$

. . .

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 2
#| fig-cap: "First-order condition for convexity."
library(ggplot2)
# Convex function
f <- function(x) x^2
f_prime <- function(x) 2 * x
# Points for tangent line
x0 <- 0.2
x_vals <- seq(-1, 2, length.out = 100)
df <- data.frame(
  x = c(x_vals, x0),
  y = c(f(x_vals), f(x0)),
  type = c(rep("f", 100), "tangent")
)
pts <- data.frame(
  x = x0,
  y = f(x0)
)

labels <- c(
  f = expression(f(x)),
  tangent = expression(f(x[2]) + nabla * f(x[2])^T * (x[1] - x[2]))
)

ggplot(df, aes(x, y)) +
  geom_line(aes(linetype = type, color = type)) +
  geom_abline(
    slope = f_prime(x0),
    intercept = f(x0) - f_prime(x0) * x0,
    linetype = "dashed",
    color = "steelblue"
  ) +
  geom_point(
    data = pts,
    aes(x, y),
    label = expression(x[2]),
    nudge_y = 0.3
  ) +
  scale_linetype_manual(
    values = c(f = "solid", tangent = "dashed"),
    labels = labels
  ) +
  scale_color_manual(
    values = c(f = "black", tangent = "steelblue"),
    labels = labels
  ) +
  lims(y = c(-1, 4)) +
  labs(color = NULL, linetype = NULL) +
  labs(y = expression(f(x)), x = "x")
```


## Optimality

Any (local) minimum is **global** (but not necessarily unique).

### First-Order Conditions

For differentiable $f$, $x^*$ is optimal iff
$$
\nabla f(x^*)^T(x - x^*) \geq 0\quad\text{for all}\quad x \in C,
$$
where $C$ is the feasible set.

. . .

### Second-Order Conditions

If $f$ is twice-differentiable, then $x^*$ is optimal iff
$$
\nabla^2f(x^*) \succeq 0.
$$

## Quasiconvex Optimization Problems

### Standard Form

$$
\begin{aligned}
  &\operatorname*{minimize}_x && f(x) \\
  &\textrm{subject to}        && g_i(x) \leq 0, \quad i = 1, \ldots, m, \\
  &                               && A^T x = b,
  \end{aligned}
$$
where $g_i$ are convex but $f$ is **quasiconvex**.

. . .

### Examples

- Utility functions (economics) \pause
- The floor function $f(x) = \lfloor x \rfloor$

. . .

### Algorithms

Won't cover here, but for instance the **bisection** method.

## Nonconvex Optimization Problems

- Generally hard to solve.
- No standard form (can be anything).
- Local minima are not necessarily global.

. . .

### Examples

- Deep learning
- Hyperparameter tuning (cross-validation)
- Combinatorial problems

. . .

### Algorithms

Won't cover here, but typically **stochastic algorithms** (next week) or
**global optimization methods** (not in this course).

##


```{r convexity-types, echo = FALSE }
#| warning: false
#| fig-height: 3.4
#| fig-width: 5.4
#| fig-cap: "$x^2$ (convex), $\\sin(x + \\pi)$ (non-convex),
#|   and $\\sqrt{|x|}$ (quasi-convex)."
convex_function <- function(x) x^2
non_convex_function <- function(x) sin(x + pi)
quasi_convex_function <- function(x) sqrt(abs(x))

# Define the gradients
convex_gradient <- function(x) 2 * x
non_convex_gradient <- function(x) cos(x + pi)
quasi_convex_gradient <- function(x) sign(x) * 0.5 * abs(x)^(-0.5)

# Create a sequence of x values
x_vals <- seq(-2, 2, length.out = 100)

# Create data frames for plotting
convex_df <- data.frame(
  x = x_vals,
  y = convex_function(x_vals),
  gradient = convex_gradient(x_vals),
  type = "Convex"
)
non_convex_df <- data.frame(
  x = x_vals,
  y = non_convex_function(x_vals),
  gradient = non_convex_gradient(x_vals),
  type = "Non-Convex"
)
quasi_convex_df <- data.frame(
  x = x_vals,
  y = quasi_convex_function(x_vals),
  gradient = quasi_convex_gradient(x_vals),
  type = "Quasi-Convex"
)

# Combine data frames
df <- bind_rows(convex_df, non_convex_df, quasi_convex_df)

# Function to add Jensen's inequality line segment
add_jensens_line <- function(df, func) {
  x1 <- -1.5
  x2 <- 0.1
  y1 <- func(x1)
  y2 <- func(x2)
  df %>%
    mutate(
      jensens_y = ifelse(
        x >= x1 & x <= x2,
        y1 + (y2 - y1) * (x - x1) / (x2 - x1),
        NA
      )
    )
}

# Add Jensen's inequality line segments
convex_df <- add_jensens_line(convex_df, convex_function)
non_convex_df <- add_jensens_line(non_convex_df, non_convex_function)
quasi_convex_df <- add_jensens_line(quasi_convex_df, quasi_convex_function)

# Combine data frames with Jensen's line
df_with_jensens <- bind_rows(convex_df, non_convex_df, quasi_convex_df)

# Plot the objective functions with Jensen's inequality line segments using facets
p1 <- ggplot(df_with_jensens, aes(x = x)) +
  geom_line(aes(y = y)) +
  geom_line(aes(y = jensens_y), linetype = "dashed", color = "steelblue4") +
  labs(y = expression(objective(x))) +
  facet_wrap(~type, scales = "free_y")

# Plot the gradients using facets
p2 <- ggplot(df, aes(x = x, y = gradient)) +
  geom_line() +
  labs(y = expression(nabla * objective(x))) +
  facet_wrap(~type, scales = "free_y")

# Print the plots
p1 /
  p2 +
  plot_layout(axis_title = "collect", guides = "collect", axes = "collect")
```

# Solving Optimization Problems

## Types of Optimization Methods

### Direct (Exact) Methods

Provides a solution up to machine precision.

\medskip\pause

**Examples:** solving a linear system of equations

. . .

### Iterative Optimization

Incrementally update optimization variable until some convergence criterion
is met.

\medskip\pause

**Examples:** gradient descent, hill-climbing

. . .

## Direct Methods

Analytical solution, up to machine precision.

### Example: OLS

Solve
$$
X^TX \beta = X^Ty
$$
for $\beta \in \mathbb{R}^p$.

\medskip\pause

How do you solve this in practice? In R you call

```r
solve(crossprod(X), crossprod(X, y))
```

. . .

but what happens \alert{under the hood}?


## Iterative Methods

Update a current best guess iteratively until it is "good enough".

. . .

### Taxonomy by Order

Can be categorized by order, in terms of the information used

- **0-order**: only use function value ($f(x)$)
- **first-order**: also use first derivative ($f'(x)$)
- **second-order**: also use second derivative ($f''(x)$)

## Zero-Order Methods

Use only the function value.

### Examples

- Hill climbing
- Bisection
- Grid search (cross validation)

## First-Order Methods

Use the gradient of the function (**first** derivative).

### Examples

- **Gradient descent**
- Conjugate gradient

## Second-Order Methods

Use the Hessian of the function (**second** derivative).

### Examples

- **Newton's Method**
- Trust-region methods

## Gradient Descent

The quintessential first-order method

\medskip\pause
:::: {.columns}

::: {.column width="47%"}

\begin{algorithm}[H]
  \caption{Gradient descent}
  \KwData{Step size $t > 0$}
  \Repeat{convergence}{
    $x \gets x - t \nabla f(x)$\;
  }
  \Return{$x$}
\end{algorithm}

:::

::: {.column width="47%"}

```{r}
#| echo: false

```

:::

::::


. . .

### Properties

- Converges to a local minimum if $f$ is convex and differentiable and $t$ small
  enough.
- Requires $f$ to be differentiable and the gradient to be continuous.

## Derivation of Update

Take the second-order Taylor expansion of $f$ around $x$:
$$
f(y) \approx \hat{f}(y) = f(x) + \nabla f(x)^T (y - x) + \frac{1}{2}(y- x)^T \nabla^2 f(x) (y - x).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

Replace $\nabla^2 f(x)$ with $\frac{1}{t} I$ and minimize w.r.t $y$ to get
$$
x^+ = x - t\nabla f(x).
$$

But how do we choose $t$?

:::

::: {.column width="47%"}

![](../images/gd-approx.png){width=85%}

:::

::::

##

![Step size is too small](../images/gd-stepsize-small.png){width=50%}

##

![Step size is too large](../images/gd-stepsize-large.png){width=50%}

##

![Step size is well-adjusted!](../images/gd-stepsize-good.png){width=50%}

## Lipschitz Constant

If
$$
\lVert \nabla f(x) - \nabla f(y)\rVert_2 \leq L \lVert x - y\rVert_2
$$
then $L$ is the Lipschitz constant of $\nabla f$.

### Example: OLS

$$
L = \lVert X^TX \rVert_2
$$

. . .

### Line Searching

In practice, we may not know $L$ or it may be difficult to compute.

## Analytical Step Size

### Theorem

For gradient descent with step size $t \leq 1/L$ and Lipschitz constant of
$\nabla f$ being $L$, we have
$$
f(x^{(k)}) - f(x^*) \leq \frac{1}{2tk} \lVert x^{(0)} - x^*\rVert_2^2.
$$

. . .

Convergence rate: $O(1/k)$.

. . .

## Exact Line Search

It may be tempting to try to solve
$$
t^* = \arg_t\min f(x - t\nabla f(x))
$$
but this is typically too expensive.

## Backtracking Line Search

Set $\beta \in (0, 1)$ and $\alpha \in (0, 1/2]$. At each iteration, start with
$t \gets t_0$, and while
$$
f(x - t \nabla f(x)) > f(x) - \alpha t \lVert \nabla f(x)\rVert_2^2
$$
set $t = t \beta.$

![Backtracking line search.](../images/backtrack-line-search.png){width=65%}

##

![Gradient descent with backtracking line
search.](../images/gd-stepsize-linesearch.png){width=50%}

## Stopping Criteria

:::: {.columns}

::: {.column width="47%"}

### Common (ad-hoc) choices

- Small gradient: $$\lVert \nabla f(x) \rVert_r \leq \epsilon$$\pause
- Small relative change in $x$: $$ \frac{\lVert x^{(k+1)} - x^{(k)}
  \rVert_r}{\lVert x^{(k)} + 10^{-q}\rVert_2} \leq \epsilon$$\pause
- Small relative change in $f$: $$\frac{f(x^{(k+1)}) - f(x^{(k)})}{f(x^{k})}
  \leq \epsilon$$

:::

. . .

::: {.column width="47%"}

### Duality Gap

If $f$ is **strongly** convex and $x$ optimal, then the duality gap is zero at
the optimum.

\medskip\pause

Most principled stopping criterion, but not always available.

:::

::::

## Problems with Gradient Descent

![Gradient descent is sensitive to
conditioning.](../images/gd-conditioning.png){width=70%}

## Newton's Method

Recall second-order expansion of $f$ around $x$:
$$
f(y) \approx \hat{f}(y) = f(x) + \nabla f(x)^T (y - x) + \frac{1}{2}(y- x)^T \nabla^2 f(x) (y - x).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

### Newton Update

Instead of replacing $\nabla^2 f(x)$ with $\frac{1}{t} I$ (gradient descent
update), minimize $\hat{f}(y)$ directly:
$$
x^+ = x - \big(\nabla^2 f(x)\big)^{-1} \nabla f(x).
$$

:::

::: {.column width="47%"}

![The Newton method, with approximation.](../images/newton-approx.png){width=52%}

:::

::::

## Damped Newton

In practice this might not converge, so we use **damped** Newton:
$$
x^+ = x - t\nabla^2 f(x)^{-1} \nabla f(x),
$$
with $t > 0$ chosen by backtracking line search.

\medskip\pause

In practice, this is often what people mean with "Newton's method".

:::: {.columns}

::: {.column width="47%"}

### Backtracking Line Search

At each iteration $t \gets 1$ and while
$$
f(x + t v) > f(x) + \alpha t \nabla f(x)^T v
$$
with $v = -(\nabla^2 f(x)^{-1} \nabla f(x))$ set
$$
t \gets \beta t.
$$

:::

::: {.column width="47%"}

### Phases

- As long as $t < 1$, we are in the **damped** phase.
- When $t = 1$, we enter the **pure** phase and $t$ will remain $1$.
- In pure phase, we have quadratic convergence.

:::

::::

## Gradient Descent vs Newton

:::: {.columns}

::: {.column width="47%"}

#### Pros

- Faster convergence: quadratic (eventually)
- Independent of conditioning

#### Cons

- Higher memory cost
- Higher computational cost $O(p^3)$ vs $O(p)$ for GD
- Needs twice differentiable $f$.

:::

::: {.column width="47%"}

![](../images/newton-gd-comp.png){width=100%}

:::

::::

## Convergence

:::: {.columns}

::: {.column width="47%"}

![](../images/newton-gd-convergence-iteration.png){width=100%}

:::

::: {.column width="47%"}

![](../images/newton-gd-convergence-time.png){width=100%}

:::

::::

In practice, results depend on the problem.

## Quasi-Newton Methods

Replace $\nabla^2 f(x)$ with an approximation $B_k$.

. . .

### BFGS

Complexity: $O(p^2)$, still same memory cost.

. . .

### L-BFGS

Same complexity but lower memory cost.

## Example: Poisson Regression

Consider observations $y_i \in \mathbb{N}_0$, $x_i \in \mathbb{R}^p$ for $i = 1,
\ldots, n$, and Poisson point probabilities
$$
f_i(y_i) = e^{-\mu(x_i)} \frac{\mu(x_i)^{y_i}}{y_i!}.
$$

. . .

If $\log(\mu(x_i)) = x_i^T \beta$ we rewrite
$$
f_i(y_i) = e^{\beta^T x_i y_i - \exp( x_i^T \beta)} \frac{1}{y_i!}.
$$

\medskip\pause

This is a _Poisson regression model_.

## Example

```{r vegetables-summary}
summary(vegetables)
dim(vegetables)
```

## Model

$$
\log(E(\text{sale})) = \beta_0 + \beta_1 \log(\text{normalSale}) + \beta_{\text{store}}.
$$

. . .

```{r pois-model, dependson="data", cache = TRUE}
## Note, variable store is a factor with 352 levels!
pois_model <- glm(
  sale ~ log(normalSale) + store,
  data = vegetables,
  family = poisson()
)
```

. . .

```{r pois-sum, echo=2, dependson="pois-model"}
old_options <- options(digits = 2)
summary(pois_model) |>
  coefficients() |>
  head()
options(digits = old_options$digits)
```

## Exponential Families

Joint density:
$$
f(\symbf{y} \mid \theta) = \prod_{i=1}^n \frac{1}{\varphi_i(\theta)} e^{\theta^T t_i(y_i)} = e^{\theta^T \sum_{i=1}^n t_i(y_i) - \sum_{i=1}^n \kappa_i(\theta)}
$$
where $\varphi_i(\theta) = \int e^{\theta^T t_i(u)} \mu_i(\mathrm{d}u)$ and
$\kappa_i(\theta) = \log(\varphi_i(\theta)).$ The log-likelihood is
$$
\ell(\theta) = \theta^T t(\symbf{y}) - \kappa(\theta)
$$
where

$$
t(\symbf{y}) = \sum_{i=1}^m t_i(y_i) \quad \text{and} \quad
\kappa(\theta) = \sum_{i=1}^m \log \varphi_i(\theta).
$$

The gradient is
$$
\nabla \ell(\theta) = t(\symbf{y}) - \nabla \kappa(\theta).
$$

## The Poisson Regression model

For the Poisson regression model we find that

$$
t(\symbf{y}) = \sum_{i=1}^n x_i y_i = \symbf{X}^T \symbf{y} \quad \text{and} \quad
\kappa(\beta) = \sum_{i=1}^n e^{x_i^T \beta}.
$$

Moreover,
$$
\nabla \kappa(\beta) = \sum_{i=1}^n x_i e^{x_i^T \beta} = \symbf{X}^T \symbf{w}(\beta)
$$
where
$$
\symbf{w}(\beta) = \exp(\symbf{X}^T \beta)
$$
with $\exp$ applied coordinatewisely.

## Model Matrix and `glm.fit()`

```{r implement, dependson="data", cache = TRUE}
X <- model.matrix(
  sale ~ log(normalSale) + store,
  data = vegetables
)
y <- vegetables$sale

dim(X)
```

. . .

```{r glm.fit}
bench::bench_time(
  pois_fit <- glm.fit(X, y, family = poisson())
)
```

## `glm.fit()`

```{r}
pois_fit$iter
pois_fit$converged
```

. . .

### Deviance

Two times (log-likelihood of null (intercept-only) model minus log-likelihood of
the full model)

```{r}
pois_fit0 <- glm.fit(X[, 1], y, family = poisson())
pois_fit0$deviance - pois_fit$deviance
```

## `glm.fit()`

```{r glm.fit-trace}
pois_fit <- glm.fit(X, y, family = poisson(), control = list(trace = TRUE))
```

## Stopping criterion

$$
|\mathrm{deviance}_{n-1} - \mathrm{deviance}_n| < \varepsilon (\mathrm{deviance}_n + 0.1)
$$
with the default tolerance parameter $\varepsilon = 10^{-8}$ and with a maximal
number of iterations set to 25.

## Implementation of Objective Function

```{r implement_H, dependson="implement", cache = TRUE}
t_map <- drop(crossprod(X, y))

objective <- function(beta) {
  drop(sum(exp(X %*% beta)) - beta %*% t_map) / nrow(X)
}

gradient <- function(beta) {
  (drop(crossprod(X, exp(X %*% beta))) - t_map) / nrow(X)
}
```

##

Recomputing the deviance difference,

```{r objective-value-deviance}
2 *
  nrow(X) *
  (objective(c(pois_fit0$coefficients, rep(0, 352))) -
    objective(pois_fit$coefficients))
```

. . .

and the value of the negative log-likelihood:

```{r objective-value}
objective(pois_fit$coefficients)
```

## Using `optim()` with Conjugate Gradient

```{r CG, dependson="implement_H", cache = TRUE}
bench::bench_time(
  pois_CG <- optim(
    rep(0, length = ncol(X)),
    objective,
    gradient,
    method = "CG",
    control = list(maxit = 10000)
  )
)
```

. . .

```{r CG-results, dependson="CG"}
pois_CG$value
pois_CG$counts
```

## Using `optim()` with BFGS

```{r BFGS, dependson="implement_H", cache = TRUE}
bench::bench_time(
  pois_BFGS <- optim(
    rep(0, length = ncol(X)),
    objective,
    gradient,
    method = "BFGS",
    control = list(maxit = 10000)
  )
)
```

. . .

```{r BFGS-results, dependson="BFGS"}
pois_BFGS$value
pois_BFGS$counts
```

## Using Sparse Matrices

```{r Matrix, message=FALSE, cache=FALSE}
library(Matrix)
```

```{r sparse, dependson="implement", cache = TRUE}
X <- Matrix(X)
```

. . .

```{r BFGS-sparse, dependson=c("implement_H", "sparse"), cache = TRUE}
bench::bench_time(
  pois_BFGS_sparse <- optim(
    rep(0, length = ncol(X)),
    objective,
    gradient,
    method = "BFGS",
    control = list(maxit = 10000)
  )
)
```

. . .

```{r BFGS-sparse-results, dependson="BFGS-sparse"}
range(pois_BFGS$par - pois_BFGS_sparse$par)
```

## Implementation

```{r GD, cache = TRUE}
source(here::here("R", "optim_alg.R"))
```

[Source code](R/optim_alg.R)

## Poisson Regression Problem

```{r poisson-def}
poisson_model <- function(form, data, response) {
  X <- model.matrix(form, data)
  y <- data[[response]]
  t_map <- drop(crossprod(X, y))
  n <- nrow(X)
  p <- ncol(X)

  objective <- function(beta) {
    drop(sum(exp(X %*% beta)) - beta %*% t_map) / n
  }

  gradient <- function(beta) {
    (drop(crossprod(X, exp(X %*% beta))) - t_map) / n
  }

  hessian <- function(beta) {
    crossprod(X, drop(exp(X %*% beta)) * X) / n
  }

  list(par = rep(0, p), H = objective, grad_H = gradient, Hessian_H = hessian)
}
```

## Test

Gradient descent is very slow for the full Poisson model, so we consider a much
smaller problem.

```{r GD-test, dependson=c("Implement", "GD"), cache = TRUE}
veg_pois <- poisson_model(
  ~ log(normalSale),
  vegetables,
  response = "sale"
)

pois_gd <- gradient_descent(
  veg_pois$par,
  veg_pois$H,
  veg_pois$grad_H
)
```

. . .

```{r GD-comp, dependson="GD-test", echo=2:3, cache = TRUE}
old_options <- options(digits = 5)
pois_glm <- glm(sale ~ log(normalSale), data = vegetables, family = poisson())
rbind(pois_glm = coefficients(pois_glm), pois_gd)
options(digits = old_options$digits)
```

## Test

```{r GD-object, dependson=c("Implement", "GD-test"), echo=2:3}
old_options <- options(digits = 15)
veg_pois$H(coefficients(pois_glm))
veg_pois$H(pois_gd)
options(digits = old_options$digits)
```

## Tracer

```{r GD-trace, dependson=c("GD-test", "vegetables-data"), results='hide', echo=-3, cache = TRUE, message = FALSE, warning = FALSE}
library(CSwR)
gd_tracer <- tracer(c("value", "h_prime", "gamma"), N = 50)
gc() # Garbage collection
```

. . .

```{r pois-trace, dependson=c("GD-trace", "GD-test", "vegetables-data"), cache = TRUE}
pois_gd <- gradient_descent(
  veg_pois$par,
  veg_pois$H,
  veg_pois$grad_H,
  cb = gd_tracer$tracer
)
```

## Trace Information

```{r trace-sum, dependson=c("GD-trace", "pois-trace"), cache = TRUE}
trace_sum_gd <- summary(gd_tracer)
head(trace_sum_gd, 3)
```

. . .

```{r trace-sum2, dependson="trace-sum"}
tail(trace_sum_gd, 3)
val_min <- veg_pois$H(coefficients(pois_glm))
```

## Trace Information

```{r trace-info, warning=FALSE}
ggplot(trace_sum_gd, aes(.time, value - val_min)) +
  geom_line() +
  scale_y_log10()
```

## Trace Information

```{r trace-sum3}
ggplot(trace_sum_gd, aes(.time, h_prime)) +
  geom_line() +
  scale_y_log10()
```

```{r prof1, eval=FALSE, echo=FALSE}
library(profvis)
profvis(
  replicate(40, {
    gradient_descent(veg_pois$par, veg_pois$H, veg_pois$grad_H, epsilon = 1e-10)
    NULL
  })
)
```

## Newton Method

```{r pois2, cache = TRUE}
veg_pois <- poisson_model(
  ~ store + log(normalSale) - 1,
  vegetables,
  response = "sale"
)
```

```{r Newton-test, dependson=c("Implement", "GD-test", "pois2"), cache = TRUE}
system.time(
  pois_newton <- newton(
    veg_pois$par,
    veg_pois$H,
    veg_pois$grad_H,
    veg_pois$Hessian_H
  )
)
```

## Test

```{r Newton-comp, dependson="Newton-test", cache = TRUE}
pois_glm <- glm(
  sale ~ store + log(normalSale) - 1,
  data = vegetables,
  family = poisson()
)
range(pois_newton - pois_glm$coefficients)
```

--

```{r Newton-object, dependson=c("Implement", "Newton-test"), echo=2:3, results='hold'}
old_options <- options(digits = 20)
veg_pois$H(pois_newton)
veg_pois$H(pois_glm$coefficients)
options(digits = old_options$digits)
```

. . .

`glm()` (and the workhorse `glm.fit()`) uses a Newton-type algorithm.

## Tracing

```{r Newton-trace, cache = TRUE}
newton_tracer <- tracer(
  c("value", "grad_norm_sq"),
  N = 0,
  expr = expression(grad_norm_sq <- sum(grad^2))
)
pois_newton <- newton(
  veg_pois$par,
  veg_pois$H,
  veg_pois$grad_H,
  veg_pois$Hessian_H,
  epsilon = 8e-28,
  cb = newton_tracer$tracer
)

gd_tracer <- tracer(c("value"), N = 0)
pois_gd <- gradient_descent(
  veg_pois$par,
  veg_pois$H,
  veg_pois$grad_H
)
```

## Tracing

```{r Newton-summary-trace, dependson="Newton-trace", cache = TRUE}
trace_sum_newton <- summary(newton_tracer)
val_min <- veg_pois$H(pois_newton)
trace_sum_newton
```

## Newton Convergence

```{r trace-sum-Newton, warning=FALSE, dependson="Newton-summary-trace"}
#| echo: false
#| fig-width: 5.4
#| fig-height: 3
pl1 <- ggplot(trace_sum_newton, aes(.time, value - val_min)) +
  geom_point() +
  geom_line() +
  scale_y_log10()

pl2 <- ggplot(trace_sum_newton, aes(.time, grad_norm_sq)) +
  geom_point() +
  geom_line() +
  scale_y_log10()

pl1 + pl2
```

```{r prof2, eval=FALSE, echo=FALSE}
library(profvis)
profvis(
  newton(
    veg_pois$par,
    veg_pois$H,
    veg_pois$grad_H,
    veg_pois$Hessian_H
  )
)
```

## Summary

### Today

- Optimization problems
- Algorithms to solve optimization problems
  - Gradient descent
  - The Newton method

. . .

### Things We Did Not Cover

- Accelerated gradient methods
- Conjugate gradient
- Constrained optimization
- Nonsmooth objectives

## Exercise

### Step 1

Implement the gradient descent algorithm to solve ridge regression:
$$
\operatorname{minmize}_{\beta \in \mathbb{R}^p}\left( \frac{1}{2} \lVert X\beta - y\rVert_2^2 + \frac{\lambda \lVert \beta \rVert_2^2}{2} \right)
$$

. . .

### Step 2

Implement a method for solving ridge regression directly.

Explicitly factorize (i.e. don't use `solve()`)

