@article{nesterovMethodSolvingConvex1983,
  title         = {
    A Method of Solving a Convex Programming Problem with Convergence Rate
    {{O}}(1/K\textasciicircum 2)
  },
  author        = {Nesterov, Yuri},
  volume        = {269},
  number        = {3},
  pages         = {543--547},
  url           = {https://cir.nii.ac.jp/crid/1570572699326076416},
  urldate       = {2024-01-16},
  date          = {1983},
  journaltitle  = {Doklady Akademii Nauk SSSR},
  file          = {
    /home/jola/Zotero/storage/3R4RVMNF/nesterov_1983_a_method_for_unconstrained_convex_minimization_problem_with.pdf;/home/jola/Zotero/storage/WUZXKJHX/1570572699326076416.html
  }
}

@article{gohWhyMomentumReally2017,
  title         = {Why Momentum Really Works},
  author        = {Goh, Gabriel},
  doi           = {10.23915/distill.00006},
  url           = {http://distill.pub/2017/momentum},
  date          = {2017},
  journaltitle  = {Distill}
}

@article{lessardAnalysisDesignOptimization2016,
  title         = {Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author        = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  publisher     = {{Society for Industrial and Applied Mathematics}},
  volume        = {26},
  number        = {1},
  pages         = {57--95},
  doi           = {10.1137/15M1009597},
  issn          = {1052-6234},
  url           = {https://epubs.siam.org/doi/abs/10.1137/15M1009597},
  urldate       = {2024-10-17},
  date          = {2016-01},
  journaltitle  = {SIAM Journal on Optimization},
  shortjournal  = {SIAM J. Optim.},
  abstract      = {
    We provide a framework for computing the exact worst-case performance of any algorithm
    belonging to a broad class of oracle-based first-order methods for composite convex
    optimization, including those performing explicit, projected, proximal, conditional, and
    inexact (sub)gradient steps. We simultaneously obtain tight worst-case guarantees and explicit
    instances of optimization problems on which the algorithm reaches this worst-case. We achieve
    this by reducing the computation of the worst-case to solving a convex semidefinite program,
    generalizing previous works on performance estimation by Drori and Teboulle  [Math. Program.,
    145 (2014), pp. 451--482] and the authors [A. B. Taylor, J. M. Hendrickx, and F. Glineur, Math.
    Program., 161 (2017), pp. 307--345]. We use these developments to obtain a tighter analysis of
    the proximal point algorithm and of several variants of fast proximal gradient, conditional
    gradient, subgradient, and alternating projection methods. In particular, we present a new
    analytical worst-case guarantee for the proximal point algorithm that is twice better than
    previously known and improve the standard worst-case guarantee for the conditional gradient
    method by more than a factor of two. We also show how the optimized gradient method proposed by
    Kim and Fessler [Math. Program., 159 (2016), pp. 81--107] can be extended by incorporating a
    projection or a proximal operator, which leads to an algorithm that converges in the worst-case
    twice as fast as the standard accelerated proximal gradient method [A. Beck and M. Teboulle,
    SIAM J. Imaging Sci., 2 (2009), pp. 183--202].
  },
  keywords      = {momentum,optimization},
  file          = {
    /home/jola/Zotero/storage/5MYJDUU5/Lessard et al. - 2016 - Analysis and design of optimization
    algorithms via integral quadratic constraints.pdf
  }
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title         = {Adam: A Method for Stochastic Optimization},
  shorttitle    = {Adam},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  booktitle     = {
    3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}},
    {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}
  },
  url           = {http://arxiv.org/abs/1412.6980},
  urldate       = {2024-10-22},
  editor        = {Bengio, Yoshua and LeCun, Yann},
  date          = {2015}
}

@unpublished{hintonLecture62018,
  title         = {Lecture 6},
  author        = {Hinton, Geoffrey},
  url           = {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture\%5Fslides\%5Flec6.pdf},
  type          = {Oral Presentation},
  namea         = {Srivastava, Nitish and Swersky, Kevin},
  nameatype     = {collaborator},
  date          = {2018},
  eventtitle    = {Coursera Course: {{Neural}} Networks for Machine Learning},
  langid        = {english},
  venue         = {Virtual},
  file          = {/home/jola/Zotero/storage/EY5I7UYB/Hinton - 2018 - Lecture 6.pdf}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title         = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author        = {Duchi, John and Hazan, Elad and Singer, Yoram},
  volume        = {12},
  number        = {61},
  pages         = {2121--2159},
  issn          = {1533-7928},
  url           = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate       = {2024-10-22},
  date          = {2011},
  journaltitle  = {Journal of Machine Learning Research},
  abstract      = {
    We present a new family of subgradient methods that dynamically incorporate knowledge of the
    geometry of the data observed in earlier iterations to perform more informative gradient-based
    learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of
    very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic
    optimization and online learning which employ proximal functions to control the gradient steps
    of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal
    function, which significantly simplifies setting a learning rate and results in regret
    guarantees that are provably as good as the best proximal function that can be chosen in
    hindsight. We give several efficient algorithms for empirical risk minimization problems with
    common and important regularization functions and domain constraints. We experimentally study
    our theoretical analysis and show that adaptive subgradient methods outperform
    state-of-the-art, yet non-adaptive, subgradient algorithms.
  },
  file          = {
    /home/jola/Zotero/storage/IHSP25C3/Duchi et al. - 2011 - Adaptive Subgradient Methods for
    Online Learning and Stochastic Optimization.pdf
  }
}

@article{polyakMethodsSpeedingConvergence1964,
  title         = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author        = {Polyak, B. T.},
  volume        = {4},
  number        = {5},
  pages         = {1--17},
  doi           = {10.1016/0041-5553(64)90137-5},
  issn          = {0041-5553},
  url           = {https://www.sciencedirect.com/science/article/pii/0041555364901375},
  urldate       = {2024-10-22},
  date          = {1964-01-01},
  journaltitle  = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal  = {USSR Computational Mathematics and Mathematical Physics},
  abstract      = {
    For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually
    linear, from B into B, and B is a Banach space) iteration methods are generally used. These
    consist of the construction of a series x0, \ldots{}, xn, \ldots{}, which converges to the
    solution (see, for example [1]). Continuous analogues of these methods are also known, in which
    a trajectory x(t), 0 \leqslant{} t \leqslant{} \infty{} is constructed, which satisfies the
    ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t
    \rightarrow{} \infty{} (see [2]). We shall call the method a k-step method if for the
    construction of each successive iteration xn+1 we use k previous iterations xn, \ldots{},
    xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential
    equation of the k-th order or k-th degree. Iteration methods which are more widely used are
    one-step (e.g. methods of successive approximations). They are generally simple from the
    calculation point of view but often converge very slowly. This is confirmed both by the
    evaluation of the speed of convergence and by calculation in practice (for more details see
    below). Therefore the question of the rate of convergence is most important. Some multistep
    methods, which we shall consider further, which are only slightly more complicated than the
    corresponding one-step methods, make it possible to speed up the convergence substantially.
    Note that all the methods mentioned below are applicable also to the problem of minimizing the
    differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution
    of the equation grad (x) = 0.
  },
  file          = {/home/jola/Zotero/storage/7KQ49U4Y/0041555364901375.html}
}
