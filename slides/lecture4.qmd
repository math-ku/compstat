---
title: "Parallelization and Scatterplot Smoothing"
---

{{< include _common.qmd >}}

## Today

### Parallelization

Using multiple cores to speed up computations.

. . .

### Scatterplot Smoothing

Cover **nearest neighbor smoothing** and **smoothing splines**

# Parallelization

## Parallelization

Most processors have multiple cores.

\medskip\pause

But unless instructed otherwise, only a single core is going to be used.

\medskip\pause

The computer doesn't automatically know that your computations are safe to do
in parallel.

## Embarrassingly Parallel Tasks

Tasks that easily separate into independent subtasks.

\pause\medskip

### Examples {.example}

- Summing a vector (or matrix): `sum()`\pause
- Linear algebra operations: `%*%` (`crossprod()`)\pause
- Running iterations of a simulation\pause
- Cross-validation

## The foreach Package

:::: {.columns}

::: {.column width="47%"}

```{r foreach-load}
#| warning: false
#| message: false
library(foreach)
```

```{r foreach-sqrt}
#| eval: false
foreach(i = seq_len(3)) %dopar%
  {
    sqrt(i)
  }
```

:::

. . .

::: {.column width="47%"}

```{r}
#| echo: false
#| ref-label: "foreach-sqrt"
```


:::

::::

\pause\medskip

### Notes {.alert}

- Returns a list (so not really a for loop).\pause
- Nothing is actually parallel yet!\pause
- First, we need to register a **backend**.

## Backends

Multiple backends, installed separately (**doParallel**, **doMC**, **doFuture**)
Load and **register** one before using foreach.

. . .

:::: {.columns}

::: {.column width="47%"}

```{r doparallel}
#| eval: false
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)

foreach(i = seq_len(3)) %dopar%
  {
    paste(
      "Hello from process",
      Sys.getpid()
    )
  }
```

:::

. . .

::: {.column width="47%"}

```{r}
#| echo: false
#| message: false
#| ref-label: "doparallel"
```

:::

::::
## Combining Results

`foreach()` always returns a list.

\medskip\pause

If you want to reduce your result, use either the `.combine` argument or
manually reduce the resulting list.

. . .

```{r}
x <- c(4, 1, 1e2, 2)

foreach(i = seq_len(4), .combine = c) %dopar%
  {
    log(x[[i]])
  }
```


## Futures

A future represents a value that may be available later on.

\medskip\pause

Sometimes called \alert{promises}: "I promise to give you the result later."

. . .

:::: {.columns}

::: {.column width="47%"}

### Without Futures {.alert}

```{r}
v <- {
  cat("Hello world!\n")
  3.14
}
v
```

:::

. . .

::: {.column width="47%"}

### With Futures {.example}

```{r}
library(future)
v %<-%
  {
    cat("Hello world!\n")
    3.14
  }
v
```

:::

::::

## Futures Enable Parallelization

A new \alert{thread} is created for each future.

\medskip\pause

- Meanwhile, the main thread can continue doing other work.
- Once the value is needed, the main thread blocks until it's available.

. . .

```{r}
#| eval: false
plan(multisession) # or multicore, mirai_multisession, etc.

z %<-%
  {
    # Peform some expensive computation
  }

# Perform other work here ...

z # This line blocks until z is available
```

## Backends for Futures

Need to pick a backend (plan), with multiple options.

. . .

### `multicore`

Available on Linux and macOS, but not Windows or RStudio. _Forks_ the current
R process, instead of starting a new one. But _not_ recommended because of
potential stability issues.

. . .

### `mirai_multisession`/`mirai_cluster`

A newer alternative, available through the
[future.mirai](https://cran.r-project.org/package=future.mirai)
package.

## Caveats

### Realistic Expectations

In practice, twice as many cores $\neq$ twice as fast (in practice).


. . .

### Overhead

Parallelization comes with overhead (starting threads, communication).


. . .

### Thread Safety

Some functions are not thread safe (e.g. random number generation).

. . .

### Redundancies

Parallelizing a task that is already parallelized may not help and may even
\alert{hurt} performance.

## Comparison of foreach and future

:::: {.columns}

::: {.column width="47%"}

### foreach

Main use case is to run computations in parallel that would otherwise be done in
a `for` loop.


:::

. . .

::: {.column width="47%"}

### future

More general abstraction for asynchronous programming and parallelization.

\pause\medskip

Primarily used when you have various independent tasks that you want to run
in parallel.

:::

::::

# Scatterplot Smoothing

## Nuuk Temperature Data

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
nuuk <- read_table(
  here::here("data", "nuuk.txt"),
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) |>
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) |>
  mutate(Temperature = Temperature / 10) |>
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) |>
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

```{r}
#| fig-cap: Annual average temperature in Nuuk, Greenland
#| fig-width: 4
nuuk_plot <- ggplot(nuuk_year, aes(Year, Temperature)) +
  geom_point()
nuuk_plot
```

Data is available [here](../data/nuuk.dat.txt).

## Nearest Neighbor Estimation

Given data points $(x_1, y_1), \ldots, (x_n, y_n)$, the $k$-nearest neighbor
estimator at $x_i$ is
$$
\hat{f}_i = \frac{1}{k} \sum_{j \in N_i} y_j
$$
where $N_i$ is the set of indices for the $k$ nearest neighbors of $x_i$.

\medskip\pause

This is an estimator of
$$
  f(x) = E(Y \mid X = x).
$$

\pdfpcnote{
 N(x) is the neighborhood of x.
}

\pdfpcnote{
  Show example with k = 3.
}

## Matrix Formulation

We can express all fitted values as a matrix-vector product:
$$
\hat{\symbf{f}} = \symbf{S} \symbf{y}
$$
where $\symbf{y} = (y_1, \ldots, y_n)^T$ and $\symbf{S}$ is the **smoother
matrix**.

\pause

### Smoother Matrix

Each entry of $\symbf{S}$ is
$$
S_{ij} = \frac{1}{k} \cdot \symbf{1}_{\{j \in N_i\}}
$$
where $\symbf{1}_{\{j \in N_i\}}$ is 1 if $j$ is a neighbor of $i$, 0 otherwise.

\pdfpcnote{
  Each fitted value is a weighted average of the y-values.

  S is a relatively sparse n x n matrix.
}

## Linear Smoothers

Smoothers of the form $\hat{\symbf{f}} = \symbf{S} \symbf{y}$ are called a
_linear_ smoothers.

\pause\medskip

The $k$ nearest neighbor smoother is a simple example of a linear smoother that
works for $x$-values in any metric space.

\pause\medskip

The representation of a linear smoother as a matrix-vector product,
$$
  \symbf{S} \symbf{y}
$$
is theoretically useful, but often not the best way to actually compute
$\hat{\symbf{f}}$.

## Running Mean

When $x_i \in \mathbb{R}$ we can sort data according to $x$-values and then use
a _symmetric_ neighbor definition:
$$
  N_i = \left\{i - \frac{k - 1}{2}, i - \frac{k - 1}{2} + 1, \ldots, i - 1 , i, i + 1, \ldots, i + \frac{k - 1}{2}\right\}
$$
(for $k$ odd.)

\pause\medskip

A version of nearest neighbor smoothing that assumes ordered and symmetric
neighborhoods.

\pause

### Simplifies Computations

We don't need to keep track of metric comparisons. Only the order matters.

\pdfpcnote{
- Symmetric neighborhoods do not take distance into account.

- Only need to sort

- Only order matters
}

## Running Mean (Naive Implementation)

Assume $y$ is sorted and $k$ is odd.

```{r runmean-simple}
run_mean_naive <- function(y, k) {
  n <- length(y)
  m <- (k - 1) / 2
  y <- y / k
  f_hat <- rep(NA, n)

  for (i in (m + 1):(n - m - 1)) {
    f_hat[i] <- mean(y[(i - m):(i + m)])
  }

  f_hat
}
```

\pdfpcnote{
- Can speed this up by replacing mean computation
}

## Efficient Running Mean

We can achieve a more efficient implementation by noting that
$$
  \hat{f}_{i+1} = \hat{f}_{i} - \frac{y_{i - (k-1)/2}}{k} + \frac{y_{i + (k + 1)/2}}{k}.
$$

. . .

```{r run_mean_def}
run_mean <- function(y, k) {
  n <- length(y)
  m <- floor((k - 1) / 2)
  k <- 2 * m + 1 # Ensures k is odd and m = (k - 1) / 2
  y <- y / k
  f_hat <- rep(NA, n)
  f_hat[m + 1] <- sum(y[1:k])

  for (i in (m + 1):(n - m - 1)) {
    f_hat[i + 1] <- f_hat[i] - y[i - m] + y[i + 1 + m]
  }
  f_hat
}
```

\pdfpcnote{
  Ensures k is odd

  Complexity goes from O(nk) to O(n).

  Based on the idea of keeping and updating a running sum.
}


## Visualization

```{r nuuk-NN-plot2}
#| warning: false
#| fig-width: 5
#| fig-height: 2.2
#| fig-cap: Running mean smoother with $k = 11$.
f_hat <- run_mean(nuuk_year$Temperature, 11)
nuuk_plot + geom_line(aes(y = f_hat), color = "blue")
```

\pdfpcnote{
  Takes 5 to the right and 5 to the left
}

## `stats::filter()`

`stats::filter()` applies linear filtering (moving averages or autoregression).

. . .

```{r run-mean-nas, echo=2}
op <- options(digits = 2)
f_hat_filter <- stats::filter(
  nuuk_year$Temperature,
  rep(1 / 11, 11)
)
f_hat_filter[c(1:10, 137:147)]
options(digits = op$digits)
```

. . .

```{r runmean-test}
f_hat <- run_mean(nuuk_year$Temperature, 11)

all.equal(f_hat, as.numeric(f_hat_filter))
```

\pdfpcnote{
  Warn about name clash with dplyr::filter.

  `rep(1, /11, 11)` are MA coefficients
}

##

```{r runMeanBench}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 5.5
#| fig-height: 3
nn_bench <- bench::press(
  n = c(512, 1024, 2048, 4196),
  {
    k <- 11
    y <- rnorm(n)
    w <- c(rep(1, k * n), rep(0, n * (n - k)))
    S <- matrix(w, n, n, byrow = TRUE)
    bench::mark(
      S %*% y,
      run_mean(y, k = k),
      stats::filter(y, rep(1 / k, k)),
      check = FALSE
    )
  }
)

plot(nn_bench)
```

The Matrix-vector multiplication is $O(n^2)$.

\medskip\pause

The two other algorithms are $O(n)$.


## Bias--Variance Trade-Off

There is a bias-variance trade-off in nearest neighbor smoothing with respect
to $k$.

\medskip\pause

If data is i.i.d. with $V(Y \mid X) = \sigma^2$ and $f(x) = E(Y \mid X = x)$,

\pause

$$
\begin{aligned}
  \text{MSE}(x) & = \operatorname{E}(f(x) - \hat{f}(x))^2 \\ \pause
                & = \operatorname{Var}\big(f(x)-\hat{f}(x)\big) + \big(\operatorname{E}(f(x)-\hat{f}(x))\big)^2\\ \pause
                & = \underbrace{\frac{\sigma^2}{k}}_{\text{variance}} + \bigg( \underbrace{f(x) - \frac{1}{k} \sum_{l \in N(x)} f(x_l)}_{\text{bias}}\bigg)^2.
\end{aligned}
$$

. . .

### Bias-Variance Trade-Off

- Small $k$ gives large variance and small bias (if $f$ is smooth).\pause
- Large $k$ gives small variance and potentially large bias (if $f$ is not
  constant).

## How to Choose $k$?

We know there is a bias-variance trade-off in $k$---so how do we choose $k$?

\pause

### Leave-One-Out Cross Validation (LOOCV)

Leave out one observation at a time, predict using remaining data, and measure
the prediction error.


\pause\medskip

:::: {.columns}

::: {.column width="47%"}

The general LOOCV formula is
$$
  \text{LOOCV} = \sum_{i=1}^n (y_i - \hat{f}^{-i}_i)^2.
$$

\pause

For a *linear* smoother, we have
$$
  \hat{f}^{-i}_i = \sum_{j \neq i} \frac{S_{ij}y_j}{1 - S_{ii}},
$$


:::

. . .

::: {.column width="47%"}

This leads to the simplified formula
$$
  \text{LOOCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - S_{ii}}\right)^2.
$$

:::

::::


\pdfpcnote{
Can for running mean in principle compute this without leaving anything out.
}

## LOOCV for Nearest Neighbors

For nearest neighbors, $S_{ii} = \frac{1}{k}$ for all $i$, so
$$
  \text{LOOCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - 1/k}\right)^2.
$$

. . .

### Implementation

```{r LOOCV-runMean}
loocv <- function(k, y) {
  f_hat <- run_mean(y, k)
  mean(((y - f_hat) / (1 - 1 / k))^2, na.rm = TRUE)
}
```

. . .

### Note {.alert}

The implementation removes missing values due to the way we handle the
boundaries, and it uses `mean()` instead of `sum()` to correctly adjust for
this.

\pdfpcnote{
- For nearest neighbors, $S_{ii} = \frac{1}{k}$ for all $i$.

- **Derive the formula on the blackboard!**
}

##

```{r nuuk-running-loocv}
k <- seq(3, 40, 2)
cv_error <- sapply(k, function(kk) loocv(kk, nuuk_year$Temperature))
k_opt <- k[which.min(cv_error)]
```

\medskip\pause

```{r nuuk-running-loocv-plot}
#| fig-width: 5
#| fig-height: 2
#| echo: false
#| fig-cap: LOOCV error for nearest neighbor smoothing as a function of $k$.
ggplot(tibble(k, cv_error), aes(k, cv_error)) +
  geom_vline(xintercept = k_opt, color = "dark orange") +
  geom_line() +
  geom_point() +
  labs(y = "LOOCV Error", x = expression(k))
```

##

```{r nuuk-NN-plot3}
#| warning: false
#| echo: false
#| fig-width: 5
#| fig-height: 2.5
#| fig-cap: Nearest neighbor smoother with LOOCV-optimal $k=15$.
nuuk_plot +
  geom_line(aes(y = run_mean(nuuk_year$Temperature, k_opt)), color = "blue")
```

## Wiggliness

The running mean is wiggly!

\pause\medskip

We can use kernel smoothing (as we did last week) to fix this.

\pause\medskip

But another idea is to use \alert{smoothing splines} instead.

# Smoothing Splines

## Smoothing Splines

We want a smooth function $f$ that balances goodness-of-fit and wiggliness.

\medskip\pause

To do this, we minimize an objective function that penalizes wiggliness:
$$
  L(f) = \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \underbrace{\int f''(z)^2 \text{d} z}_{\lVert f''\rVert_2^2}
$$
where $\lambda \geq 0$ is a tuning parameter that controls the trade-off.

\pause\medskip

The minimizer of this objective is a **cubic spline** with **knots** in the data
points $x_i$.

\medskip\pause

That is, a function
$$
  f = \sum_j \beta_j \varphi_j
$$
where $\varphi_j$ is a basis function for the $n$-dimensional space of such
splines.

\medskip\pause

The problem reduces to finding the coefficients $\symbf{\beta}$.

\pdfpcnote{
Without penalty term, any interpolating function would work well.

Penalty term penalizes solutions with large second derivatives (in norm).
}

## Loss Function

In vector notation
$$
  \symbf{f} = \symbf{\Phi}\symbf{\beta}
$$
with $\symbf{\Phi}_{ij} = \varphi_j(x_i)$, and \pause
$$
\begin{aligned}
  L(\symbf{f}) & = (\symbf{y} - \symbf{f})^T (\symbf{y} - \symbf{f}) + \lambda \lVert f''\rVert_2^2 \\ 
               & = (\symbf{y} - \symbf{\Phi}\symbf{\beta})^T (\symbf{y} - \symbf{\Phi}\symbf{\beta}) + \lambda \symbf{\beta}^T \symbf{\Omega} \symbf{\beta}
\end{aligned}
$$
with
$$
  \symbf{\Omega}_{jk} = \int \varphi_j''(z) \varphi_k''(z) \text{d}z.
$$


\pause

### Solution

The solution is a _linear smoother_ with smoother matrix $\symbf{S}_{\lambda}$
and penalty matrix $\symbf{\Omega}$.

\pdfpcnote{
  Similar to ridge (Tikhonov) regression: OLS + squared l2 penalty.

  Can rewrite penalty term in quadratic form $\lambda \symbf{\beta}^T \symbf{\Omega} \symbf{\beta}$.

  Derive the minimizer on the blackboard.
}

## Splines in R

```{r splines}
#| message: false
#| warning: false
knots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)
xx <- seq(0, 1, 0.005)
b_splines <- splines::splineDesign(knots, xx)
```

. . .

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 2
#| fig-cap: Cubic B-spline basis functions with knots at 0, 0.2, 0.4, 0.6, 0.8,
#|   and 1.
df <- data.frame(
  x = rep(xx, times = ncol(b_splines)),
  y = as.vector(b_splines),
  group = as.character(rep(seq_len(ncol(b_splines)), each = length(xx)))
)

knot_points <- c(0, 0.2, 0.4, 0.6, 0.8, 1)

ggplot(df, aes(x = x, y = y, group = group, color = group)) +
  scale_x_continuous(breaks = knot_points, minor_breaks = NULL) +
  geom_line() +
  labs(x = "x", y = "Basis Value") +
  guides(color = "none")
```

\pdfpcnote{
Repeated knots at boundaries necessary to ensure function goes through
control points and is smooth.
}

## Penalty Matrix $\symbf{\Omega}$

```{r Omega-Simpson}
#| echo: false
#| fig-cap: The penalty matrix $\symbf{\Omega}$ for a cubic spline with knots at
#|   0, 0.2, ..., 0.8, 1.
#| fig-width: 5
#| fig-height: 3
library(Matrix)
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splines::splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splines::splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) /
    6
}

omega <- pen_mat(
  seq(0, 1, 0.1)
)

omega_mat <- as.matrix(omega)

n_row <- nrow(omega_mat)
n_col <- ncol(omega_mat)

omega_df <- expand.grid(
  Row = seq_len(n_row),
  Col = seq_len(n_col)
)
omega_df$Value <- as.vector(omega_mat)

ggplot(omega_df, aes(x = Col, y = Row, fill = Value)) +
  geom_tile() +
  scale_y_reverse() +
  scale_fill_gradient2() +
  coord_fixed()
```

## Fitting a Smoothing Spline

We implement the matrix-algebra directly for computing $\symbf{S}_{\lambda}
\symbf{y}$.

```{r nuuk-smooth-spline}
inner_knots <- nuuk_year$Year

# Note that order does not matter
knots <- c(rep(range(inner_knots), 3), inner_knots)

Phi <- splines::splineDesign(knots, inner_knots)
```

. . .

```{r nuuk-smooth-spline2}
omega <- pen_mat(inner_knots) # Complicated, but depends only on knots
smoother <- function(lambda) {
  Phi %*%
    solve(
      crossprod(Phi) + lambda * omega, # Phi^T Phi + lambda Omega
      crossprod(Phi, nuuk_year$Temperature) # Phi^T y
    )
}
```

##

```{r nuuk-smoother-plot}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot +
  geom_line(aes(y = smoother(10)), color = "blue") +
  geom_line(aes(y = smoother(1000)), color = "red") +
  geom_line(aes(y = smoother(100000)), color = "orange")
```

## Generalized Cross-Validation (GCV)

For smoothing splines, $S_{ii}$ varies with $i$, which makes LOOCV expensive
to compute.

\medskip\pause

### Idea

Replace $S_{ii}$ in LOOCV by $\text{df} / n$, with $\text{df} =
\text{trace}(\symbf{S}) = \sum_{i=1}^n S_{ii}$, to get the **generalized**
cross-validation criterion
$$
  \text{GCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - \text{df} / n}\right)^2.
$$

. . .

### Implementation

```{r gcv-smooth-spline}
gcv <- function(lambda, y) {
  S <- Phi %*% solve(crossprod(Phi) + lambda * omega, t(Phi))
  df <- sum(diag(S))
  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE)
}
```

\pdfpcnote{
  The df approximation relies on S_ii being nearly constant.

  GCV does not make sense for running mean since S_ii is constant.
}

## GCV-Optimal $\lambda$

Apply `gcv()` across grid of $\lambda$-values and choose $\lambda$ that
minimizes GCV.

```{r nuuk-spline-gcv}
lambda <- seq(50, 250, 2)
gcv <- sapply(lambda, gcv, y = nuuk_year$Temperature)
lambda_opt <- lambda[which.min(gcv)]
```

```{r gc-plot}
#| echo: false
#| fig-width: 5
#| fig-height: 2
tibble(lambda, gcv) |>
  ggplot(aes(lambda, gcv)) +
  geom_vline(xintercept = lambda_opt, color = "dark orange") +
  geom_line() +
  labs(y = "GCV Error", x = expression(lambda))
```

## GCV-Optimal Smoothing Spline

```{r nuuk-spline-opt}
#| fig-width: 5
#| fig-height: 2.5
smooth_opt <- smoother(lambda_opt)
nuuk_plot + geom_line(aes(y = smooth_opt), color = "blue")
```

## Using `smooth.spline()`

```{r nuuk-spline-opt2}
#| fig-width: 5
#| fig-height: 2.5
smooth <- smooth.spline(nuuk_year$Year, nuuk_year$Temperature)
nuuk_plot + geom_line(aes(y = smooth$y), color = "blue")
```

\pdfpcnote{
- Disable fast heuristic to have similar results as us.
}

## Efficient Computations

When $n$ is large, computing $\symbf{S}_{\lambda} \symbf{y}$ directly is
expensive.

\medskip\pause

In practice, we therefore often use fewer basis functions than data points, $p
< n$.

\pause

### Singular Value Decomposition

Using the singular value decomposition
$$
  \Phi = \symbf{U} D \symbf{V}^T
$$
\pause we can rewrite the smoother matrix as
$$
  \symbf{S}_{\lambda} = \widetilde{\symbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\symbf{U}}^T
$$
where $\widetilde{\symbf{U}} = \symbf{U} \symbf{W}$ and \pause
$$
  D^{-1} \symbf{V}^T \symbf{\Omega} \symbf{V} D^{-1} = \symbf{W} \Gamma \symbf{W}^T.
$$

\medskip\pause

The columns of $\widetilde{\symbf{U}}$ form the **Demmler-Reinsch basis**.

\pdfpcnote{
  Can be done whether we use all or a subset of our basis functions

  S_\lambda is diagonalized

  Derive $\symbf{S}_\lambda$ on blackboard.
}

## Interpretation

Project the data $y$ onto the Demmler-Reinsch basis (columns of
$\widetilde{\symbf{U}}$) to get coefficients $\hat{\symbf{\beta}} =
\widetilde{\symbf{U}}^Ty$.

\pause\medskip

Each coefficient is shrunk according to the corresponding eigenvalue $\gamma_i$
and smoothing parameter $\lambda$:
$$
  \hat{\symbf{\beta}}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.
$$

\pause\medskip

The final smoothed values are reconstructed by combining the shrunk coefficients
with the basis functions.
$$
  \hat{\symbf{f}} = \widetilde{\symbf{U}} \hat{\symbf{\beta}}(\lambda)
$$

## The Demmler-Reinsch Basis (Columns of $\widetilde{\symbf{U}}$)

```{r spline-diagonalization}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
inner_knots <- seq(1867, 2013, length.out = 18)
Phi <- splines::splineDesign(
  c(rep(range(inner_knots), 3), inner_knots),
  nuuk_year$Year
)
omega <- pen_mat(inner_knots)
Phi_svd <- svd(Phi)
omega_tilde <- t(
  t(
    crossprod(Phi_svd$v, omega %*% Phi_svd$v)
  ) /
    Phi_svd$d
) /
  Phi_svd$d

Omega_tilde_svd <- svd(omega_tilde)
U_tilde <- Phi_svd$u %*% Omega_tilde_svd$u

colnames(U_tilde) <- paste0(rep("u", 20), 1:20)
bind_cols(select(nuuk_year, Year), as_tibble(U_tilde)) |>
  gather(key = "term", value = "value", -Year, factor_key = TRUE) |>
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

##

```{r spline-gamma}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
#| fig-cap: The eigenvalues $\gamma_i$ of the penalty matrix in the
#|   Demmler-Reinsch basis.
library(patchwork)

d <- Omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2 + plot_layout(axes = "collect")
```

# Exercises

## Exercises

### Exercise 1: Parallel Square

Use the `foreach` and `doParallel` packages to compute the squares of a vector
in parallel. Benchmark the parallel version against a regular `for` loop using
`bench::mark()`.

```{r}
#| echo: false
#| eval: false
library(foreach)
library(doParallel)
library(bench)

x <- rnorm(10000)

loop_square <- function(x) {
  n <- length(x)
  out <- numeric(n)

  for (i in seq_len(n)) {
    out[i] <- x[i]^2
  }

  out
}

parallel_square <- function(x) {
  out <- foreach(i = seq_along(x), .combine = c) %dopar%
    {
      x[i]^2
    }
  out
}

cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)

bench::mark(
  loop = loop_square(x),
  parallel = parallel_square(x),
  check = FALSE
)
```

. . .

### Exercise 2: Future

Create a future that waits for 5 seconds using `Sys.sleep(5)`. When it's done,
it should prints "The future is here!" and return the value 42. Try accessing
the value of the future before and after the computation is complete.

```{r}
#| echo: false
#| eval: false
library(future)
plan(multicore) # Not on Windows or RStudio!
v %<-%
  {
    Sys.sleep(5)
    cat("The future is here!\n")
    42
  }

v # This line blocks until v is available
```

. . .

### Exercise 3: Smoothing Air Quality Data

Apply `stats::filter()` and `smooth.spline()` to the `airquality$Temp` data. Try
different window sizes for the moving average and different `spar` values for
the spline. Plot and compare the results.
