---
title: "Parallelization and Scatterplot Smoothing"

execute:
  cache: false
---

{{< include _common.qmd >}}

## Today

### Parallelization

Using multiple cores to speed up computations.

. . .

### Nearest Neighbors

Simple algorithm for nonparametric smoothing.

. . .

### Smoothing Splines

A generalization of polynomial regression.

# Parallelization

## Parallelization

Most processors have multiple cores.

\medskip\pause

But unless instructed otherwise, only a single core is going to be used.

\medskip\pause

The computer doesn't automatically know that your computations are safe to do
in parallel.

## Embarrassingly Parallel Tasks

Tasks that easily separate into independent subtasks.

\pause\medskip

### Examples {.example}

- Summing a vector (or matrix): `sum()`\pause
- Linear algebra operations: `%*%` (`crossprod()`)\pause
- Running iterations of a simulation\pause
- Cross-validation

## The foreach Package

:::: {.columns}

::: {.column width="47%"}

```{r foreach-load}
#| warning: false
#| message: false
library(foreach)
```

```{r foreach-sqrt}
#| eval: false
foreach(i = seq_len(3)) %dopar%
  {
    sqrt(i)
  }
```

:::

. . .

::: {.column width="47%"}

```{r}
#| echo: false
#| ref-label: "foreach-sqrt"
```


:::

::::

\pause\medskip

### Notes {.alert}

- Returns a list (so not really a for loop).\pause
- Nothing is actually parallel yet!\pause
- First, we need to register a **backend**.

## Backends

Multiple backends, installed separately (**doParallel**, **doMC**, **doFuture**)
Load and **register** one before using foreach.

. . .

:::: {.columns}

::: {.column width="47%"}

```{r doparallel}
#| eval: false
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)

foreach(i = seq_len(3)) %dopar%
  {
    paste(
      "Hello from process",
      Sys.getpid()
    )
  }
```

:::

. . .

::: {.column width="47%"}

```{r}
#| echo: false
#| message: false
#| ref-label: "doparallel"
```

:::

::::
## Combining Results

`foreach()` always returns a list.

\medskip\pause

If you want to reduce your result, use either the `.combine` argument or
manually reduce the resulting list.

. . .

```{r}
x <- c(4, 1, 1e2, 2)

foreach(i = seq_len(4), .combine = c) %dopar%
  {
    log(x[[i]])
  }
```


## Futures

A future represents a value that may be available later on.

\medskip\pause

Sometimes called \alert{promises}: "I promise to give you the result later."

. . .

:::: {.columns}

::: {.column width="47%"}

### Without Futures {.alert}

```{r}
v <- {
  cat("Hello world!\n")
  3.14
}
v
```

:::

. . .

::: {.column width="47%"}

### With Futures {.example}

```{r}
library(future)
v %<-%
  {
    cat("Hello world!\n")
    3.14
  }
v
```

:::

::::

## Futures Enable Parallelization

A new \alert{thread} is created for each future.

\medskip\pause

- Other work can be done in the meantime on main thread.
- Once the value is needed, hopefully it is already (or best: just-in-time)
  available.

```{r}
#| eval: false
plan(multicore) # Not on Windows or RStudio!

z %<-%
  {
    # Peform some expensive computation
  }

# Perform other work here ...

z # This line blocks until z is available
```

## Caveats

### Realistic Expectations

In practice, twice as many cores $\neq$ twice as fast (in practice).


. . .

### Overhead

Parallelization comes with overhead (starting threads, communication).


. . .

### Thread Safety

Some functions are not thread safe (e.g. random number generation).

. . .

### Redundancies

Parallelizing a task that is already parallelized may not help and may even
\alert{hurt} performance.


## Nuuk Temperature Data

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
nuuk <- read_table(
  here::here("data", "nuuk.txt"),
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) |>
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) |>
  mutate(Temperature = Temperature / 10) |>
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) |>
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

```{r}
#| fig-cap: Annual average temperature in Nuuk, Greenland
#| fig-width: 4
nuuk_plot <- ggplot(nuuk_year, aes(Year, Temperature)) +
  geom_point()
nuuk_plot
```

Data is available [here](../data/nuuk.dat.txt).

## Nearest Neighbor Estimation

Our data is of the form
$$
  (x_1, y_1), \ldots, (x_n, y_n).
$$

\medskip\pause

The $k$-nearest neighbor smoother in $x$ is defined as
$$
  \hat{f}(x) = \frac{1}{k} \sum_{j \in N(x)} y_j
$$
where $N(x)$ is the set of indices for the $k$ nearest neighbors of $x$.

\medskip\pause

This is an estimator of
$$
  f(x) = E(Y \mid X = x).
$$

\pdfpcnote{
 N(x) is the neighborhood of x.
}

## Nearest Neighbor Estimation

The estimator in $x_i$ is
$$
  \hat{f}_i = \frac{1}{k} \sum_{j \in N_i} y_j
$$
where $N_i = N(x_i)$.

\medskip\pause

$S_{ij} = \frac{1}{k} \symbf{1}_{N_i}(j)$, $\symbf{S} = (S_{ij})$, and
$$
  \hat{\symbf{f}} = (\hat{f}_i) = \symbf{S} \symbf{y}.
$$

. . .

$\hat{\symbf{f}}$ is an estimator of the vector $(f(x_i))$.

\pdfpcnote{
  The vector hat(f) can be computed as a linear function of y.
  S is a relatively sparse n x n matrix.
}

## Linear Smoothers

$(f(x_i))$ of the form $\symbf{S} \symbf{y}$ for a _smoother matrix_ $\symbf{S}$
is called a _linear smoother_.

\pause\medskip

The $k$ nearest neighbor smoother is a simple example of a linear smoother that
works for $x$-values in any metric space.

\pause\medskip

The representation of a linear smoother as a matrix-vector product,
$$
  \symbf{S} \symbf{y}
$$
is theoretically useful, but often not the best way to actually compute
$\hat{\symbf{f}}$.

## Running Mean

When $x_i \in \mathbb{R}$ we can sort data according to $x$-values and then use
a _symmetric_ neighbor definition:

$$
  N_i = \{i - (k - 1) / 2, i - (k - 1) / 2 + 1, \ldots, i - 1 , i, i + 1, \ldots, i + (k - 1) / 2\}
$$
(for $k$ odd.)

\pause\medskip

This simplifies computations: we don't need to keep track of metric comparisons,
only the order matters.

\pdfpcnote{
- Symmetric neighborhoods do not take distance into account.

- Only need to sort

- Only order matters
}

## Running Mean (Naive Implementation)

Assume $y$ is sorted and $k$ is odd.

```{r runmean-simple}
run_mean_naive <- function(y, k) {
  n <- length(y)
  m <- (k - 1) / 2
  y <- y / k
  s <- rep(NA, n)

  for (i in (m + 1):(n - m - 1)) {
    s[i] <- mean(y[(i - m):(i + m)])
  }

  s
}
```

\pdfpcnote{
- Can speed this up by replacing mean computation
}

## Running Mean

Implementation (assuming $y$ is sorted) using the identity
$$
  \hat{f}_{i+1} = \hat{f}_{i} - \frac{y_{i - (k-1)/2}}{k} + \frac{y_{i + (k + 1)/2}}{k}.
$$

. . .

```{r run_mean_def}
run_mean <- function(y, k) {
  n <- length(y)
  m <- floor((k - 1) / 2)
  k <- 2 * m + 1 # Ensures k is odd and m = (k - 1) / 2
  y <- y / k
  s <- rep(NA, n)
  s[m + 1] <- sum(y[1:k])

  for (i in (m + 1):(n - m - 1)) {
    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]
  }
  s
}
```

\pdfpcnote{
  Ensures $k$ is odd
}


## Visualization

```{r nuuk-NN-plot2}
#| warning: false
#| fig-width: 5
#| fig-height: 2.2
#| fig-cap: Running mean smoother with $k = 11$.
f_hat <- run_mean(nuuk_year$Temperature, 11)
nuuk_plot + geom_line(aes(y = f_hat), color = "blue")
```

\pdfpcnote{
  Takes 5 to the right and 5 to the left
}

## `stats::filter()`

The R function `stats::filter()` applies linear filtering (moving averages or
autoregression).

. . .

```{r run-mean-nas, echo=2}
op <- options(digits = 2)
f_hat_filter <- stats::filter(
  nuuk_year$Temperature,
  rep(1 / 11, 11)
)
f_hat_filter[c(1:10, 137:147)]
options(digits = op$digits)
```

. . .

```{r runmean-test}
f_hat <- run_mean(nuuk_year$Temperature, 11)
f_hat_filter <- stats::filter(
  nuuk_year$Temperature,
  rep(1 / 11, 11)
)

all.equal(f_hat, as.numeric(f_hat_filter))
```

\pause

There is a `filter()` function in the **dplyr** package (part of the tidyverse),
so look out for name clashes. Safest to call `stats::filter()`.

\pdfpcnote{
  - filter(): general linear filtering function

  - `rep(1, /11, 11)` are MA coefficients
}

##

```{r runMeanBench}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 5.5
#| fig-height: 3
nn_bench <- bench::press(
  n = c(512, 1024, 2048, 4196),
  {
    k <- 11
    y <- rnorm(n)
    w <- c(rep(1, k * n), rep(0, n * (n - k)))
    S <- matrix(w, n, n, byrow = TRUE)
    bench::mark(
      S %*% y,
      run_mean(y, k = k),
      stats::filter(y, rep(1 / k, k)),
      check = FALSE
    )
  }
)

plot(nn_bench)
```

The Matrix-vector multiplication is $O(n^2)$.

\medskip\pause

The two other algorithms are $O(n)$.


## Mean-Squared Error

If data is i.i.d. with $V(Y \mid X) = \sigma^2$ and $f(x) = E(Y \mid X = x)$,

$$
\begin{aligned}
  \mathrm{MSE}(x) & = \operatorname{E}(f(x) - \hat{f}(x))^2 \\
                  & = \operatorname{Var}\big(f(x)-\hat{f}(x)\big) + \big(\operatorname{E}(f(x)-\hat{f}(x))\big)^2\\
                  & = \underbrace{\frac{\sigma^2}{k}}_{\text{variance}} + \underbrace{\left(f(x) - \frac{1}{k} \sum_{l \in N(x)} f(x*l)\right)^2}*{\text{squared bias}}.
\end{aligned}
$$

. . .

### Bias-Variance Trade-Off

- Small $k$ gives large variance and small bias (if $f$ is smooth).
- Large $k$ gives small variance and potentially large bias (if $f$ is not
  constant).

## Leave-One Out Cross-Validation (LOOCV)

The running mean/nearest neighbor smoother is a _linear smoother_,
$\hat{\symbf{f}} = \symbf{S} \symbf{Y}$.

\medskip\pause

How to predict $y_i$ if $(x_i, y_i)$ is left out?

\medskip\pause

A _definition_ for a linear smoother is
$$
  \hat{f}^{-i}_i = \sum_{j \neq i} \frac{S_{ij}y_j}{1 - S_{ii}}.
$$

\medskip\pause

For many smoothing procedures with a natural "out-of-sample" prediction method,
the identity above holds.

\pdfpcnote{
Can for running mean in principle compute this without leaving anything out.
}

## LOOCV

It follows that for **leave-one-out cross validation,**

$$
  \text{LOOCV} = \sum_{i} (y_i - \hat{f}^{-i}_i)^2 = \sum_{i} \left(\frac{y_i - \hat{f}_i}{1 - S_{ii}}\right)^2
$$

. . .

### Implementation

```{r LOOCV-runMean}
loocv <- function(k, y) {
  f_hat <- run_mean(y, k)
  mean(((y - f_hat) / (1 - 1 / k))^2, na.rm = TRUE)
}
```

[The implementation removes missing values due to the way we handle the
boundaries, and it uses `mean()` instead of `sum()` to correctly adjust for
this.

\pdfpcnote{
- For nearest neighbors, $S_{ii} = \frac{1}{k}$ for all $i$.

- **Derive the formula on the blackboard!**
}

##

```{r nuuk-running-loocv}
k <- seq(3, 40, 2)
cv_error <- sapply(k, function(kk) loocv(kk, nuuk_year$Temperature))
k_opt <- k[which.min(cv_error)]
```

\medskip\pause

```{r nuuk-running-loocv-plot}
#| fig-width: 5
#| fig-height: 2.5
#| echo: false
ggplot(tibble(k, cv_error), aes(k, cv_error)) +
  geom_vline(xintercept = k_opt, color = "dark orange") +
  geom_line() +
  geom_point() +
  labs(y = "LOOCV Error", x = expression(k))
```

##

The optimal choice of $k$ is `r k_opt`.

```{r nuuk-NN-plot3}
#| warning: false
#| echo: false
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot +
  geom_line(aes(y = run_mean(nuuk_year$Temperature, k_opt)), color = "blue")
```

## Wiggliness

The running mean is wiggly!

\pause\medskip

We can use kernel smoothing (as we did last week) to fix this

\pause\medskip

But another idea is to use \alert{smoothing splines} instead.

# Smoothing Splines

## Smoothing Splines

The minimizer of
$$
  L(f) = \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \underbrace{\int f''(z)^2 \mathrm{d} z}_{|f''|_2^2}
$$
is a **cubic spline** with **knots** in the data points $x_i$, that is, a
function
$$
  f = \sum_j \beta_j \varphi_j
$$
where $\varphi_j$ is a basis function for the $n$-dimensional space of such
splines.

\medskip\pause

Cubic splines are piecewise degree 3 polynomials in between knots.

\pdfpcnote{
- Without penalty term, any interpolating function would work well.
- Penalty term penalizes solutions with large second derivatives (in norm).
- **Derive the minimizer on the blackboard!**
}

## Loss Function

In vector notation
$$
  \hat{\symbf{f}} = \symbf{\Phi}\hat{\beta}
$$
with $\symbf{\Phi}_{ij} = \varphi_j(x_i)$, and\pause
$$
\begin{aligned}
  L(\symbf{f}) & = (\symbf{y} - \symbf{f})^T (\symbf{y} - \symbf{f}) + \lambda \lVert f''\rVert_2^2 \\ 
               & = (\symbf{y} - \symbf{\Phi}\beta)^T (\symbf{y} - \symbf{\Phi}\beta) + \lambda \beta^T \symbf{\Omega} \beta
\end{aligned}
$$
with
$$
  \symbf{\Omega}_{jk} = \int \varphi_j''(z) \varphi_k''(z) \mathrm{d}z.
$$

\pdfpcnote{
  Can rewrite penalty term in quadratic form $\lambda \beta^T \symbf{\Omega} \beta$.
}


### Solution


\pause\medskip

We recognize this as a _linear smoother_ with smoother matrix
$\symbf{S}_{\lambda}$.

\pdfpcnote{
  Similar to ridge (Tikhonov) regression: OLS + squared l2 penalty.
}

## Splines in R

```{r splines}
#| fig-width: 5
#| fig-height: 2
#| message: false
#| warning: false
library(Matrix)
library(splines)
# Note the specification of repeated boundary knots
knots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)
xx <- seq(0, 1, 0.005)
b_splines <- splineDesign(knots, xx)
matplot(xx, b_splines, type = "l", lty = 1)
```

\pdfpcnote{
Repeated knots at boundaries are necessary to ensure function goes throgh
control points and is smooth.
}

## Penalty Matrix

```{r Omega-Simpson}
#| echo: false
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) /
    6
}
```

:::: {.columns}

::: {.column width="40%"}

```{r Omega-Simpson2}
#| eval: false
omega <- pen_mat(
  seq(0, 1, 0.1)
)

image(Matrix(omega))
```

(See [slide source](/slides/lecture4.qmd) for implementation of `pen_mat()`.)

:::

::: {.column width="54%"}

```{r}
#| ref-label: Omega-Simpson2
#| fig-width: 4
#| fig-height: 2
#| echo: false
```

:::

::::

. . .


## Fitting a Smoothing Spline

We implement the matrix-algebra directly for computing $\symbf{S}_{\lambda}
\symbf{y}$.

```{r nuuk-smooth-spline}
inner_knots <- nuuk_year$Year

# Note that order does not matter
knots <- c(rep(range(inner_knots), 3), inner_knots)

phi <- splineDesign(knots, inner_knots)
```

. . .

```{r nuuk-smooth-spline2}
omega <- pen_mat(inner_knots)
smoother <- function(lambda) {
  phi %*%
    solve(
      crossprod(phi) + lambda * omega, # Phi^T Phi + lambda Omega
      crossprod(phi, nuuk_year$Temperature) # Phi^T y
    )
}
```

##

```{r nuuk-smoother-plot}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot +
  geom_line(aes(y = smoother(10)), color = "blue") +
  geom_line(aes(y = smoother(1000)), color = "red") +
  geom_line(aes(y = smoother(100000)), color = "orange")
```

## Generalized Cross-Validation

With $\text{df} = \text{trace}(\symbf{S}) = \sum_{i=1}^n S_{ii}$ we replace
$S_{ii}$ in LOOCV by $\text{df} / n$ to get the **generalized** cross-validation
criterion
$$
  \mathrm{GCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - \mathrm{df} / n}\right)^2.
$$

. . .

### Implementation

```{r gcv-smooth-spline}
gcv <- function(lambda, y) {
  S <- phi %*% solve(crossprod(phi) + lambda * omega, t(phi))
  df <- sum(diag(S)) # The trace of the smoother matrix
  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE)
}
```

## GCV-Optimal $\lambda$

Apply `gcv()` across grid of $\lambda$-values and choose $\lambda$ that
minimizes GCV.

```{r nuuk-spline-gcv}
lambda <- seq(50, 250, 2)
gcv <- sapply(lambda, gcv, y = nuuk_year$Temperature)
lambda_opt <- lambda[which.min(gcv)]
```

```{r gc-plot}
#| echo: false
#| fig-width: 5
#| fig-height: 2
tibble(lambda, gcv) |>
  ggplot(aes(lambda, gcv)) +
  geom_vline(xintercept = lambda_opt, color = "dark orange") +
  geom_line() +
  geom_point() +
  labs(y = "GCV Error", x = expression(lambda))
```

## GCV-Optimal Smoothing Spline

```{r nuuk-spline-opt}
#| fig-width: 5
#| fig-height: 2.5
smooth_opt <- smoother(lambda_opt)
nuuk_plot + geom_line(aes(y = smooth_opt), color = "blue")
```

### Using `smooth.spline()`

```{r nuuk-spline-opt2, fig.width = 9, fig.height = 5}
smooth <- smooth.spline(nuuk_year$Year, nuuk_year$Temperature, all.knots = TRUE)
nuuk_plot + geom_line(aes(y = smooth$y), color = "blue")
```

\pdfpcnote{
- Disable fast heuristic to have similar results as us.
}

## Efficient Computations

In practice we use $p < n$ basis functions.

Using the singular value decomposition
$$
  \Phi = \symbf{U} D \symbf{V}^T
$$
\pause it holds that
$$
  \symbf{S}_{\lambda} = \widetilde{\symbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\symbf{U}}^T
$$
where $\widetilde{\symbf{U}} = \symbf{U} \symbf{W}$ and\pause
$$
  D^{-1} \symbf{V}^T \symbf{\Omega} \symbf{V} D^{-1} = \symbf{W} \Gamma \symbf{W}^T.
$$

\pdfpcnote{
- Can be done whether we use all or a subset of our basis functions
- $S_\lambda$ is diagonalized
- DERIVE $\symbf{S}_\lambda$ ON BLACKBOARD!
}

## Interpretation

The coefficients, $\hat{\beta} = \widetilde{\symbf{U}}^Ty$, are computed for
expanding $y$ in the basis given by the columns of $\widetilde{\symbf{U}}$.

\pause\medskip

The $i$-th coefficient is shrunk towards 0,
$$
  \hat{\beta}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.
$$

\pause\medskip

The smoothed values, $\widetilde{\symbf{U}} \hat{\beta}(\lambda)$, are computed
as an expansion using the shrunken coefficients.

## The Demmler-Reinsch Basis (Columns of $\widetilde{\symbf{U}}$)

```{r spline-diagonalization}
#| echo: false
#| fig-width: 5
#| fig-height: 2.5
inner_knots <- seq(1867, 2013, length.out = 18)
phi <- splineDesign(c(rep(range(inner_knots), 3), inner_knots), nuuk_year$Year)
omega <- pen_mat(inner_knots)
phi_svd <- svd(phi)
omega_tilde <- t(
  t(
    crossprod(phi_svd$v, omega %*% phi_svd$v)
  ) /
    phi_svd$d
) /
  phi_svd$d

# It is safer to use the numerical singular value decomposition ('svd')
# for diagonalizing a positive semidefinite matrix than to use a
# more general numerical diagonalization implementation such as 'eigen'.
omega_tilde_svd <- svd(omega_tilde)
u_tilde <- phi_svd$u %*% omega_tilde_svd$u

colnames(u_tilde) <- paste0(rep("u", 20), 1:20)
bind_cols(select(nuuk_year, Year), as_tibble(u_tilde)) |>
  gather(key = "term", value = "value", -Year, factor_key = TRUE) |>
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

## The Eigenvalues $\gamma_i$

```{r spline-gamma}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3.5
library(patchwork)

d <- omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2
```

# S3 Smoother

## A LOESS Smoother

LOESS: locally weighted scatterplot smoothing^[Default behavior in ggplot2 is to
use LOESS for small data sets and generalized additive models (GAMs) for larger
data sets.]

```{r loess}
#| fig-width: 5
#| fig-heigth: 3
nuuk_plot + geom_smooth()
```

## LOESS

- A mix of nearest neighbors and smoothing splines.
- Implemented in `loess()`
- Does not automatically select the span (tuning parameter)
- Default span is 0.75, often too large
- Nonlinear, so the formulas for linear smoothers do not apply.
- Instead use 5- or 10-fold cross validation for tuning.
- Or use the GCV criterion (using `trace.hat` entry in the returned object).

. . .

Loess is a **robust** smoother (linear smoothers are not) and relatively
insensitive to outliers.

## Another LOESS Smoother

```{r loess2}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "loess", span = 0.5)
```

## A Linear "Smoother"

```{r linear-smoother}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "lm")
```

## A Polynomial Smoother

```{r poly-smoother}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "lm", formula = y ~ poly(x, 5))
```

## Another Polynomial Smoother

```{r poly-smother2}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "lm", formula = y ~ poly(x, 20))
```

## A Spline Smoother

```{r spline-smoother}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "gam", formula = y ~ s(x))
```

## Another Spline Smoother

```{r spline-smoother2}
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "gam", formula = y ~ s(x, k = 100))
```

## Smoothing with ggplot2

The `geom_smooth()` function easily adds miscellaneous model fits or scatter
plot smoothers to the scatter plot.

\pause\medskip

Spline smoothing is performed via the `gam()` function in the mgcv package,
whereas loess smoothing is via the `loess()` function in the stats package.

\pause\medskip

Any "smoother" can be used that supports a formula interface and has a
prediction function adhering to the standards of `predict.lm()`.

## Our Running Mean Implementation

```{r run_mean_def_again, ref.label="run_mean_def"}
run_mean
```

## An Interface for `geom_smooth()`

```{r}
running_mean <- function(..., data, k = 5) {
  ord <- order(data$x)
  s <- run_mean(data$y[ord], k = k)
  structure(
    list(x = data$x[ord], y = s),
    class = "running_mean"
  )
}
```

. . .

### Prediction Method

```{r}
predict.running_mean <- function(object, newdata, ...) {
  approx(object$x, object$y, newdata$x)$y # Linear interpolation
}
```

## A Running Mean

```{r}
#| warning: false
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot + geom_smooth(method = "running_mean", se = FALSE, n = 200)
```

## Specifying Arguments to Our Method

```{r}
#| warning: false
#| fig-width: 5
#| fig-height: 2.5
nuuk_plot +
  geom_smooth(
    method = "running_mean",
    se = FALSE,
    n = 200,
    method.args = list(k = 13)
  )
```

## Handling Boundary Values

```{r}
running_mean <- function(..., data, k = 5, boundary = NULL) {
  ord <- order(data$x)
  y <- data$y[ord]
  n <- length(y)
  m <- floor((k - 1) / 2)
  if (m > 0 && !is.null(boundary)) {
    if (boundary == "pad") {
      y <- c(rep(y[1], m), y, rep(y[n], m))
    }
    if (boundary == "rev") {
      y <- c(y[m:1], y, y[n:(n - m + 1)])
    }
  }
  s <- run_mean(y, k = k)
  if (!is.null(boundary)) {
    s <- na.omit(s)
  }
  structure(list(x = data$x[ord], y = s), class = "running_mean")
}
```

### No Boundary Adjustment

```{r}
#| warning: false
#| echo: false
#| fig-width: 5
#| fig-height: 3
smooth1 <- geom_smooth(
  method = "running_mean",
  se = FALSE,
  n = 200,
  method.args = list(k = 13)
)
nuuk_plot + smooth1
```

### Padding

```{r}
#| warning: false
#| echo: false
#| fig-width: 5
#| fig-height: 3
smooth2 <- geom_smooth(
  method = "running_mean",
  se = FALSE,
  n = 200,
  method.args = list(k = 13, boundary = "pad"),
  color = "red"
)
nuuk_plot + smooth2 + smooth1
```

### Reversion

```{r}
#| warning: false
#| echo: false
#| fig-width: 5
#| fig-height: 3
smooth3 <- geom_smooth(
  method = "running_mean",
  se = FALSE,
  n = 200,
  method.args = list(k = 13, boundary = "rev"),
  color = "purple"
)
nuuk_plot + smooth3 + smooth2 + smooth1
```

# Exercises

## Exercises

### Exercise 1

Write a parallelized version of the `gauss()` function using the `foreach`
package. Your function should take a vector and devide the work across multiple
cores.

\medskip\pause

Benchmark your implementation against the original `gauss()` function.


## Exercise

$\symbf{S}$ is a sparse matrix but it is stored in a dense format currently.

**Note:** The following is **not** the real $\symbf{S}$. It is just used for
illustration.

```{r ex-sparse}
#| warning: false
k <- 11
y <- rnorm(n)
w <- c(rep(1, k * n), rep(0, n * (n - k)))
S <- matrix(w, n, n, byrow = TRUE)
```

Use the [Matrix package](https://CRAN.R-project.org/package=Matrix) to convert
$\symbf{S}$ to a sparse format using `Matrix::Matrix(S, sparse = TRUE)` and
then benchmark the sparse multiplication against the dense one. Parameterize the
benchmark with $n$ and $k$.

\pause\medskip

Don't include the time used to construct the matrix in the benchmark.

```{r ex-sparse-result}
#| echo: false
#| message: false
#| warning: false
#| fig-width: 5.5
sparse_bench <- bench::press(
  n = c(512, 1024, 4196),
  k = c(10, 100),
  {
    k <- 11
    y <- rnorm(n)
    w <- c(rep(1, k * n), rep(0, n * (n - k)))
    S <- matrix(w, n, n, byrow = TRUE)
    S_sparse <- Matrix::Matrix(S, sparse = TRUE)
    bench::mark(
      S %*% y,
      S_sparse %*% y,
      check = FALSE
    )
  }
)

plot(sparse_bench)
```
