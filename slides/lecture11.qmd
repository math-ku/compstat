---
title: "Stochastic Gradient Descent"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(mvtnorm)

set.seed(1422)

source(here::here("R", "sgd.R"))
```

## Last Time

### The Expectation-Maximization (EM) Algorithm

A general approach to maximum likelihood estimation with latent variables.

. . .

### Fisher Information

We saw three different ways to define the Fisher information
and how they relate to the EM algorithm.

## Recap: The EM Algorithm

The EM algorithm iteratively optimizes
$$
\theta^{(t+1)} = \argmax_\theta Q(\theta \mid \theta^{(t)}),
$$
with
$$
Q(\theta \mid \theta^{(t)}) = \E_{Z \mid X,\theta^{(t)}}\big(\log p(X, Z \mid \theta)\big).
$$

\medskip\pause

Convergence is **linear** near a local maximum.

## Recap: Fisher Information

The Fisher information is defined as
$$
i(\theta) = - \E\left( \frac{\partial^2}{\partial \theta^2} \log p(X \mid \theta) \right).
$$

\medskip\pause

We mentioned three ways to compute it:

1. Differentiate the score function (defined through $Q$).\pause
2. Compute complete and missing information and using the information
   identity. \pause
3. Differentiate the EM mapping.

## Today

### Gaussian Mixture EM Example

We pick up where we left off last time,
focusing on the Gaussian mixture example.

. . .

### Stochastic Gradient Descent

Useful stochastic method for optimization

\medskip\pause

Can be used in a **mini-batch** version.

# EM Gaussian Mixture Example

## Finite Mixtures

A finite mixture is 

Let $Z
\in \{1, \ldots, K\}$ with $P(Z = k) =
p_k$, and the conditional
distribution of $X$ given $Z
= k$ has density $f_k( \cdot \mid
\psi_k)$.

\medskip\pause

The joint density is
$$
f_k(x \mid \psi_k) p_k
$$

. . .

and the marginal density for the distribution of $X$ is
$$
f(x \mid \theta) =  \sum_{k=1}^K f_k(x \mid \psi_k) p_k.
$$

## Gaussian Mixtures $(K
= 2)$

The two Gaussian distributions are parametrized by five parameters
$\mu_1,
\mu_2 \in \mathbb{R}$ and $\sigma_1, \sigma_2 > 0$, and
$p = P(Z = 1) = 1
- P(Z = 2)$.

\medskip\pause

The conditional distribution of $X$ given $Z =
k$ is
$$
f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.
$$

\medskip\pause

The marginal density is
$$
f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.
$$

## Simulation

:::: {.columns align="center"}

::: {.column width="53%"}

```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

:::

::: {.column width="41%"}

```{r, echo=FALSE}
#| fig-cap: "Histogram of simulated data from a two-component Gaussian mixture model."
gausdens <- function(x, mu1, mu2, sigma1, sigma2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

xx <- seq(-3, 11, 0.1)

pl <- ggplot(data.frame(x), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightgray") +
  geom_line(
    aes(x, gausdens(x, mu1, mu2, sigma1, sigma2)),
    color = "black",
    linewidth = 1
  ) +
  labs(y = "Density", x = "x")

pl
```

:::

::::

## Log-likelihood

We assume $\sigma_1$ and $\sigma_2$ known.

```{r gaus-mix-loglik}
neg_loglik <- function(par, x) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  -sum(log(
    p *
      exp(-(x - mu1)^2 / (2 * sigma1^2)) /
      sigma1 +
      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  ))
}
```

## General-Purpose Optimization

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), neg_loglik, x = x)[1:2]
```

. . .

Again, however, initialization matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), neg_loglik, x = x)[1:2]
```

## The $Q$-Function

The complete data log-likelihood is
$$
\sum_{i=1}^n \symbf{1}_{z_i = 1} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + \symbf{1}_{z_i = 2}\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$

. . .

and
$$
Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$
where $\hat{p}_i
= P_{\theta'}(Z_i = 1 \mid X_i =
x_i)$.

\medskip\pause

The maximum is attained at
$$
\hat{\theta} = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).
$$

## The E-Step

The conditional probability in a mixture model is generally
$$
P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}
$$

. . .

which for the $K
= 2$ Gaussian case gives
$$
\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.
$$

## EM Algorithm

```{r em-alg}
em <- function(par, em_step, eps = 1e-6, maxit = 20, cb = NULL) {
  for (i in seq_len(maxit)) {
    par0 <- par
    par <- em_step(par)

    if (!is.null(cb)) {
      cb()
    }

    if (sum((par - par0)^2) <= eps * (sum(par^2) + eps)) {
      break
    }
  }
  par
}
```

## Implementation

See the source code for an implementation.

```{r source-em-mix}
#| echo: false
e_step_gauss <- function(par, x, sigma1 = 1, sigma2 = sigma1) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  a <- p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1
  b <- (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  a / (a + b)
}

m_step_gauss <- function(p_hat, x) {
  n <- length(x)
  N1 <- sum(p_hat)
  N2 <- n - N1
  c(N1 / n, sum(p_hat * x) / N1, sum((1 - p_hat) * x) / N2)
}

create_em_gauss_step <- function(x, sigma1 = 1, sigma2 = sigma1) {
  force(x)
  force(sigma1)
  force(sigma2)

  function(par) m_step_gauss(e_step_gauss(par, x, sigma1, sigma2), x)
}

em_gauss_step <- create_em_gauss_step(x, sigma1, sigma2)
```

## Testing

```{r test-em-gauss-mix}
em(c(0.5, -0.5, 4), em_gauss_step)
```

. . .

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.5, -0.5, 4), neg_loglik, x = x)[1:2]
```

. . .

```{r run-em-gauss-mix-again}
em(c(0.9, 3, 1), em_gauss_step) # Starting value still matters
```

## Gradients and Numerical Differentiation

```{r numderiv-grad1}
library(numDeriv)
grad1 <- function(par) grad(function(par) neg_loglik(par, x), par)
```

. . .

```{r numderiv-grad2}
Q <- function(par, par_prime) {
  p_hat <- e_step_gauss(par_prime, x, sigma1, sigma2)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    p_hat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - p_hat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}
grad2 <- function(par) -grad(Q, par, par_prime = par)
```

## Gradient Identity

```{r gradient-identities1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

. . .

```{r gradient-identities2}
par_hat <- em(c(0.5, 0, 4), em_gauss_step)
grad1(par_hat)
grad2(par_hat)
```

## Convergence

```{r tracing-convergence}
library(CSwR)
em_tracer <- tracer(
  c("par0", "par", "nll", "h_prime"),
  Delta = 0,
  expr = quote({
    nll <- neg_loglik(par, x)
    h_prime <- sum(grad2(par)^2)
  })
)

par_hat <- em(c(0.2, 2, 2), em_gauss_step, cb = em_tracer$tracer)

em_trace <- summary(em_tracer)
```

##

:::: {.columns}

::: {.column width="47%"}

```{r convergence-plots-1, warning=FALSE}
autoplot(
  em_trace,
  y = nll - min(nll)
)
```

:::

. . .

::: {.column width="47%"}

```{r convergence-plots-2, warning=FALSE}
autoplot(
  em_trace,
  y = h_prime
)
```

:::

::::

##

```{r mixture-convergence}
#| echo: false
gaussdens2 <- function(x, p, mu1, mu2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

em_gauss_illustration <- function(i, em_trace) {
  p_i <- em_trace$par0.1[i]
  mu1_i <- em_trace$par0.2[i]
  mu2_i <- em_trace$par0.3[i]

  pl +
    geom_line(
      aes(y = gaussdens2(x, p_i, mu1_i, mu2_i)),
      color = "darkorange",
      size = 1
    ) +
    lims(y = c(0, 0.25)) +
    labs(title = paste0("k = ", i))
}
```

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(1, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(4, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(8, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(12, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(16, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(20, em_trace)
```

## Label Switching

If we set $\sigma_1 = \sigma_2$, then the likelihood is invariant to
switching the labels of the components, i.e., $(p, \mu_1, \mu_2)$ and
$(1-p, \mu_2, \mu_1)$ give the same likelihood.

\medskip\pause

So the EM algorithm may converge to
**either** of the two symmetric modes.

```{r mixture-gaus-sim2}
#| echo: false
sigma1 <- 1
sigma2 <- 1
mu1 <- -0.5
mu2 <- 4
p <- 0.8
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)

em_gauss_step_alt <- create_em_gauss_step(x, sigma1)
```

. . .

```{r}
em(c(0.2, 3, -0.5), em_gauss_step_alt)
```

. . .

```{r}
em(c(0.8, -3, -0.5), em_gauss_step_alt)
```

. . .

In this example, we generated data with $p = 0.8$ and $\sigma_1 = \sigma_2$.

# Stochastic Gradient Descent

## Minimizing Sums

Many of the functions we are trying to minimize are of the form
$$
\frac{1}{n} \sum_{i=1}^n f_i(x).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

### Gradient Descent

Since $\nabla \left(\sum_{i=1}^n f_i(x)\right) = \sum_{i=1}^n \nabla f_i(x)$, GD
steps are
$$
x_{k+1} = x_k - t \sum_{i=1}^n \nabla f_i(x_k).
$$

:::

. . .

::: {.column width="47%"}

### Stochastic Gradient Descent

SGD instead takes steps
$$
x_{k+1} = x_k - t \nabla f_{i}(x_k)
$$
where $i$ is an index in $\{1, \ldots, n\}$ drawn at random.

:::

::::

## Unbiasedness

If $i$ is drawn uniformly at random from $\{1, \ldots, n\}$ then
$$
\operatorname{E}(\nabla f_i(x)) = \nabla f(x).
$$

So SGD gradients are **unbiased** estimates of the full gradient.

. . .

### Iteration Cost

The main benefit of this is that the cost of each iteration is much lower:
$O(p)$ vs. $O(np)$ (for full gradient descent).

. . .

### Epoch

When discussing SGD, we often refer to an **epoch** as $n$ iterations of SGD.

\medskip\pause

A full pass over the data.

## Example: Logistic Regression

:::: {.columns}

::: {.column width="47%"}

We have
$$
f_i(\beta) = \log(1 + e^{-y_i x^T_i \beta})
$$
and
$$
\nabla f_i(\beta) = - \frac{y_i x_i}{1 + e^{y_i x^T_i \beta}}.
$$

. . .

SGD typically converges quickly at the start but slows down as it approaches
the minimum.

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-height: 3.6
#| fig-width: 3.2
set.seed(123)

n <- 1000
p <- 2
Sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2)
mu <- c(0, 0)

X <- rmvnorm(n, mean = mu, sigma = Sigma)
beta <- c(1, 2)
y <- rbinom(n, 1, plogis(X %*% beta))

res_glm <- glm.fit(X, y, family = binomial())

loss_2d <- function(beta1, beta2, X, y) {
  beta <- c(beta1, beta2)
  z <- X %*% beta
  p_hat <- 1 / (1 + exp(-z))
  -mean(y * log(p_hat) + (1 - y) * log(1 - p_hat))
}

beta1 <- seq(-1, 3, length = 100)
beta2 <- seq(-1, 3, length = 100)

loss_2d_vectorized <- Vectorize(function(b1, b2) loss_2d(b1, b2, X, y))

z <- outer(beta1, beta2, loss_2d_vectorized)

res_sgd <- logreg_sgd(X, y, max_epochs = 100, batch_size = 1)
res_sgd2 <- logreg_sgd(X, y, max_epochs = 100, batch_size = 1, K = 5, a = 0.5)
res_gd <- logreg_sgd(X, y, max_epochs = 100, batch_size = n)

pal <- palette.colors(palette = "Okabe-Ito")

contour(
  beta1,
  beta2,
  z,
  col = "dark grey",
  drawlabels = FALSE,
  asp = 1
)

lines(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ],
  col = pal[3]
)
points(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ],
  col = pal[3],
  pch = 19,
  cex = 0.5
)

lines(
  res_gd$beta_history[1, ],
  res_gd$beta_history[2, ],
  col = pal[1]
)
points(
  res_gd$beta_history[1, ],
  res_gd$beta_history[2, ],
  col = pal[1],
  pch = 19,
  cex = 0.5
)

points(
  res_glm$coefficients[1],
  res_glm$coefficients[2],
  col = pal[2],
  pch = 19,
  cex = 1
)

legend(
  "bottomright",
  legend = c("SGD", "GD"),
  col = pal[c(3, 1)],
  pch = 19,
  cex = 0.8
)
```


:::

::::

## Convergence in Epochs and Iterations

In terms of iterations, gradient descent converges much more quickly than SGD.
But in terms of epochs, SGD can be faster.

\medskip\pause

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| fig-cap: "Convergence of gradient descent (GD) and stochastic gradient descent (SGD) for logistic regression i
#|   terms of iterations and epochs."
#| echo: false
loss_optim <- loss_2d(res_glm$coefficients[1], res_glm$coefficients[2], X, y)
loss1 <- loss_2d_vectorized(res_gd$beta_history[1, ], res_gd$beta_history[2, ])
loss2 <- loss_2d_vectorized(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ]
)
loss3 <- loss_2d_vectorized(
  res_sgd2$beta_history[1, ],
  res_sgd2$beta_history[2, ]
)

n1 <- length(loss1)
n2 <- length(loss2)
n3 <- length(loss3)

conv_data <- tibble(
  method = rep(c("GD", "SGD"), times = c(length(loss1), length(loss2))),
  epoch = c(seq_len(n1), seq_len(n2) / n),
  loss = c(loss1 - loss_optim, loss2 - loss_optim),
  iteration = c(seq_len(n1), seq_len(n2))
)

epochs_plot <- ggplot(conv_data, aes(epoch, loss, col = method)) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality", color = NULL)

iteration_plot <- ggplot(conv_data, aes(iteration, loss, col = method)) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality", color = NULL)

ggpl <- epochs_plot +
  iteration_plot +
  plot_layout(axes = "collect_y", guides = "collect")

ggpl
```

## Step Size (Learning Rate)

:::: {.columns align = "center"}

::: {.column width="47%"}

For gradient descent, we could use a fixed step size (learning rate), and
guarantee convergence.

\medskip\pause

But this is **not** the case for SGD.

\medskip

Can we find conditions on the step sizes $t_k$ that ensure convergence of SGD? Yes!

\medskip

But first, we need to introduce the concept of **strong convexity**.

:::

::: {.column width="47%"}

```{r}
#| fig-cap: "Convergence of SGD for logistic regression with a fixed step size."
#| echo: false
res_sgd <- logreg_sgd(X, y, max_epochs = 200, batch_size = 1, gamma0 = 5)
pal <- palette.colors(palette = "Okabe-Ito")

loss <- loss_2d_vectorized(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ]
)

ggplot(
  tibble(iteration = seq_along(loss), loss = loss - loss_optim),
  aes(iteration, loss)
) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality")
```

:::

::::

## Strong Convexity

A differentiable function $f: \mathbb{R}^p \to \mathbb{R}$ is **strongly convex** if
$$
f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \lVert y - x\rVert_2^2.
$$

\medskip\pause

```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 5
f <- function(x) x^2 # strongly convex function
df <- function(x) 2 * x # gradient
mu <- 1 # strong convexity parameter
x0 <- 1 # point of tangent

x <- seq(-1, 2, length.out = 400)

# Tangent line at x0
tangent <- df(x0) * (x - x0) + f(x0)

# Strong convexity lower bound
sc_bound <- tangent + 0.5 * mu * (x - x0)^2

plot(
  x,
  f(x),
  type = "l",
  axes = FALSE,
  ylim = c(-0.5, 4),
  ylab = "f(x)",
  xlab = "x"
)
lines(x, tangent, col = "darkorange", lty = 2)
lines(x, sc_bound, col = "steelblue", lty = 3)
points(x0, f(x0), pch = 19)
legend(
  "topleft",
  legend = c(
    expression(f(x)),
    expression(f(x) + nabla * f(x)^T * (y - x)),
    expression(
      f(x) +
        nabla * f(x)^T * (y - x) +
        frac(mu, 2) * group("|", group("|", y - x, "|"), "|")^2
    )
  ),
  col = c("black", "darkorange", "steelblue"),
  box.col = "transparent",
  lty = c(1, 2, 3),
  bg = "white"
)
```

## Example: Strong Convexity


## The Robbins--Monro Convergence Theorem

Now, suppose $f$ is strongly convex and
$$
\operatorname{E}\big(\rVert \nabla f(x)\lVert_2^2\big) \leq A + B \lVert x\rVert_2^2
$$
for some constants $A$ and $B$. If $x^*$ is the global minimizer of $f$
then $x_n$ converges almost surely toward $x^*$ if
$$
\sum_{k=1}^{\infty} t_k^2 < \infty \quad \text{and} \quad
\sum_{k=1}^{\infty} t_k = \infty.
$$

. . .

The theorem applies to learning rates satisfying
$$
t_k \propto \frac{1}{k}.
$$

## Diminishing Step Sizes

Assume cyclic rule and set $t_k = t$ for $n$ updates. We get
$$
x_{k + n} = x_k - t \sum_{i=1}^n \nabla f_i(x_{k + i - 1}).
$$

. . .

Meanwhile, **full** gradient descent with step size $nt$ would give
$$
x_{k + 1} = x_k - t \sum_{i=1}^n \nabla f_i(x_k).
$$

. . .

Difference between the two is
$$
t \sum_{i=1}^n \left( \nabla f_i(x_{k + i - 1}) - \nabla f_i(x_k) \right)
$$
which does not tend to zero if $t$ is constant.

\pdfpcnote{
  What am I trying to say here?
}

## Convergence Rates

### Gradient Descent

For convex $f$, GD with diminishing step sizes converges at rate
$$
f(x_n) - f^* = O(1/\sqrt{k}).
$$

. . .

When $f$ is differentiable with Lipschitz gradient, we get
$$
f(x_n) - f^* = O(1/k).
$$

. . .

### SGD

For convex $f$ with diminishing step sizes, SGD converges at rate
$$
\operatorname{E}f(x_n) - f^* = O(1/\sqrt{k}).
$$

. . .

But this **does not improve** when $f$ is differentiable with Lipschitz
gradient.

## Rates for Strongly Convex Functions

When $f$ is strongly convex and has Lipschitz gradient, GD satisfies
$$
f(x_n) - f^* = O(\delta^k), \qquad \delta \in (0, 1).
$$

. . .

Under same conditions, SGD gives us
$$
\operatorname{E}(f(x_n) - f^*) = O(1/k).
$$

. . .

So SGD does **not** enjoy linear convergence rates under strong convexity.

## Illustration of Convergence Rates

## Permuted Gradient Descent

## Mini-Batch Gradient Descent

Idea: take mini-batch $A_k$ of size $b$ of the data and iterate through
$$
x_{k+1} = x_k - \frac{t}{b} \sum_{i \in A_k} \nabla f_i(x_k).
$$

. . .

Estimates are still unbiased,
$$
\operatorname{E}\frac{1}{b}\sum_{i \in A_k}\nabla f_i(x) = \nabla f(x),
$$
but variance is reduced by a factor of
$1/b$.

\medskip\pause

Under Lipschitz gradient, rate goes from $O(1/\sqrt{k})$ to
$O(1/\sqrt{bk} + 1/k)$.

\medskip\pause

Typically more efficient than standard SGD for various computational reasons,
but somewhat **less** robust to local minima.

##

![](../images/lecture12-batch-sizes.png)

## So, Why Use SGD?

- If $n \gg p$, then we can pick a batch size that is reasonably accurate but
  still much smaller than $n$.
- In many applications (e.g. deep learning), we don't care about optimizing to
  high accuracy.
- Stochastic gradients help us escape local minima.

## Example of SGD beating GD

## Learning Rate

The learning rate $t$ is crucial for SGD, but hard to set.

\medskip\pause

Convergence theorem does not give much of a hint.

. . .

### A Class of Decay Schedules

$$t_k = \frac{t_0 K}{K + k^a} = \frac{t_0 }{1 + K^{-1} k^{a}}$$
with initial learning rate $t_0
> 0$ and constants $K, a >
0$.

\medskip\pause

Convergence is ensured by Robbinsâ€“Monro if $a \in (0.5, 1]$.

\medskip\pause

Fixing the exponent $a$ and picking a target rate, $t_1$, to be reached
after $k_1$ steps, we can solve for $K$ and get
$$K = \frac{k_1^a t_1}{t_0 - t_1}.$$

##

```{r decay, cache = TRUE}
decay_scheduler <- function(
  t0 = 1,
  a = 1,
  K = 1,
  t1 = NULL,
  n1 = NULL
) {
  force(a)

  if (!is.null(t1) && !is.null(n1)) {
    K <- n1^a * t1 / (t0 - t1)
  }

  b <- t0 * K

  function(n) b / (K + n^a)
}
```

##

```{r decay-fig, echo=FALSE, dependson="decay", fig.cap="Examples of learning rate schedules."}
#| fig-height: 3.2
#| fig-width: 5.2
grid_par <- expand.grid(
  n = seq(0, 1000, 2),
  K = c(25, 50, 100),
  a = c(0.6, 1),
  t0 = 1
)

grid_par <- dplyr::mutate(grid_par, rate = t0 * K / (K + n^a))

p1 <- ggplot(grid_par, aes(n, rate, color = factor(K), linetype = factor(a))) +
  geom_line() +
  scale_color_discrete("K") +
  scale_linetype_discrete("a") +
  guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2))

grid_par <- expand.grid(
  n = seq(0, 1000, 2),
  K = c(1e4, 1e5, 1e6),
  a = c(2, 2.5),
  t0 = 1
)

grid_par <- dplyr::mutate(grid_par, rate = t0 * K / (K + n^a))

p2 <- ggplot(grid_par, aes(n, rate, color = factor(K), linetype = factor(a))) +
  geom_line() +
  scale_color_discrete("K", labels = c(quote(10^4), quote(10^5), quote(10^6))) +
  scale_linetype_discrete("a") +
  guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2))

p1 + p2 + plot_layout(axes = "collect_y")
```

## So, Is The Problem of Setting Learning Rate Solved?

No, unfortunately not

:::: {.columns}

::: {.column width="47%"}

![](../images/lecture12-lrdecay-fast.png)

:::

. . .

::: {.column width="47%"}

![](../images/lecture12-lrdecay-slow.png)

:::

::::

## Example: Least Squares Loss

Consider an objective of the type
$$f(\beta) = \frac{1}{2n} \sum_{i=1}^n  (y_i - \mu(x_i, \beta))^2.$$

. . .

Gives us gradient

$$\nabla f_i(\beta) = -\nabla_{\beta} \mu(x_i, \beta) (y_i - \mu(x_i, \beta) ).$$

. . .

For a linear model, $\mu(x_i, \beta) = x_i^T \beta$, and
$\nabla_{\beta} \mu(x_i, \beta) = x_i$ so
$$\nabla f_i(\beta) = - x_i (y_i - x_i^T \beta).$$

## A Poisson Regression Model

$$
Y_i \mid Z_i = z_i \sim \mathrm{Poisson}(e^{\beta_0 + \beta_1 z_i})
$$ for
$\beta = (\beta_0, \beta_1)^T$ and $Z_i$ uniformly distributed in $(-1, 1)$.

\medskip\pause

The conditional mean of $Y_i$ given $Z_i = z_i$ is thus
$$
\mu(z_i, \beta) = e^{\beta_0 + \beta_1 z_i}.
$$

## Least Squares for the Poisson Model

```{r online-poisson-SG-SE0, echo=-1, cache = TRUE}
set.seed(13102020)
n <- 5000
beta_true <- c(2, 3)
mu <- function(z, beta) exp(beta[1] + beta[2] * z)
beta <- vector("list", n)
```

. . .

```{r online-poisson-SG-SE1}
rate <- decay_scheduler(t0 = 0.0004, K = 100)
beta[[1]] <- c(beta0 = 1, beta1 = 1)
```

##

```{r online-poisson-SG-SE, cache = TRUE}
for (i in 2:n) {
  # Simulating a new data point
  z <- runif(1, -1, 1)
  y <- rpois(1, mu(z, beta_true))
  # Update via squared error gradient
  mu_old <- mu(z, beta[[i - 1]])
  beta[[i]] <- beta[[i - 1]] - rate(i) * mu_old * (mu_old - y) * c(1, z)
}
beta[[n]] # Compare this to beta_true
```

### Log-Likelihood Loss

```{r online-poisson-SG-LL0, echo=-c(1, 2), dependson="online-poisson-SG-SE", cache = TRUE}
set.seed(13102020)
beta_se <- cbind(
  as.data.frame(do.call(rbind, beta)),
  data.frame(iteration = 1:n, loss = "squared error")
)

rate <- decay_scheduler(t0 = 0.01, K = 100)
beta[[1]] <- c(beta0 = 1, beta1 = 1)
```

##

```{r online-poisson-SG-LL, dependson=c("online-poisson-SG-SE", "online-poisson-SG-LL0"), cache = TRUE}
for (i in 2:n) {
  # Simulating a new data point
  z <- runif(1, -1, 1)
  y <- rpois(1, mu(z, beta_true))
  # Update via log-likelihood gradient
  mu_old <- mu(z, beta[[i - 1]])
  beta[[i]] <- beta[[i - 1]] - rate(i) * (mu_old - y) * c(1, z)
}
beta[[n]] # Compare this to beta_true
```

##

```{r pois-sgd, echo=FALSE, dependson="online_poisson_SG", warning=FALSE, fig.height=3, fig.width=5.1, fig.cap = "Convergence for Poisson regression model."}
beta_all <- rbind(
  cbind(
    as.data.frame(do.call(rbind, beta)),
    data.frame(iteration = 1:n, loss = "log-likelihood")
  ),
  beta_se
) |>
  tidyr::pivot_longer(cols = c("beta0", "beta1"), names_to = "Parameter") |>
  dplyr::filter(iteration %% 50 == 0)

ggplot(beta_all, aes(iteration, value, color = Parameter)) +
  geom_hline(yintercept = beta_true[1], color = "blue") +
  geom_hline(yintercept = beta_true[2], color = "red") +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  ylim(c(1.75, 3.25)) +
  ylab("Parameter value") +
  xlab("Number of data points") +
  facet_wrap("loss") +
  scale_color_manual(values = c("blue", "red"))
```

## Implementation

```{r SG, cache = TRUE}
sgd <- function(
  par,
  grad, # Function of parameter and observation index
  n, # Sample size
  t, # Decay schedule or a fixed learning rate
  maxiter = 100, # Max epoch iterations
  sampler = sample, # How data is resampled. Default is a random permutation
  cb = NULL,
  ...
) {
  t <- if (is.function(t)) t(1:maxiter) else rep(t, maxiter)
  for (k in 1:maxiter) {
    if (!is.null(cb)) {
      cb()
    }
    samp <- sampler(n)
    for (j in 1:n) {
      i <- samp[j]
      par <- par - t[k] * grad(par, i, ...)
    }
  }
  par
}
```

## Poisson Regression Model, Mini-Batch Learning

```{r pois-gradient, dependson="SG", echo=-1, cache = TRUE}
set.seed(17102020)
n <- 50 # Small sample size
z <- runif(n, -1, 1)
y <- rpois(n, mu(z, beta_true))
grad_pois <- function(par, i) (mu(z[i], par) - y[i]) * c(1, z[i])
```

. . .

```{r batch-poisson-SG, dependson="pois-gradient"}
library(CSwR)
pois_SG_tracer <- tracer("par", Delta = 0)
rate <- decay_scheduler(t0 = 0.02, t1 = 0.001, n1 = 1000)
sgd(c(0, 0), grad_pois, n, rate, 1000, cb = pois_SG_tracer$tracer)
```

. . .

### Test

```{r pois-beta-hat, dependson="pois-gradient", echo=1}
beta_hat <- coefficients(glm(y ~ z, family = poisson))
cat("beta_hat:", beta_hat)
```

## Poisson Regression Model, Batch Learning

```{r batch-poisson-SG-2, dependson="pois-gradient", echo=-1, cache = TRUE}
set.seed(17102020)
n <- 500 # Larger sample size
z <- runif(n, -1, 1)
y <- rpois(n, mu(z, beta_true))
pois_SG_tracer_2 <- tracer("par", Delta = 0)
rate <- decay_scheduler(t0 = 0.02, t1 = 0.001, n1 = 100)
sgd(c(0, 0), grad_pois, n = n, t = rate, cb = pois_SG_tracer_2$tracer)
```

. . .

### Test

```{r pois-beta-hat-2, dependson="batch-poisson-SG-2", echo=1}
beta_hat_2 <- coefficients(glm(y ~ z, family = poisson))
cat("beta_hat_2:", beta_hat_2)
```

##

```{r batch-pois-sgd-fig, echo=FALSE, dependson="online_poisson_SG", warning=FALSE, fig.height=3, fig.width=5.1, fig.cap = "Convergence for the Poisson regression problem using different batch sizes."}
hline_data <- data.frame(
  beta0 = c(beta_hat[1], beta_hat_2[1]),
  beta1 = c(beta_hat[2], beta_hat_2[2]),
  n = c("n = 50", "n = 500")
)

dplyr::bind_rows(
  small = summary(pois_SG_tracer)[seq(10, 1000, 10), ],
  large = summary(pois_SG_tracer_2),
  .id = "n"
) %>%
  dplyr::mutate(
    n = ifelse(n == "small", "n = 50", "n = 500"),
    it = rep(5 * seq(10, 1000, 10), 2)
  ) %>%
  dplyr::rename(beta0 = par.1, beta1 = par.2) %>%
  tidyr::pivot_longer(cols = c("beta0", "beta1"), names_to = "Parameter") %>%
  ggplot(aes(it, value, color = Parameter)) +
  geom_hline(data = hline_data, aes(yintercept = beta0), color = "blue") +
  geom_hline(data = hline_data, aes(yintercept = beta1), color = "red") +
  geom_hline(yintercept = beta_true[1], linetype = 2, alpha = 0.5) +
  geom_hline(yintercept = beta_true[2], linetype = 2, alpha = 0.5) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  ylim(c(1.5, 3.5)) +
  ylab("Parameter value") +
  facet_wrap("n") +
  scale_color_manual(values = c("blue", "red"))
```

## A Linear Model

```{r ls-model, cache = TRUE}
ls_model <- function(X, y) {
  n <- length(y)
  X <- unname(X) # Strips X of names
  list(
    # Initial parameter value
    par0 = rep(0, ncol(X)),
    # Objective function
    f = function(beta) {
      norm(y - X %*% beta, "2")^2 / (2 * n)
    },
    # Gradient in a single observation
    grad = function(beta, i) {
      xi <- X[i, ]
      xi * drop(xi %*% beta - y[i])
    }
  )
}
```

## Data Example

```{r news-data, message=FALSE, cache = TRUE}
news <- readr::read_csv(
  here::here("data", "online_news_popularity.csv")
)
dim(news)
```

##

```{r news-X-y, dependson="news-data", cache = TRUE}
news <- dplyr::select(
  news,
  -url,
  -timedelta,
  -is_weekend,
  -n_non_stop_words,
  -n_non_stop_unique_tokens,
  -self_reference_max_shares,
  -kw_min_max
)
# The model matrix without an explicit intercept is constructed using
# all variables remaining in the data set but the target variable 'shares'
X <- model.matrix(shares ~ . - 1, data = news)
y <- log(news$shares)
```

## Implementation

```{r LS-model, dependson=c("ls-model", "news-X-y")}
X_raw <- X
# Standardization
X <- scale(X, center = FALSE)
library(zeallot)
# The '%<-%' unpacking operator is from the zeallot package
c(par0, f, grad_obs) %<-% ls_model(X, y)
```

. . .

For the linear model it's straightforward to compute the solution

```{r lm-News, dependson="news-X-y"}
system.time(lm_news <- lm.fit(X, y))
par_hat <- coefficients(lm_news)
```

## Objective and Gradient Values

```{r}
f(par_hat)
```

. . .

```{r}
range(grad_obs(rep(0, ncol(X)), 100))
range(grad_obs(par_hat, 100))
```

## Running the SGD Algorithm

```{r SG-tracer, cache = TRUE, results = "hide"}
sgd_tracer <- tracer(
  "value",
  expr = quote(value <- f(par)),
  Delta = 0
)

sgd(
  par0,
  grad = grad_obs,
  n = nrow(X),
  t = 1e-5,
  maxiter = 50,
  cb = sgd_tracer$tracer
)

sgd_trace_low <- summary(sgd_tracer)
```

##

:::: {.columns}

::: {.column width="47%"}

```{r SG-tracer-sum, dependson="SG-tracer"}
tail(sgd_trace_low)
```

:::

::: {.column width="47%"}

```{r SG-tracer-parhat, dependson="SG-tracer"}
f(par_hat)
```

:::

::::

## Three Additional Variants

```{r GD, echo = FALSE}
gd <- function(
  par,
  f,
  gr,
  d = 0.8,
  c = 0.1,
  t0 = 0.01,
  epsilon = 1e-4,
  maxiter = 1000,
  cb = NULL
) {
  t <- t0
  for (i in 1:maxiter) {
    value <- f(par)
    grad <- gr(par)
    h_prime <- sum(grad^2)
    if (!is.null(cb)) {
      cb()
    }
    # Convergence criterion based on gradient norm
    if (h_prime <= epsilon) {
      break
    }
    # Proposed descent step
    par1 <- par - t * grad
    # Backtracking while descent is insufficient
    while (f(par1) > value - c * t * h_prime) {
      t <- d * t
      par1 <- par - t * grad
    }
    par <- par1
  }
  if (i == maxiter) {
    warning("Maximal number, ", maxiter, ", of iterations reached")
  }
  par
}
```

### Fixed but higher learning rate

```{r SG-tracer-2, results="hide"}
sgd_tracer$clear()
sgd(
  par0,
  grad_obs,
  n = nrow(X),
  t = 5e-5,
  maxiter = 50,
  cb = sgd_tracer$tracer
)
sgd_trace_high <- summary(sgd_tracer)
```

## Decay Schedule

```{r SG-tracer-3, results="hide"}
sgd_tracer$clear()
rate <- decay_scheduler(
  t0 = 1e-3,
  t1 = 1e-5,
  a = 0.6,
  n1 = 50
)
sgd(
  par0,
  grad_obs,
  n = nrow(X),
  t = rate,
  maxiter = 50,
  cb = sgd_tracer$tracer
)
sgd_trace_decay <- summary(sgd_tracer)
```

## Full-Batch Gradient Descent with Line Search

```{r News-GD, warning=FALSE, results='hide', dependson="GD"}
grad_f <- function(beta) crossprod(X, X %*% beta - y) / nrow(X)
gd_tracer <- tracer("value", Delta = 10)
gd(par0, f, grad_f, t = 1, maxiter = 800, cb = gd_tracer$tracer)
gd_trace <- summary(gd_tracer)
```

##

```{r news-trace-plot, echo=FALSE, dependson=c("News-GD", "SG-tracer-2", "SG-tracer-sum"), fig.width=5.2, fig.height=3, fig.cap = "A comparison of the four algorithms"}
dplyr::bind_rows(
  low = sgd_trace_low,
  high = sgd_trace_high,
  decay = sgd_trace_decay,
  gd = gd_trace[seq(1, 800, 20), ],
  .id = "Algorithm"
) |>
  dplyr::filter(.time < 10) |>
  autoplot(y = value - f(par_hat)) +
  aes(color = Algorithm, shape = Algorithm) +
  geom_line() +
  scale_color_brewer(
    limits = c(
      "low",
      "high",
      "decay",
      "gd",
      "mini",
      "moment",
      "adam",
      "adam_decay"
    ),
    breaks = c("low", "high", "decay", "gd"),
    type = "qual",
    palette = 2
  ) +
  scale_shape_manual(
    limits = c(
      "low",
      "high",
      "decay",
      "gd",
      "mini",
      "moment",
      "adam",
      "adam_decay"
    ),
    breaks = c("low", "high", "decay", "gd"),
    values = c(0, 1, 2, 5, 15, 16, 17, 19)
  )
```

## Exercise

We will attempt to minimize the following function:^[This is actually ridge
regression with a single predictor.]
$$
F(\beta; x, y, \lambda) = \sum_{i=1}^n f_i(\beta; x, y, \lambda) = \frac{1}{2} \sum_{i=1}^n (y_i - x_i \beta)^2 + \frac{\lambda}{2} \beta^2
$$

. . .

```{r}
#| echo: false
#| include: false
f <- function(beta, xx, yy, lambda) {
  sum(0.5 * (xx * beta - yy)^2 + 0.5 * lambda * beta^2)
}

f_grad <- function(beta, i, xx, yy, lambda) {
  (xx[i] * beta - yy[i]) * xx[i] + 2 * lambda * beta
}

beta <- 3

x <- rnorm(100)
y <- x * beta + rnorm(100)

beta_grid <- seq(-2, 5, length.out = 200)

f_vec <- Vectorize(f, vectorize.args = "beta")

plot(beta_grid, f_vec(beta_grid, x, y, 0.6), type = "l")
```

### Step 1

Implement a stochastic gradient descent method for solving this problem. Plot convergence
in terms of epochs and time. 

. . .

### Step 2

Test and benchmark your implementation against R's built-in `optimize()`.

. . .

### Step 3

Try different learning rates and, if you have time, implement mini-batch SGD.

## Summary

 Stochastic gradient descent is a popular optimization method for large-scale
optimization

\medskip\pause

Convergence results are relatively weak, but for many applications this does
not matter.

\medskip\pause

In practice, most implementations use mini-batches.

. . .

## Next Time

### Rcpp

We will look at how to use Rcpp to speed up R code.

\medskip\pause

This will be very useful for implementing optimization algorithms later on.
