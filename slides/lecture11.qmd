---
title: "Stochastic Gradient Descent"
execute:
  cache: false
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(mvtnorm)

set.seed(1422)

source(here::here("R", "sgd.R"))
```

## Last Time

### The Expectation-Maximization (EM) Algorithm

A general approach to maximum likelihood estimation with latent variables.

. . .

### Fisher Information

We saw three different ways to define the Fisher information
and how they relate to the EM algorithm.

## Recap: The EM Algorithm

The EM algorithm iteratively optimizes
$$
\theta^{(t+1)} = \argmax_\theta Q(\theta \mid \theta^{(t)}),
$$
with
$$
Q(\theta \mid \theta^{(t)}) = \E_{Z \mid X,\theta^{(t)}}\big(\log p(X, Z \mid \theta)\big).
$$

\medskip\pause

Convergence is **linear** near a local maximum.

## Recap: Fisher Information

The Fisher information is defined as
$$
i(\theta) = - \E\left( \frac{\partial^2}{\partial \theta^2} \log p(X \mid \theta) \right).
$$

\medskip\pause

We mentioned three ways to compute it:

1. Differentiate the score function (defined through $Q$).\pause
2. Compute complete and missing information and using the information
   identity. \pause
3. Differentiate the EM mapping.

## Today

### Gaussian Mixture EM Example

We pick up where we left off last time,
focusing on the Gaussian mixture example.

. . .

### Stochastic Gradient Descent

Useful stochastic method for optimization

\medskip\pause

Can be used in a **mini-batch** version.

# EM Gaussian Mixture Example

## A Mixture of Two Gaussians

### Goal

Fit a two-component Gaussian mixture model to data.

## Finite Mixtures

In a finite mixture, we assume that the marginal density of $X$ is
$$
p(x \mid \psi) = \sum_{k=1}^K p_k f_k(x \mid \psi_k)
$$
where $p_k \geq 0$, $\sum_k p_k = 1$, and $f_k$ are densities.

\medskip\pause

### Latent Variable

Assume each $X_i$ comes from one of the $K$ components,
but we don't know which one.

\medskip\pause

We introduce a latent variable $Z_i \in \{1, \ldots, K\}$ such that
$$
\Pr(Z_i = k) = \pi_k
$$

. . .

### Complete-Data Model

Then the complete data model is
$$
p(x_i, z_i \mid \psi) = \pi_{z_i} f_{z_i}(x_i \mid \psi_{z_i}).
$$

## Gaussian Mixture

For Gaussian mixtures, the components are Gaussian:
$$
f_k(x \mid \mu_k, \sigma_k) = \frac{1}{\sqrt{2 \pi \sigma_k^2}} e^{-\frac{(x - \mu_k)^2}{2 \sigma_k^2}}.
$$
with $\psi_k = (\mu_k, \sigma_k^2)$.


\medskip\pause

### Complete-Data Log-Likelihood

Letting $\theta = (\pi_1, \ldots, \pi_K, \psi_1, \ldots, \psi_K)$,
the complete-data log-likelihood is
$$
\begin{aligned}
\ell_c (\theta) &= \sum_{i=1}^n \log p(x_i, z_i \mid \theta) \\ \pause
                &= \sum_{i=1}^n \log \left( f(x_i \mid z_i, \theta) f(z_i \mid \theta) \right) \\ \pause
                &= \sum_i \sum_k \symbf{1}(Z_i = k) \left( \log \pi_k + \log f_k(x_i \mid \mu_k, \sigma_k^2) \right) 
\end{aligned}
$$

## Simple Case: Fixed Variance Components, $K=2$

:::: {.columns align="center"}

::: {.column width="47%"}

Let's simplify and assume $K=2$ and that $\sigma_1$ and $\sigma_2$ are known.

\medskip\pause

Then the model has three unknown parameters:
$$
\theta = (\pi, \mu_1, \mu_2).
$$

:::

::: {.column width="47%"}

\begin{tikzpicture}[
    node distance=1.6cm,
    latent/.style={circle,draw,minimum size=10mm,inner sep=0pt},
    observed/.style={circle,draw,fill=gray!20,minimum size=10mm,inner sep=0pt},
    param/.style={rectangle,draw,minimum size=7mm,inner sep=2pt},
  ]

  % Parameters for components
  \node[param] (pi) {$\pi$};
  \node[param, right=2.5cm of pi, yshift=1cm] (mu1) {$\mu_1$};
  \node[param, right=2.5cm of pi, yshift=-1cm] (mu2) {$\mu_2$};

  % Latent and observed variables
  \node[latent, below=1.8cm of pi] (z) {$Z_i$};
  \node[observed, below=1.8cm of z] (x) {$X_i$};

  % Arrows
  \draw[->] (pi) -- (z);
  \draw[->] (z) -- (x);
  \draw[->] (mu1) -- (x);
  \draw[->] (mu2) -- (x);

  % Plate for data points
  \node[plate={below:{$i=1,\dots,n$}}, fit=(z)(x)] (plateN) {};
\end{tikzpicture}

:::

::::

## The E-Step

The complete-data log-likelihood is now
$$
\begin{aligned}
\ell_c(\theta) &= \sum_{i=1}^n \symbf{1}(Z_i = 1) \left( \log(\pi) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) \\
               &\phantom{= {}}+ \symbf{1}(Z_i = 2)\left( \log(1-\pi) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right).
\end{aligned}
$$

. . .

Taking expectations gives
$$
Q(\theta \mid \theta') = \sum_{i=1}^n \tau_{i} \left(\log(\pi) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \tau_{i})\left( \log(1-\pi) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)
$$
where $\tau_i = \Pr(Z_i = 1 \mid X_i = x_i, \theta')$ is the
**responsibility** of component 1 for observation $i$.

\medskip\pause

The responsibility tells us how "responsible" component 1 is for generating $x_i$.

## The Responsibilities

The conditional probability in a mixture model is generally
$$
P(Z = z \mid X = x) = \frac{p_z f_z(x \mid \psi_z)}{\sum_{k = 1}^K p_k f_k(x \mid \psi_k)}.
$$

\medskip\pause

For the $K = 2$ Gaussian case, this gives us
$$
\tau_i = \Pr(Z_i = 1 \mid X = x_i, \theta') =
\frac{ \pi'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( \pi'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - \pi'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }.
$$

## The Q-Step

The maximum is attained at
$$
\hat{\theta} = 
\left(\frac{1}{n} \sum_{i} \tau_i, \quad \frac{1}{\sum_{i} \tau_i} \sum_{i} \tau_i x_i, \quad \frac{1}{\sum_{i} (1 - \tau_i)} \sum_{i} (1 - \tau_i) x_i \right).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

### Updating $\pi$

We update $\pi$ as
$$
\pi' = \frac{1}{n} \sum_{i=1}^n \tau_i, 
$$
which is the fraction of all data points "assigned" to component 1.

:::

. . .

::: {.column width="47%"}

### Updating $\mu_1$ and $\mu_2$

We update $\mu_1$ as 
$$
\mu_1' = \frac{\sum_{i=1}^n \tau_i x_i}{\sum_{i=1}^n \tau_i},
$$
and similarly for $\mu_2$.

\medskip\pause

This is a weighted mean, weighted by how much each component is responsible for
each observation.

:::

::::

## Example: Simulation

We start by setting the parameters.


```{r mixture-gaus-sim}
sigma1 <- 1
sigma2 <- 2
mu1 <- -0.5
mu2 <- 4
p <- 0.5
```

. . .

Then we simulate data.

```{r}
n <- 5000
z <- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p))
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

##

```{r, echo=FALSE}
#| fig-cap: "Histogram of simulated data from a two-component Gaussian mixture model."
#| fig-width: 4
#| fig-height: 2.7
gausdens <- function(x, mu1, mu2, sigma1, sigma2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

xx <- seq(-3, 11, 0.1)

pl <- ggplot(data.frame(x), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, fill = "lightgray") +
  geom_line(
    aes(x, gausdens(x, mu1, mu2, sigma1, sigma2)),
    color = "black",
    linewidth = 1
  ) +
  labs(y = "Density", x = "x")

pl
```

```{r gaus-mix-loglik}
#| echo: false
neg_loglik <- function(par, x) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]

  if (p < 0 || p > 1) {
    return(Inf)
  }

  -sum(log(
    p *
      exp(-(x - mu1)^2 / (2 * sigma1^2)) /
      sigma1 +
      (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  ))
}
```

## Optimizing the Log-Likelihood

For this simple example, we can also optimize the log-likelihood directly.

## General-Purpose Optimization

```{r gaus-mix-example, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim_par <- optim(c(0.5, -0.5, 4), neg_loglik, x = x)
optim_par[1:2]
```

. . .

But the starting value matters!

```{r gaus-mix-example-bad, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
optim(c(0.9, 3, 1), neg_loglik, x = x)[1:2]
```


## The EM Algorithm

```{r em-alg} 
em <- function(par, em_step, eps = 1e-6, maxit = 20, cb = NULL) {
  for (i in seq_len(maxit)) {
    par0 <- par
    par <- em_step(par)

    if (!is.null(cb)) {
      cb() # Callback function (tracer)
    }

    if (sum((par - par0)^2) <= eps * (sum(par^2) + eps)) {
      break
    }
  }
  par
}
```

## Implementation

Next, we implement the E and M steps. The 
implementations are not shown here, but can be found in the [source 
code](./slides/lecture11.qmd)


```{r source-em-mix}
#| echo: false
library(numDeriv)

e_step_gauss <- function(par, x, sigma1 = 1, sigma2 = sigma1) {
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  a <- p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1
  b <- (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2
  a / (a + b)
}

m_step_gauss <- function(p_hat, x) {
  n <- length(x)
  N1 <- sum(p_hat)
  N2 <- n - N1
  c(N1 / n, sum(p_hat * x) / N1, sum((1 - p_hat) * x) / N2)
}

create_em_gauss_step <- function(x, sigma1 = 1, sigma2 = sigma1) {
  force(x)
  force(sigma1)
  force(sigma2)

  function(par) m_step_gauss(e_step_gauss(par, x, sigma1, sigma2), x)
}

Q <- function(par, par_prime) {
  p_hat <- e_step_gauss(par_prime, x, sigma1, sigma2)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    p_hat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - p_hat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}

grad1 <- function(par) grad(function(par) neg_loglik(par, x), par)
grad2 <- function(par) -grad(Q, par, par_prime = par)

em_gauss_step <- create_em_gauss_step(x, sigma1, sigma2)
```

## Testing

```{r test-em-gauss-mix}
em_par <- em(c(0.5, -0.5, 4), em_gauss_step)
```

. . .

```{r gaus-mix-example2, dependson=c("gaus-mix-loglik", "gaus-mix-sim")}
testthat::test_that("EM and optim give similar results", {
  testthat::expect_equal(em_par, optim_par$par, tolerance = 1e-3)
})
```

. . .

```{r run-em-gauss-mix-again}
em(c(0.9, 3, 1), em_gauss_step) # Starting value still matters
```

## Tracing Convergence

The `tracer` object from the **CSwR** package lets us evaluate
arbitrary expressions at each iteration of the EM algorithm via `expr`.

```{r tracing-convergence}
library(CSwR)
em_tracer <- tracer(
  c("par0", "par", "nll"),
  Delta = 0, # No printing
  expr = quote({
    nll <- neg_loglik(par, x)
  })
)
```

. . .

```{r}
par_hat <- em(c(0.2, 2, 2), em_gauss_step, cb = em_tracer$tracer)

em_trace <- summary(em_tracer)
```

##

:::: {.columns}

::: {.column width="47%"}

**CSwR** contains a `autoplot` S3 method for `trace` objects. 

. . .

```{r gaussmix-convergence-plot}
#| eval: false
autoplot(
  em_trace,
  y = nll - min(nll)
)
```

:::

. . .

::: {.column width="47%"}

```{r convergence-plots-2, warning=FALSE}
#| echo: false
#| ref-label: gaussmix-convergence-plot
#| fig-cap: "Convergence of the EM algorithm for Gaussian mixture model."
```

:::

::::

##

```{r mixture-convergence}
#| echo: false
gaussdens2 <- function(x, p, mu1, mu2) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}

em_gauss_illustration <- function(i, em_trace) {
  p_i <- em_trace$par0.1[i]
  mu1_i <- em_trace$par0.2[i]
  mu2_i <- em_trace$par0.3[i]

  pl +
    geom_line(
      aes(y = gaussdens2(x, p_i, mu1_i, mu2_i)),
      color = "darkorange",
      size = 1
    ) +
    lims(y = c(0, 0.25)) +
    labs(title = paste0("k = ", i))
}
```

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(1, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(4, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(8, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(12, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(16, em_trace)
```

##

```{r}
#| fig-width: 4.2
#| fig-height: 2.8
#| echo: false
#| fig-cap: "Iterations of the EM algorithm"
em_gauss_illustration(20, em_trace)
```

## Benchmarking

You might be inclined to wonder which is faster: EM or `optim()`? 

\medskip\pause

```{r benchmark-em-gauss-mix}
gauss_bm <- bench::mark(
  em = em(c(0.5, -0.5, 4), em_gauss_step),
  optim = optim(c(0.5, -0.5, 4), neg_loglik, x = x),
  check = FALSE
)
```

##

```{r}
#| fig-cap: "Benchmarking results for EM vs. optim() for Gaussian mixture model."
#| fig-width: 4.5
plot(gauss_bm)
```

\medskip\pause

But this comparison is flawed: **why?**


## Label Switching

If we set $\sigma_1 = \sigma_2$, then the likelihood is invariant to
switching the labels of the components, i.e., $(p, \mu_1, \mu_2)$ and
$(1-p, \mu_2, \mu_1)$ give the same likelihood.

\medskip\pause

So the EM algorithm may converge to
**either** of the two symmetric modes.

```{r mixture-gaus-sim2}
#| echo: false
sigma1 <- 1
sigma2 <- 1
mu1 <- -0.5
mu2 <- 4
p <- 0.8
n <- 5000
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)

em_gauss_step_alt <- create_em_gauss_step(x, sigma1)
```

. . .

```{r}
em(c(0.2, 3, -0.5), em_gauss_step_alt)
```

. . .

```{r}
em(c(0.8, -3, -0.5), em_gauss_step_alt)
```

. . .

In this example, we generated data with $p = 0.8$ and $\sigma_1 = \sigma_2$.

# Stochastic Gradient Descent

## Minimizing Sums

Many of the functions we are trying to minimize are of the form
$$
\frac{1}{n} \sum_{i=1}^n f_i(x).
$$

. . .

:::: {.columns}

::: {.column width="47%"}

### Gradient Descent

Since $\nabla \left(\sum_{i=1}^n f_i(x)\right) = \sum_{i=1}^n \nabla f_i(x)$, GD
steps are
$$
x_{k+1} = x_k - t \sum_{i=1}^n \nabla f_i(x_k).
$$

:::

. . .

::: {.column width="47%"}

### Stochastic Gradient Descent

SGD instead takes steps
$$
x_{k+1} = x_k - t \nabla f_{i}(x_k)
$$
where $i$ is an index in $\{1, \ldots, n\}$.

:::

::::

## Example: Ordinary Least Squares

The loss function is
$$
f(\beta) = \frac{1}{2n} \sum_{i=1}^n (y_i - x_i^T \beta)^2.
$$
so
$$
f_i(\beta) = \frac{1}{2} (y_i - x_i^T \beta)^2.
$$

And the gradient is
$$
\nabla f_i(\beta) = - x_i (y_i - x_i^T \beta).
$$

## Basic Implementation of SGD

```{r}
sgd_basic <- function(
  par,
  grad,
  n, # number of data points
  t, # step size
  sampler = sample, # function that samples indices
  maxit = 100,
  ...
) {
  for (k in seq_len(maxit)) {
    for (i in seq_len(n)) {
      ind <- sampler(n)
      par <- par - t * grad(par, ind, ...)
    }
  }
  par
}
```

## Unbiasedness

If $i$ is drawn uniformly at random from $\{1, \ldots, n\}$ then
$$
\operatorname{E}(\nabla f_i(x)) = \nabla f(x).
$$

So SGD gradients are **unbiased** estimates of the full gradient.

. . .

### Iteration Cost

The main benefit of this is that the cost of each iteration is much lower:
$O(p)$ vs. $O(np)$ (for full gradient descent).

. . .

### Epoch

When discussing SGD, we often refer to an **epoch** as $n$ iterations of SGD.

\medskip\pause

A full pass over the data.

## Example: Logistic Regression

:::: {.columns}

::: {.column width="47%"}

We have
$$
f_i(\beta) = \log(1 + e^{-y_i x^T_i \beta})
$$
and
$$
\nabla f_i(\beta) = - \frac{y_i x_i}{1 + e^{y_i x^T_i \beta}}.
$$

. . .

SGD typically converges quickly at the start but slows down as it approaches
the minimum.

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-height: 3.6
#| fig-width: 3.2
set.seed(123)

n <- 1000
p <- 2
Sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2)
mu <- c(0, 0)

X <- rmvnorm(n, mean = mu, sigma = Sigma)
beta <- c(1, 2)
y <- rbinom(n, 1, plogis(X %*% beta))

res_glm <- glm.fit(X, y, family = binomial())

loss_2d <- function(beta1, beta2, X, y) {
  beta <- c(beta1, beta2)
  z <- X %*% beta
  p_hat <- 1 / (1 + exp(-z))
  -mean(y * log(p_hat) + (1 - y) * log(1 - p_hat))
}

beta1 <- seq(-1, 3, length = 100)
beta2 <- seq(-1, 3, length = 100)

loss_2d_vectorized <- Vectorize(function(b1, b2) loss_2d(b1, b2, X, y))

z <- outer(beta1, beta2, loss_2d_vectorized)

res_sgd <- logreg_sgd(X, y, max_epochs = 100, batch_size = 1)
res_sgd2 <- logreg_sgd(X, y, max_epochs = 100, batch_size = 1, K = 5, a = 0.5)
res_gd <- logreg_sgd(X, y, max_epochs = 100, batch_size = n)

loss_optim <- loss_2d(res_glm$coefficients[1], res_glm$coefficients[2], X, y)

res_gd <- logreg_sgd(X, y, max_epochs = 100, batch_size = n)
loss1 <- loss_2d_vectorized(res_gd$beta_history[1, ], res_gd$beta_history[2, ])
loss2 <- loss_2d_vectorized(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ]
)
loss3 <- loss_2d_vectorized(
  res_sgd2$beta_history[1, ],
  res_sgd2$beta_history[2, ]
)

n1 <- length(loss1)
n2 <- length(loss2)
n3 <- length(loss3)


pal <- palette.colors(palette = "Okabe-Ito")

contour(
  beta1,
  beta2,
  z,
  col = "dark grey",
  drawlabels = FALSE,
  asp = 1
)

lines(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ],
  col = pal[3]
)
points(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ],
  col = pal[3],
  pch = 19,
  cex = 0.5
)

lines(
  res_gd$beta_history[1, ],
  res_gd$beta_history[2, ],
  col = pal[1]
)
points(
  res_gd$beta_history[1, ],
  res_gd$beta_history[2, ],
  col = pal[1],
  pch = 19,
  cex = 0.5
)

points(
  res_glm$coefficients[1],
  res_glm$coefficients[2],
  col = pal[2],
  pch = 19,
  cex = 1
)

legend(
  "bottomright",
  legend = c("SGD", "GD"),
  col = pal[c(3, 1)],
  pch = 19,
  cex = 0.8
)
```


:::

::::


## Step Size (Learning Rate)

:::: {.columns align = "center"}

::: {.column width="47%"}

For gradient descent, we could use a fixed step size (learning rate), and
guarantee convergence.

\medskip\pause

But this is **not** the case for SGD.

:::

::: {.column width="47%"}

```{r}
#| fig-cap: "Convergence of SGD for logistic regression with a fixed step size."
#| echo: false
res_sgd <- logreg_sgd(X, y, max_epochs = 200, batch_size = 1, gamma0 = 5)
pal <- palette.colors(palette = "Okabe-Ito")

loss <- loss_2d_vectorized(
  res_sgd$beta_history[1, ],
  res_sgd$beta_history[2, ]
)

ggplot(
  tibble(iteration = seq_along(loss), loss = loss - loss_optim),
  aes(iteration, loss)
) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality")
```

:::

::::

\medskip\pause

Can we find conditions on the step sizes $t_k$ that ensure convergence of SGD? **Yes!**

\medskip

But first, we need to introduce the concept of **strong convexity**.


## Strong Convexity

A differentiable function $f: \mathbb{R}^p \to \mathbb{R}$ is **strongly convex** if
$$
f(x_0) \geq f(x) + \nabla f(x)^T (x_0 - x) + \frac{\mu}{2} \lVert x_0 - x\rVert_2^2.
$$

\medskip\pause

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=7cm, height=4.3cm,
        axis lines=none,
        xlabel={$x$},
        ylabel={$f(x)$},
        ymin=-0.5, ymax=4,
        xmin=-1, xmax=2.2,
        samples=200,
        domain=-0.8:2.2,
        clip=false
      ]
      % f(x) = x^2
      \addplot[thick] {x^2};
      % Tangent at x0=1: 2*(x-1) + 1
      \addplot[DarkOrange, dashed, thick] {2*(x-1) + 1};
      % Strong convexity lower bound: tangent + 0.5*(x-1)^2
      \addplot[RoyalBlue, dotted, thick] {2*(x-1) + 1 + 0.5*(x-1)^2};
      % Point at (1,1)
      \addplot[only marks, mark=*] coordinates {(1,1)};
      % Annotations
      \node[anchor=west] at (axis cs:2.2,5) {$f(x)$};
      \node[anchor=west, RoyalBlue] at (axis cs:2.2,4.3) {$f(x) + \nabla f(x)^\intercal (x_0-x) + \frac{\mu}{2}\lVert x_0-x \rVert^2$};
      \node[anchor=west, DarkOrange] at (axis cs:2.2,3.5) {$f(x) + \nabla f(x)^\intercal (x_0-x)$};
    \end{axis}
  \end{tikzpicture}
\end{figure}

## Example: Strong Convexity

The function 
$$
f(x) = x^2
$$\pause
is strongly convex with $\mu = 2$ since
$$
\begin{aligned}
f(x_0) - f(x) - \nabla f(x)^T (x_0 - x) & = x_0^2 - x^2 - 2x(x_0 - x) \\ \pause
                                        & = (x_0 - x)^2 \\ \pause
                                        & \geq \frac{\mu}{2}(x_0 - x)^2.
\end{aligned}
$$ 

\medskip\pause

The inequality holds for $\mu \leq 2$, and in particular for $\mu = 2$.

## Relationship to Lipschitz Gradient

If $f$ is strongly convex with parameter $\mu$ and has Lipschitz gradient with
constant $L$, then
$$
\mu I \preceq \nabla^2 f(x) \preceq L I.
$$

\medskip\pause

- Strong convexity provides a **lower** bound on the curvature of $f$\pause
- Lipschitz gradient provides an **upper** bound on the curvature of $f$

. . .


### Example

The function $f(x) = x^2$ is strongly convex with $\mu = 2$ and has Lipschitz
gradient with $L = 2$, since $\nabla^2 f(x) = 2$.

\medskip\pause

This is *identical* to the strong convexity parameter.

## More Complicated Example

:::: {.columns}

::: {.column width="47%"}

Let's take the function
$$
f(x) = x^2 + \sin(x).
$$

\medskip\pause

This is twice differentiable with
$$
\nabla^2 f(x) = 2 - \sin(x).
$$

\medskip\pause

And since
$$
1 \leq 2 - \sin(x) \leq 3,
$$
$f$ is strongly convex with $\mu = 1$ and has Lipschitz gradient with $L = 3$.

\medskip\pause

:::

::: {.column width="47%"}

\begin{figure}
  \begin{tikzpicture}
    \pgfmathsetmacro{\a}{1}
    \pgfmathsetmacro{\xzero}{4.712389} % 3*pi/2
    \pgfmathsetmacro{\mu}{2 - \a}      % =1
    \pgfmathsetmacro{\Lips}{2 + \a}    % =3
    \pgfmathsetmacro{\fz}{\xzero*\xzero + sin(deg(\xzero))}
    \pgfmathsetmacro{\dfz}{2*\xzero + cos(deg(\xzero))}
    \begin{axis}[
        width=5cm,
        height=6.2cm,
        domain=-0.5:12.5,
        clip=false,
        samples=200,
        axis lines=none,
        xlabel={$x$},
        ylabel={$f(x)$},
        legend style={draw=none,at={(0.02,0.98)},anchor=north west}
      ]
      % f
      \addplot[thick, line label={$f(x)$}] {x^2 + sin(deg(x))};
      % lower quadratic
      \addplot[
        DarkOrange,
        dashed,
        thick,
        line label={Strong convexity bound},
      ] {
        \fz + \dfz*(x-\xzero) + 0.5*\mu*(x-\xzero)^2
      };
      % upper quadratic
      \addplot[
        RoyalBlue,
        dashed,
        thick,
        line label={Lipschitz bound}
      ] {
        \fz + \dfz*(x-\xzero) + 0.5*\Lips*(x-\xzero)^2
      };
      % point
      \addplot[only marks,mark=*] coordinates {(\xzero,\fz)};
    \end{axis}
  \end{tikzpicture}
  \caption{The function $f(x) = x^2 + \sin(x)$ is strongly convex with $\mu = 1$
    and has Lipschitz gradient with $L = 3$.}
\end{figure}

:::

::::


## Conditioning

The **condition number** of a strongly convex function with Lipschitz gradient
is
$$
\kappa = \frac{L}{\mu} \geq 1.
$$

. . .

The closer $\kappa$ is to $1$, the better conditioned the problem is.

### Example

For $f(x) = x^2$, we have $\kappa = 2/2 = 1$, so the problem is perfectly
conditioned.

\medskip\pause

We reach the minimum in one step with gradient descent and step size $t = 1/L = 1/2$.

## The Robbins--Monro Convergence Theorem

Suppose $f$ is strongly convex and
$$
\operatorname{E}\big(\rVert \nabla f(x)\lVert_2^2\big) \leq A + B \lVert x\rVert_2^2
$$
for some constants $A$ and $B$. If $x^*$ is the global minimizer of $f$
then $x_n$ converges almost surely toward $x^*$ if
$$
\sum_{k=1}^{\infty} t_k^2 < \infty \quad \text{and} \quad
\sum_{k=1}^{\infty} t_k = \infty.
$$

. . .

The theorem applies to learning rates satisfying
$$
t_k \propto \frac{1}{k}.
$$

## Diminishing Step Sizes

Assume cyclic rule and set $t_k = t$ for $n$ updates. We get
$$
x_{k + n} = x_k - t \sum_{i=1}^n \nabla f_i(x_{k + i - 1}).
$$

. . .

Meanwhile, **full** gradient descent with step size $nt$ would give
$$
x_{k + 1} = x_k - t \sum_{i=1}^n \nabla f_i(x_k).
$$

. . .

Difference between the two is
$$
t \sum_{i=1}^n \left( \nabla f_i(x_{k + i - 1}) - \nabla f_i(x_k) \right)
$$
which does not tend to zero if $t$ is constant.

\pdfpcnote{
  What am I trying to say here?
}

## convergence rates

### gradient descent

for convex $f$, gd with diminishing step sizes converges at rate
$$
f(x_n) - f^* = o\left(\frac{1}{\sqrt{k}}\right). \quad \text{(sublinear)}
$$

. . .

when $f$ is differentiable with lipschitz gradient, we get
$$
f(x_n) - f^* = o\left(\frac{1}{k}\right). \quad \text{(linear)}
$$

. . .

### stochastic gradient descent

for convex $f$ with diminishing step sizes, sgd converges at rate
$$
\operatorname{e}f(x_n) - f^* = o\left(\frac{1}{\sqrt{k}}\right).
$$

. . .

but this **does not improve** when $f$ is differentiable with lipschitz
gradient.

## rates for strongly convex functions

when $f$ is strongly convex and has lipschitz gradient, gd satisfies
$$
f(x_n) - f^* = o(\delta^k), \qquad \delta \in (0, 1).
$$

. . .

under same conditions, SGD gives us
$$
\operatorname{e}(f(x_n) - f^*) = o(1/k).
$$

. . .

so SGD does **not** enjoy linear convergence rates under strong convexity.

## illustration of convergence rates

\begin{tikzpicture}
  \begin{axis}[
      width=7cm, height=7cm,
      xlabel={$k$},
      ylabel={error},
      legend pos=north east,
      ymode=log,
      xmax=40,
      domain=1:30
    ]
    % o(1/sqrt(k))
    \addplot[
      blue,
      thick,
      line label={$o(1/\sqrt{k})$}
    ] {1/sqrt(x)};

    % o(1/k)
    \addplot[
      red,
      thick,
      dashed,
      line label={$o(1/k)$}
    ] {1/x};

    % o(delta^k) with delta = 0.5
    \addplot[
      green!60!black,
      thick,
      dotted,
      samples=50,
      line label={$o(\delta^k)$}
    ] {0.5^x};
  \end{axis}
\end{tikzpicture}

## Mini-Batch Gradient Descent

Take mini-batch $A_k$ of size $b$ of the data and iterate through
$$
x_{k+1} = x_k - \frac{t}{b} \sum_{i \in A_k} \nabla f_i(x_k).
$$

. . .

Estimates are still unbiased,
$$
\operatorname{E}\frac{1}{b}\sum_{i \in A_k}\nabla f_i(x) = \nabla f(x),
$$
but variance is reduced by a factor of
$1/b$.

\medskip\pause

Under Lipschitz gradient, rate goes from $O(1/\sqrt{k})$ to
$O(1/\sqrt{bk} + 1/k)$.

\medskip\pause

Typically more efficient than standard SGD for various computational reasons,
but somewhat **less** robust to local minima.

##

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 2.8
#| fig-cap: "Convergence of SGD on the logistic regression problem, with
#|   different batch sizes. Note that batch size 1000 is standard gradient
#|   descent."
set.seed(123)

n <- 1000
p <- 2
Sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2)
mu <- c(0, 0)

X <- rmvnorm(n, mean = mu, sigma = Sigma)
beta <- c(1, 2)
y <- rbinom(n, 1, plogis(X %*% beta))

res_glm <- glm.fit(X, y, family = binomial())
loss_optim <- loss_2d(res_glm$coefficients[1], res_glm$coefficients[2], X, y)

loss_2d <- function(beta1, beta2, X, y) {
  beta <- c(beta1, beta2)
  z <- X %*% beta
  p_hat <- 1 / (1 + exp(-z))
  -mean(y * log(p_hat) + (1 - y) * log(1 - p_hat))
}

conv_data <- data.frame(b = NULL, epoch = NULL, loss = NULL)

for (b in c(1, 10, 100, n)) {
  res_sgd <- logreg_sgd(
    X,
    y,
    max_epochs = 100,
    batch_size = b,
    a = 1,
    gamma0 = 3
  )

  conv_data <- rbind(
    conv_data,
    data.frame(b = b, epoch = seq_len(100), loss = res_sgd$loss - loss_optim)
  )
}

ggplot(conv_data, aes(epoch, loss, col = as.factor(b))) +
  geom_line() +
  scale_y_log10() +
  labs(color = "Batch size", y = "Suboptimality")
```


## So, Why Use SGD?

We have shown that convergence rates of SGD are worse than those of GD. 

\medskip\pause

So why even care about SGD?

### Reasons to Use SGD

- If $n \gg p$, then we can pick a batch size that is reasonably accurate but
  still much smaller than $n$. \pause
- In many applications (e.g. deep learning), we don't *care* about optimizing to
  high accuracy. \pause
- Sometimes, the stochasticity in SGD helps it escape local minima.

## Convergence in Epochs and Iterations

In terms of iterations, gradient descent converges much more quickly than SGD.
But in terms of epochs, SGD can be faster.

\medskip\pause

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| fig-cap: "Convergence of gradient descent (GD) and stochastic gradient descent (SGD) for logistic regression i
#|   terms of iterations and epochs."
#| echo: false
conv_data <- tibble(
  method = rep(c("GD", "SGD"), times = c(length(loss1), length(loss2))),
  epoch = c(seq_len(n1), seq_len(n2) / n),
  loss = c(loss1 - loss_optim, loss2 - loss_optim),
  iteration = c(seq_len(n1), seq_len(n2))
)
epochs_plot <- ggplot(conv_data, aes(epoch, loss, col = method)) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality", color = NULL)

iteration_plot <- ggplot(conv_data, aes(iteration, loss, col = method)) +
  geom_line() +
  scale_y_log10() +
  labs(y = "Suboptimality", color = NULL)

ggpl <- epochs_plot +
  iteration_plot +
  plot_layout(axes = "collect_y", guides = "collect")

ggpl
```

## 

```{r sgd-local-minima}
#| echo: false
#| fig-width: 5.2
#| fig-height: 3.2
#| fig-cap: "The stochasticity in SGD sometimes helps it escape local minima."
set.seed(1)

f <- function(x) x^4 - 2 * x^2 + 1 * x
df <- function(x) 4 * x^3 - 4 * x + 1

# Parameters
eta <- 0.07
steps <- 25
x_gd <- numeric(steps)
x_sgd <- numeric(steps)
x_gd[1] <- x_sgd[1] <- 1.4

for (k in 1:(steps - 1)) {
  # Exact gradient descent
  x_gd[k + 1] <- x_gd[k] - eta * df(x_gd[k])

  # Noisy gradient descent (SGD-like)
  noise <- rnorm(1, sd = 4) # stochasticity
  x_sgd[k + 1] <- x_sgd[k] - eta * (df(x_sgd[k]) + noise)
}


plot_curves <- function(main) {
  curve(
    f,
    from = -1.6,
    to = 1.7,
    n = 200,
    lwd = 2,
    col = "gray60",
    ylab = "",
    xlab = "",
    axes = TRUE,
    yaxt = if (main == "GD") "s" else "n",
    main = main
  )
}

plot_iteration <- function(i, x_gd, x_sgd) {
  x_gd_i <- x_gd[1:i]
  x_sgd_i <- x_sgd[1:i]

  opar <- par(no.readonly = TRUE)

  par(
    mfrow = c(1, 2),
    oma = c(4.1, 4.1, 0.1, 0.3),
    mai = c(0.1, 0.2, 0.5, 0.0)
  )

  plot_curves("GD")
  lines(x_gd_i, f(x_gd_i), col = "royalblue", lwd = 1)
  points(x_gd_i, f(x_gd_i), col = "royalblue", pch = 19, cex = 0.8)

  plot_curves("SGD")
  lines(x_sgd_i, f(x_sgd_i), col = "darkorange", lwd = 1)
  points(x_sgd_i, f(x_sgd_i), col = "darkorange", pch = 19, cex = 0.8)

  mtext("x", side = 1, outer = TRUE, line = 2)
  mtext("f(x)", side = 2, outer = TRUE, line = 2)

  par(opar)
}

plot_iteration(steps, x_gd, x_sgd)
```

## Learning Rate

The learning rate $t$ is crucial for SGD, but hard to set.

\medskip\pause

Convergence theorem only gives a few hints.

. . .

### A Class of Decay Schedules

$$t_k = \frac{t_0 K}{K + k^a} = \frac{t_0 }{1 + K^{-1} k^{a}}$$
with initial learning rate $t_0
> 0$ and constants $K, a >
0$.

\medskip\pause

Convergence is ensured by Robbins–Monro if $a \in (0.5, 1]$.

\medskip\pause

Fixing the exponent $a$ and picking a target rate, $t_1$, to be reached
after $k_1$ steps, we can solve for $K$ and get
$$K = \frac{k_1^a t_1}{t_0 - t_1}.$$

##

```{r decay, cache = TRUE}
decay_scheduler <- function(
  t0 = 1,
  a = 1,
  K = 1,
  t1 = NULL,
  n1 = NULL
) {
  force(a)

  if (!is.null(t1) && !is.null(n1)) {
    K <- n1^a * t1 / (t0 - t1)
  }

  b <- t0 * K

  function(n) b / (K + n^a)
}
```

##

```{r decay-fig, echo=FALSE, dependson="decay", fig.cap="Examples of learning rate schedules."}
#| fig-height: 3
#| fig-width: 5
grid_par <- expand.grid(
  n = seq(0, 1000, 2),
  K = c(25, 50, 100),
  a = c(0.6, 1),
  t0 = 1
)

grid_par <- dplyr::mutate(grid_par, rate = t0 * K / (K + n^a))

p1 <- ggplot(grid_par, aes(n, rate, color = factor(K), linetype = factor(a))) +
  geom_line() +
  scale_color_discrete("K") +
  scale_linetype_discrete("a") +
  guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2))

grid_par <- expand.grid(
  n = seq(0, 1000, 2),
  K = c(1e4, 1e5, 1e6),
  a = c(2, 2.5),
  t0 = 1
)

grid_par <- dplyr::mutate(grid_par, rate = t0 * K / (K + n^a))

p2 <- ggplot(grid_par, aes(n, rate, color = factor(K), linetype = factor(a))) +
  geom_line() +
  scale_color_discrete("K", labels = c(quote(10^4), quote(10^5), quote(10^6))) +
  scale_linetype_discrete("a") +
  guides(color = guide_legend(order = 1), linetype = guide_legend(order = 2))

p1 + p2 + plot_layout(axes = "collect_y")
```

## So, Is The Problem of Setting Learning Rate Solved?

No, not really, because it's still hard to pick $t_0$, $K$, and $a$.

:::: {.columns}

::: {.column width="47%"}

![SGD with fast decay.](../images/lecture12-lrdecay-fast.png)

:::

. . .

::: {.column width="47%"}

![SGD with slow decay.](../images/lecture12-lrdecay-slow.png)

:::

::::

## Implementation Details

### Nested Loops

Implementing a SGD algorithm typically means implementing a nested loop,
and we know this isn't R's strongest suite.

\medskip\pause

Next lecture, when we turn to Rcpp, you will have the tools to solve this problem.

. . .

### Data Storage

SGD algorithms typically subset rows, or block of rows.

\medskip\pause

In a column-major order matrix, this is not optimal. Transposing the matrix
first can help.

## Exercise

Minimize the following function:^[This is actually ridge
regression with a single predictor.]
$$
F(\beta) = \sum_{i=1}^n f_i(\beta) = \frac{1}{2} \sum_{i=1}^n (y_i - x_i \beta)^2 + \frac{\lambda}{2} \beta^2
$$

. . .

```{r}
#| echo: false
#| include: false
f <- function(beta, xx, yy, lambda) {
  sum(0.5 * (xx * beta - yy)^2 + 0.5 * lambda * beta^2)
}

f_grad <- function(beta, i, xx, yy, lambda) {
  (xx[i] * beta - yy[i]) * xx[i] + 2 * lambda * beta
}

beta <- 3

x <- rnorm(100)
y <- x * beta + rnorm(100)

beta_grid <- seq(-2, 5, length.out = 200)

f_vec <- Vectorize(f, vectorize.args = "beta")

plot(beta_grid, f_vec(beta_grid, x, y, 0.6), type = "l")
```

### Step 1

Implement a stochastic gradient descent method for solving this problem. Plot convergence
in terms of epochs and time. 

. . .

### Step 2

Test and benchmark your implementation against R's built-in `optimize()`.

. . .

### Step 3

Try different learning rates and, if you have time, implement mini-batch SGD.

## Summary

 Stochastic gradient descent is a popular optimization method for large-scale
optimization

\medskip\pause

Convergence results are relatively weak, but for many applications this does
not matter.

\medskip\pause

In practice, most implementations use mini-batches.

. . .

## Next Time

### Rcpp

We will look at how to use Rcpp to speed up R code.

\medskip\pause

Because SGD typically involves nested loops, these algorithms
benefit greatly from relying on C++.
