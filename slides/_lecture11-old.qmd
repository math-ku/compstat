---
title: "Gaussian Mixtures and Mixed Models with the EM Algorithm"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}

library(animate)
library(patchwork)
library(Matrix)

set.seed(1422)
```

## Today

Continue with the EM algorithm. Two examples.

### Gaussian Mixtures

Hands-on example with Gaussian mixtures, computing the empirical Fisher
information in multiple ways.

### Mixed Models

Fitting mixed models using the EM algorithm

# Gaussian Mixtures

## Gaussian Mixtures

The marginal density is

$$
f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.
$$

and we will regard $\theta = (p, \mu_1, \mu_2)$ as the unknown parameters, while
$\sigma_1$ and $\sigma_2$ are fixed.

. . .

$$Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)$$

where $\hat{p}_i = P_{\theta'}(Z_i = 1 \mid X_i = x_i)$,

-- which attains its maximum in

$$
\theta = \left(\frac{1}{n} \sum_{i} \hat{p}_i,\quad \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,\quad
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).
$$

### Simulation

:::: {.columns}

::: {.column width="47%"}

```{r}
sigma1 <- 1.5
sigma2 <- 1.5 # Same variances

p <- 0.5
mu1 <- -0.5
mu2 <- 4

n <- 5000
set.seed(321)
z <- sample(
  c(TRUE, FALSE),
  n,
  replace = TRUE,
  prob = c(p, 1 - p)
)

x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```

:::

::: {.column width="47%"}

```{r, echo=FALSE, fig.height=5, fig.width = 6}
gausdens <- function(x) {
  (p *
    exp(-(x - mu1)^2 / (2 * sigma1^2)) /
    sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) /
    sqrt(2 * pi)
}
xx <- seq(-5, 9, 0.1)
hist(x, freq = FALSE, ylim = c(0, 0.25), main = "", border = "transparent")
curve(gausdens(x), add = TRUE, col = "dark orange", lwd = 2)
```

:::

::::

### The E-Step

$$
\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }
$$

### EM Algorithm

```{r}
source(here::here("R/em_gauss_mix_exercise.R"))
EM <- EM_gauss_mix(x)
```

. . .

```{r}
EM(c(0.4, -0.2, 4.5))
EM(c(0.9, 1, 2))
```

. . .

What happens when evaluating the following, and why?

```{r, eval = FALSE}
EM(c(0.6, 3, 1))
```

## Gradients and numDeriv

Recall that
$$\nabla_{\theta} \ell(\hat{\theta}) = \nabla_{\theta} Q(\hat{\theta}, \hat{\theta}).$$

. . .

```{r q-def}
Q <- function(par, par_prime, EStep) {
  phat <- EStep(par_prime)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(
    phat *
      (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
      (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2))
  )
}
```

. . .

```{r numderiv-load}
library(numDeriv)
grad1 <- function(par) grad(function(par) -neg_loglik(par, x), par)
grad2 <- function(par) {
  grad(Q, par, par_prime = par, EStep = environment(EM)$EStep)
}
```

## Checking the Gradient Identity

```{r grad-identities-1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

--

```{r grad-identities-2}
par_hat <- EM(c(0.5, 0, 4))
grad1(par_hat)
grad2(par_hat)
```

## Fisher Information

Let $\hat{i}_X = - D^2_{\theta} \ell(\hat{\theta})$ denote the observed Fisher
information.

:::: {.columns}

::: {.column width="47%"}

Then \begin{align} \hat{i}_X &= - D_{\overline{\theta}} \left( \nabla*{\theta}
Q(\overline{\theta} \mid \overline{\theta})\right) |*{\overline{\theta} =
\hat{\theta}} \\ & = - D^2*{\theta} Q(\hat{\theta} \mid \hat{\theta}) -
D*{\theta'} \nabla*{\theta} Q(\hat{\theta} \mid \hat{\theta})\\ & = - \left(I -
D*{\theta} \Phi(\hat{\theta})^T\right) D^2*{\theta} Q(\hat{\theta} \mid
\hat{\theta}) \end{align} where $$\Phi(\theta') = \textrm{arg max}*{\theta} \
Q(\theta \mid \theta')$$ is the EM-map.

:::

::: {.column width="47%"}

### Exercise

- Download the [source code here](R/em_gauss_mix_exercise.R)
- Implement these three methods for computing the Fisher information in the
  Gaussian mixture problem.
- Test that they work.
- Benchmark them against one another.

:::

::::

