---
title: "Random Number Generation and Rejection Sampling"

format:
  beamer:
    keep-tex: true
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
library(profvis)

knitr::read_chunk(here::here("R", "vMSim.R"))

source(here::here("R", "vMSim.R"), keep.source = TRUE)
```

## Today

### Leftovers From Last Lecture

Efficient computations of smoothing splines

. . .

### Pseudo-Random Numbers

How do you simulate random numbers in R?

. . .

### Rejection Sampling

General and useful method for sampling from a target distribution


## Efficient Computations of Smoothing Splines

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
library(Matrix)
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splines::splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splines::splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) /
    6
}

omega <- pen_mat(
  seq(0, 1, 0.1)
)

nuuk <- read_table(
  here::here("data", "nuuk.txt"),
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) |>
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) |>
  mutate(Temperature = Temperature / 10) |>
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) |>
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

When $n$ is large, computing $\symbf{S}_{\lambda} \symbf{y}$ directly is
expensive.

\medskip\pause

In practice, we therefore often use fewer basis functions than data points, $p
< n$.

\pause

### Singular Value Decomposition

Using the singular value decomposition
$$
\Phi = \symbf{U} D \symbf{V}^T
$$
\pause we can rewrite the smoother matrix as
$$
\symbf{S}_{\lambda} = \widetilde{\symbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\symbf{U}}^T
$$
where $\widetilde{\symbf{U}} = \symbf{U} \symbf{W}$ and \pause
$$
D^{-1} \symbf{V}^T \symbf{\Omega} \symbf{V} D^{-1} = \symbf{W} \Gamma \symbf{W}^T.
$$

\medskip\pause

The columns of $\widetilde{\symbf{U}}$ form the **Demmler-Reinsch basis**.

\pdfpcnote{
  Can be done whether we use all or a subset of our basis functions

  S_\lambda is diagonalized

  Derive $\symbf{S}_\lambda$ on blackboard.
}

## Interpretation

Project the data $y$ onto the Demmler-Reinsch basis (columns of
$\widetilde{\symbf{U}}$) to get coefficients $\hat{\symbf{\beta}} =
\widetilde{\symbf{U}}^Ty$.

\pause\medskip

Each coefficient is shrunk according to the corresponding eigenvalue $\gamma_i$
and smoothing parameter $\lambda$:
$$
\hat{\symbf{\beta}}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.
$$

\pause\medskip

The final smoothed values are reconstructed by combining the shrunk coefficients
with the basis functions.
$$
\hat{\symbf{f}} = \widetilde{\symbf{U}} \hat{\symbf{\beta}}(\lambda)
$$

\pdfpcnote{
  \hat\beta are the original coefficients (if lambda = 0).
}

## The Demmler-Reinsch Basis (Columns of $\widetilde{\symbf{U}}$)

```{r spline-diagonalization}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
inner_knots <- seq(1867, 2013, length.out = 18)
Phi <- splines::splineDesign(
  c(rep(range(inner_knots), 3), inner_knots),
  nuuk_year$Year
)
omega <- pen_mat(inner_knots)
Phi_svd <- svd(Phi)
omega_tilde <- t(
  t(
    crossprod(Phi_svd$v, omega %*% Phi_svd$v)
  ) /
    Phi_svd$d
) /
  Phi_svd$d

Omega_tilde_svd <- svd(omega_tilde)
U_tilde <- Phi_svd$u %*% Omega_tilde_svd$u

colnames(U_tilde) <- paste0(rep("u", 20), 1:20)
bind_cols(select(nuuk_year, Year), as_tibble(U_tilde)) |>
  gather(key = "term", value = "value", -Year, factor_key = TRUE) |>
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

##

```{r spline-gamma}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
#| fig-cap: The eigenvalues $\gamma_i$ of the penalty matrix in the
#|   Demmler-Reinsch basis.
library(patchwork)

d <- Omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2 + plot_layout(axes = "collect")
```

## Pseudo-Random Numbers

Computers usually generate **pseudo-random** numbers.

\medskip\pause


Not really random, but have properties that make them appear to be so.

\medskip\pause

A research field in itself, see `?RNG` in R for available algorithms.

. . .

### Mersenne Twister

The default in R, which generates integers in the range
$$
\{0, 1, \ldots, 2^{32} -1\}.
$$

\medskip\pause

Long period: all combinations of consecutive integers up to dimension 623 occur
equally often in a period.

. . .

### Uniform Random Variables i n R

`runif()` samples by just scaling these pseduo-random integers to the unit
interval (with some tweaks to avoid division by 0).

## Transformation Methods

If $T : \mathcal{Z} \to \mathbb{R}$ is a map and $Z \in \mathcal{Z}$ is a random
variable we can sample, then we can sample $X = T(Z).$

. . .

### Inversion Sampling

If $F^{-1} : (0,1) \mapsto \mathbb{R}$ is the generalized inverse of a
distribution function and $U$ is uniformly distributed on $(0, 1)$ then
$$
F^{-1}(U)
$$
has distribution function $F$.

\medskip\pause

Computing quantile function is easy for discrete distributions, but hard for
continuous ones.

## Gaussian Random Variables

### Box--Muller Method

A transformation of two independent uniforms into two independent Gaussian
random variables (polar coordinates).

. . .

### Inverse Transform

$X = \Phi^{-1}(U)$where $\Phi$ is the distribution function for the Gaussian distribution.

. . .

### Rejection Sampling

See [Exercise 5.1 in
CSwR](https://cswr.nrhstat.org/random-number-generation#univariate:ex) or the
[Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm).

. . .

```{r rng-kind}
RNGkind()
```

## Computing $\Phi^{-1}$

Recall that
$$
\Phi(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-z^2/2} \mathrm{d} z.
$$

. . .

[This](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/snorm.c#L265),
together with [this technical
approximation](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/qnorm.c#L52)
of $\Phi^{-1}$ is how R generates samples from $\mathcal{N}(0,1)$.

\medskip\pause

The technical approximation is a rational function.

## Sampling from a $t$-Distribution

Let $Z = (Y, W) \in \mathbb{R} \times (0, \infty)$ with $Z \sim \mathcal{N}(0,
1)$ and $W \sim \chi^2_k$ independent.

\medskip\pause

Define $T : \mathbb{R} \times (0, \infty) \to \mathbb{R}$ by
$$
T(z,w) = \frac{z}{\sqrt{w/k}},
$$
then
$$
X = T(Z, W) = \frac{Z}{\sqrt{W/k}} \sim t_k.
$$

. . .

This is how R simulates from a $t$-distribution with $W$ generated from a Gamma
distribution with shape parameter $k / 2$ and scale parameter $2$.


## Von Mises Distribution

The density on $(-\pi, \pi]$ is
$$
f(x) = \frac{1}{2 \pi I_0(\kappa)} \ e^{\kappa \cos(x - \mu)}
$$
for $\kappa > 0$ and $\mu \in (-\pi, \pi]$ parameters and $I_0$ is a modified
Bessel function.

. . .

```{r vMsim}
library(movMF)
xy <- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5)))

# rmovMF represents samples as elements on the unit circle
x <- acos(xy[, 1]) * sign(xy[, 2])
```

. . .

### Problem {.alert}

Can we sample via the transformation method? Yes, but not easily. There is no
closed-form expression for the quantile function.

##


```{r vMhist}
#| fig-height: 2.5
#| fig-width: 5.5
#| echo: false
#| fig-cap: Histogram of samples from rmovMF()
ggplot(tibble(x = x), aes(x)) +
  geom_rug(alpha = 0.1) +
  geom_histogram(
    aes(y = ..density..),
    bins = 15,
    fill = "grey70",
    color = "white"
  ) +
  stat_function(
    aes(color = "Theoretical Density"),
    fun = function(x) exp(0.5 * cos(x + 1.5)) / (2 * pi * besselI(0.5, 0))
  ) +
  geom_density(
    aes(color = "SJ density estimate"),
    bw = "sj",
    key_glyph = "path"
  ) +
  labs(y = "Density", x = "x", color = NULL)
```

## Rejection Sampling

We want to sample from a target distribution $f(x)$ but have no readily
available transform $T$ for our proposal $g(x)$.

\medskip\pause

Let $Y_1, Y_2, \ldots$ be i.i.d. with density $g$ on $\mathbb{R}$ and $U_1, U_2,
\ldots$ be i.i.d. uniform and independent of $Y_i$ for all $i$.

\medskip\pause

Define
$$
\sigma = \inf\{n \geq 1 \mid U_n \leq \alpha f(Y_n) / g(Y_n)\},
$$
for $\alpha \in (0, 1]$, where $f$ is a density.

. . .

### Theorem

If $\alpha f(y) \leq g(y)$ for all $y \in \mathbb{R}$ then the distribution of
$Y_{\sigma}$ has density $f$.

\medskip\pause

$\alpha$is the **acceptance probability** and $g(y)/\alpha$ the **envelope** of $f$.

## Normalizing Constants

If $f(y) = c q(y)$ and $g(y) = d p(y)$ for (unknown) normalizing constants $c, d
> 0$ and $\alpha' q \leq p$ then <br> <br>
$$
\underbrace{\left(\frac{\alpha' d}{c}\right)}_{= \alpha} \ f \leq g.
$$

. . .

Moreover,
$$
u > \frac{\alpha f(y)}{g(y)} \Leftrightarrow u > \frac{\alpha'q(y)}{p(y)},
$$
and rejection sampling can be implemented without computing $c$ or $d$.

. . .

## Von Mises Rejection Sampling

Rejection sampling using the uniform proposal, $g(y) \propto 1$.

\medskip\pause

Since
$$
e^{\kappa(\cos(y) - 1)} = \alpha' e^{\kappa \cos(y)} \leq 1,
$$
where $\alpha' = \exp(-\kappa)$, we reject if
$$
U > e^{\kappa(\cos(Y) - 1)}.
$$

. . .

\begin{algorithm}[H]
  \caption{Rejection sampling for the von Mises distribution}
  \Repeat{sample is accepted}{
    Generate $Y \sim \mathrm{Uniform}(-\pi, \pi)$\;
    Generate $U \sim \mathrm{Uniform}(0, 1)$\;
    \If{$U \leq e^{\kappa(\cos(Y) - 1)}$}{
      Accept $Y$\;
    }
  }
\end{algorithm}

## Implementation

```{r}
get_one_sample_vmises <- function(kappa) {
  repeat {
    Y <- runif(1, -pi, pi)
    U <- runif(1)
    if (U <= exp(kappa * (cos(Y) - 1))) {
      break
    }
  }
  Y
}
```

. . .

```{r}
sample_vonmises_slow <- function(n, kappa) {
  replicate(n, get_one_sample_vmises(kappa))
}
```


##

```{r vMsim2}
#| fig-width: 5.4
#| fig-height: 3
#| echo: false
#| fig-cap: Histogram of samples from `sample_vonmises_slow()`, with
#|   theoretical density overlaid.
f <- function(x, kappa) exp(kappa * cos(x)) / (2 * pi * besselI(kappa, 0))

# Simulate data
x1 <- sample_vonmises_slow(100000, 0.5)
x2 <- sample_vonmises_slow(100000, 2)

df <- tibble(
  x = c(x1, x2),
  kappa = factor(rep(c(0.5, 2), each = 100000))
)

ggplot(df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 30,
    fill = "grey60",
    color = "white"
  ) +
  stat_function(
    fun = f,
    args = list(kappa = 0.5),
    color = "blue",
    data = data.frame(x = seq(-pi, pi, length.out = 200), kappa = 0.5)
  ) +
  stat_function(
    fun = f,
    args = list(kappa = 2),
    color = "blue",
    data = data.frame(x = seq(-pi, pi, length.out = 200), kappa = 2)
  ) +
  labs(y = "Density", x = "x") +
  facet_wrap(~kappa, ncol = 2, labeller = "label_both")
```

##

```{r profile-vsim}
#| echo: false
#| eval: false
source(here::here("R", "vmises-sim-slow.R"))
profvis(sample_vonmises_slow(100000, 5))
```

![A line profile of the naive von Mises sampling
implementation.](../images/vmises-slow-profiling.png){width=82%}

## Findings from Profiling

The bottleneck is the repeated calls to `runif(1)`.

\medskip\pause

Both because of the for loop overhead, but also because calling the RNG for a
single number is inefficient.

\medskip\pause

It is faster to generate all random numbers once and store them in a vector.

\medskip\pause

But how to do that for rejection sampling with an \alert{unknown} number of
rejections?


## Vectorized (but Random Length) Rejection Sampler

We can speed up the rejection sampler by generating $m$ proposal samples at
once.

```{r vM-rejection-vec}
fixed_vonmises_sampler <- function(m, kappa) {
  Y <- runif(m, -pi, pi)
  u <- runif(m)
  accept <- u <= exp(kappa * (cos(Y) - 1))
  Y[accept]
}
```

. . .

Simple and fast, but returns a **random** number of samples!

\medskip\pause

We need a function that returns exactly $n$ samples. But we don't know $\alpha$
in advance.

##

\begin{algorithm}[H]
  \caption{Sampling exactly $n$ values via rejection sampling with unknown
    acceptance probability $\alpha$.}
  $\alpha \gets 1$\;
  $\symbf{y} \gets$ empty list\;
  $n_\text{accepted} \gets 0$\;
  \While{$n_\text{accepted} < n$}{
    $m \gets \lceil (n - n_\text{accepted}) / \alpha \rceil$\;
    $\symbf{y}_\text{new} \gets$ Generate $m$ candidate samples $x_1, \ldots, x_m$\;
    $n_\text{accepted} \gets n_\text{accepted} +$ length$(\symbf{y}_\text{new})$\;
    \If{first batch}{
      $\alpha \gets (n_\text{accepted} + 1) / (m + 1)$\;
    }
    Append $\symbf{y}_\text{new}$ to $\symbf{y}$\;
  }
  \KwRet{$\symbf{y}_{1:n}$}
\end{algorithm}

## Function Factory for Rejection Sampling

```{r new_rejection_sampler}
new_rejection_sampler <- function(generator) {
  function(n, ...) {
    alpha <- 1
    y <- numeric(0)
    n_accepted <- 0
    while (n_accepted < n) {
      m <- ceiling((n - n_accepted) / alpha)
      y_new <- generator(m, ...)
      n_accepted <- n_accepted + length(y_new)
      if (length(y) == 0) {
        alpha <- (n_accepted + 1) / (m + 1)
      }
      y <- c(y, y_new)
    }
    list(x = y[seq_len(n)], alpha = alpha)
  }
}
```

##

```{r vmises-bench}
#| fig-width: 5
#| fig-height: 1.5
sample_vonmises <- new_rejection_sampler(fixed_vonmises_sampler)

bench::mark(
  sample_vonmises(1e5, 5),
  sample_vonmises_slow(1e5, 5),
  check = FALSE
) |>
  plot()
```

## Adaptive Envelopes

When $f$ is _log-concave_ on $I$ we can construct bounds of the form
$$
f(x) \leq e^{V(x)}
$$
where
$$
V(x) = \sum_{i=1}^m  (a_i x + b_i) \symbf{1}_{I_i}(x)
$$
for intervals $I_i$ forming a partition of $I$.

\medskip\pause

Typically, $a_i x + b_i$ is tangent to the graph of $\log(f)$ at $x_i \in I_i =
(z_{i-1}, z_i]$ for
$$
z_0 < x_1 < z_1 < x_2 < \ldots < z_{m-1} < x_m < z_m.
$$

## Beta Distribution

```{r}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
fixed_beta_sampler <- function(m, x1, x2, alpha, beta) {
  lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x)
  lf_deriv <- function(x) (alpha - 1) / x - (beta - 1) / (1 - x)
  a1 <- lf_deriv(x1)
  a2 <- lf_deriv(x2)
  if (a1 == 0 || a2 == 0 || a1 - a2 == 0) {
    stop(
      "\nThe implementation requires a_1 and a_2 different and both different from zero. Choose different values of x_1 and x_2.",
      call. = FALSE
    )
  }
  b1 <- lf(x1) - a1 * x1
  b2 <- lf(x2) - a2 * x2
  z1 <- (b2 - b1) / (a1 - a2)
  Q1 <- exp(b1) * (exp(a1 * z1) - 1) / a1
  c <- Q1 + exp(b2) * (exp(a2 * 1) - exp(a2 * z1)) / a2

  y <- ratio <- numeric(m)
  uy <- c * runif(m)
  u <- runif(m)
  i <- uy < Q1
  y[i] <- z <- log(a1 * exp(-b1) * uy[i] + 1) / a1
  y[!i] <- zz <- log(a2 * exp(-b2) * (uy[!i] - Q1) + exp(a2 * z1)) / a2
  ratio[i] <- exp(lf(z) - a1 * z - b1)
  ratio[!i] <- exp(lf(zz) - a2 * zz - b2)
  accept <- u <= ratio
  y[accept]
}

sample_beta <- new_rejection_sampler(fixed_beta_sampler)
```

```{r Beta-fig}
#| fig-cap: Histograms of samples from sample_beta() with theoretical
#|   densities (blue) and envelopes (red) overlaid.
#| fig-width: 5.5
#| fig-height: 3
#| echo: false
par(mfrow = c(2, 2))
envelope <- function(x, x1, x2, alpha, beta) {
  lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x)
  lf_deriv <- function(x) (alpha - 1) / x - (beta - 1) / (1 - x)
  a1 <- lf_deriv(x1)
  a2 <- lf_deriv(x2)
  b1 <- lf(x1) - a1 * x1
  b2 <- lf(x2) - a2 * x2
  z1 <- (b2 - b1) / (a1 - a2)
  ifelse(x < z1, exp(a1 * x + b1), exp(a2 * x + b2))
}

hist(
  sample_beta(100000, x1 = 0.3, x2 = 0.7, alpha = 4, beta = 2)$x,
  prob = TRUE,
  main = "",
  ylim = c(0, 3)
)
curve(x^3 * (1 - x) / beta(4, 2), col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, x1 = 0.3, x2 = 0.7, alpha = 4, beta = 2) / beta(4, 2),
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

hist(
  sample_beta(100000, x1 = 0.3, x2 = 0.7, alpha = 1.8, beta = 2.4)$x,
  prob = TRUE,
  main = "",
  ylim = c(0, 3)
)
curve(x^0.8 * (1 - x)^1.4 / beta(1.8, 2.4), col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, x1 = 0.3, x2 = 0.7, alpha = 1.8, beta = 2.4) / beta(1.8, 2.4),
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

hist(
  sample_beta(100000, x1 = 0.2, x2 = 0.5, alpha = 4, beta = 2)$x,
  prob = TRUE,
  main = "",
  ylim = c(0, 3)
)
curve(x^3 * (1 - x) / beta(4, 2), col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, x1 = 0.2, x2 = 0.5, alpha = 4, beta = 2) / beta(4, 2),
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

hist(
  sample_beta(100000, x1 = 0.2, x2 = 0.5, alpha = 1.8, beta = 2.4)$x,
  prob = TRUE,
  main = "",
  ylim = c(0, 3)
)
curve(x^0.8 * (1 - x)^1.4 / beta(1.8, 2.4), col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, x1 = 0.2, x2 = 0.5, alpha = 1.8, beta = 2.4) / beta(1.8, 2.4),
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)
par(mfrow = c(1, 1))
```

##

```{r beta-envelopes}
#| echo: false
#| warning: false
#| fig-width: 5.5
#| fig-height: 3
#| fig-cap: Theoretical densities (blue) and envelopes (red) for
#|   different parameter values and choices of $x_1$ and $x_2$.
par(mfrow = c(1, 2))
curve(x^3 * (1 - x) / beta(4, 2), col = "blue", lwd = 2, log = "y")
curve(
  envelope(x, x1 = 0.3, x2 = 0.7, alpha = 4, beta = 2) / beta(4, 2),
  col = "red",
  lwd = 2,
  n = 400,
  add = TRUE
)
curve(x^0.8 * (1 - x)^1.4 / beta(1.8, 2.4), col = "blue", lwd = 2, log = "y")
curve(
  envelope(x, x1 = 0.2, x2 = 0.5, alpha = 1.8, beta = 2.4) / beta(1.8, 2.4),
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)
par(mfrow = c(1, 1))
```

## Von Mises Adaptive Envelope

```{r vonmises-adaptive}
#| echo: false
dvm <- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0))

von_mises_adapt_random <- function(N, x1, x2, kappa) {
  lf <- function(x) kappa * cos(x)
  lf_deriv <- function(x) -kappa * sin(x)
  a1 <- 2 * kappa / pi
  a2 <- lf_deriv(x1)
  a3 <- lf_deriv(x2)
  a4 <- -a1

  b1 <- kappa
  b2 <- lf(x1) - a2 * x1
  b3 <- lf(x2) - a3 * x2
  b4 <- kappa

  z0 <- -pi
  z1 <- -pi / 2
  z2 <- (b3 - b2) / (a2 - a3)
  z3 <- pi / 2
  z4 <- pi

  Q1 <- exp(b1) * (exp(a1 * z1) - exp(a1 * z0)) / a1
  Q2 <- Q1 + exp(b2) * (exp(a2 * z2) - exp(a2 * z1)) / a2
  Q3 <- Q2 + exp(b3) * (exp(a3 * z3) - exp(a3 * z2)) / a3
  c <- Q3 + exp(b4) * (exp(a4 * z4) - exp(a4 * z3)) / a4

  u0 <- c * runif(N)
  u <- runif(N)

  I1 <- (u0 < Q1)
  I2 <- (u0 >= Q1) & (u0 < Q2)
  I3 <- (u0 >= Q2) & (u0 < Q3)
  I4 <- (u0 >= Q3)

  x <- numeric(N)
  accept <- logical(N)
  x[I1] <- log(a1 * exp(-b1) * u0[I1] + exp(a1 * z0)) / a1
  accept[I1] <- u[I1] <= exp(lf(x[I1]) - a1 * x[I1] - b1)
  x[I2] <- log(a2 * exp(-b2) * (u0[I2] - Q1) + exp(a2 * z1)) / a2
  accept[I2] <- u[I2] <= exp(lf(x[I2]) - a2 * x[I2] - b2)
  x[I3] <- log(a3 * exp(-b3) * (u0[I3] - Q2) + exp(a3 * z2)) / a3
  accept[I3] <- u[I3] <= exp(lf(x[I3]) - a3 * x[I3] - b3)
  x[I4] <- log(a4 * exp(-b4) * (u0[I4] - Q3) + exp(a4 * z3)) / a4
  accept[I4] <- u[I4] <= exp(lf(x[I4]) - a4 * x[I4] - b4)

  x[accept]
}

von_mises_adapt <- new_rejection_sampler(von_mises_adapt_random)

envelope <- function(x, x1, x2, kappa) {
  lf <- function(x) kappa * cos(x)
  lf_deriv <- function(x) -kappa * sin(x)
  a1 <- 2 * kappa / pi
  a2 <- lf_deriv(x1)
  a3 <- lf_deriv(x2)
  a4 <- -a1

  b1 <- kappa
  b2 <- lf(x1) - a2 * x1
  b3 <- lf(x2) - a3 * x2
  b4 <- kappa

  z0 <- -pi
  z1 <- -pi / 2
  z2 <- (b3 - b2) / (a2 - a3)
  z3 <- pi / 2
  z4 <- pi

  env <- numeric(length(x))
  i1 <- x < z1
  i2 <- x >= z1 & x < z2
  i3 <- x >= z2 & x < z3
  i4 <- x >= z3

  env[i1] <- exp(a1 * x[i1] + b1)
  env[i2] <- exp(a2 * x[i2] + b2)
  env[i3] <- exp(a3 * x[i3] + b3)
  env[i4] <- exp(a4 * x[i4] + b4)

  env
}
```

```{r vonmises-adaptive-plot}
#| fig-cap: Histograms of samples from von_mises_adapt() with theoretical
#|   densities (blue) and envelopes (red) overlaid.
#| fig-width: 5.5
#| fig-height: 3
#| echo: false
par(mfrow = c(2, 2))

x <- von_mises_adapt(100000, -0.4, 0.4, 5)$x
hist(
  x,
  breaks = seq(-pi, pi, length.out = 30),
  prob = TRUE,
  main = expression(paste(kappa == 5, ", ", x[1] == -0.4, ", ", x[2] == 0.4)),
  ylim = c(0, 1.5)
)
curve(dvm(x, 5), -pi, pi, col = "blue", lwd = 2, add = TRUE, n = )
curve(
  envelope(x, -0.4, 0.4, 5) / (2 * pi * besselI(5, 0)),
  -pi,
  pi,
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

x <- von_mises_adapt(100000, -0.1, 0.1, 5)$x
hist(
  x,
  breaks = seq(-pi, pi, length.out = 30),
  prob = TRUE,
  main = expression(paste(kappa == 5, ", ", x[1] == -0.1, ", ", x[2] == 0.1)),
  ylim = c(0, 1.5)
)
curve(dvm(x, 5), -pi, pi, col = "blue", lwd = 2, add = TRUE, n = )
curve(
  envelope(x, -0.1, 0.1, 5) / (2 * pi * besselI(5, 0)),
  -pi,
  pi,
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

x <- von_mises_adapt(100000, -0.4, 0.4, 2)$x
hist(
  x,
  breaks = seq(-pi, pi, length.out = 30),
  prob = TRUE,
  main = expression(paste(kappa == 2, ", ", x[1] == -0.4, ", ", x[2] == 0.4)),
  ylim = c(0, 1.5)
)
curve(dvm(x, 2), -pi, pi, col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, -0.4, 0.4, 2) / (2 * pi * besselI(2, 0)),
  -pi,
  pi,
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)

x <- von_mises_adapt(100000, -1, 1, 2)$x
hist(
  x,
  breaks = seq(-pi, pi, length.out = 30),
  prob = TRUE,
  main = expression(paste(kappa == 2, ", ", x[1] == -1, ", ", x[2] == 1)),
  ylim = c(0, 1.5)
)
curve(dvm(x, 2), -pi, pi, col = "blue", lwd = 2, add = TRUE)
curve(
  envelope(x, -1, 1, 2) / (2 * pi * besselI(2, 0)),
  -pi,
  pi,
  col = "red",
  lwd = 2,
  add = TRUE,
  n = 400
)
```

## Benchmark

```{r benchmark-vonmises-adaptive}
#| echo: false
#| fig-width: 5.5
#| fig-height: 2.5
bench::mark(
  adaptive = von_mises_adapt(100, -1, 1, 5),
  vec = sample_vonmises(100, 5),
  check = FALSE
) |>
  plot()
```

. . .

Differences due to R implementation.

## Notes about Adaptive Envelopes

- General and powerful technique
- But tricky to implement
- Efficiency depends on number of rejections and also quality of implementation.
- A good implementation should make use of new points to update envelope.

## Exercises

### Step 1

Use that $W = X_1^2 + \ldots + X_k^2 \sim \chi^2_k$ for $X_1, \ldots, X_k$
i.i.d. $\mathcal{N}(0, 1)$ to implement a function, `my_rchisq()`, such that
`my_rchisq(n, k)` returns a sample of $n$ i.i.d. observations from $\chi^2_k$.

```{r rchisq2-implementation, include = FALSE}
my_rchisq <- function(n, k) {
  y <- numeric(n)
  for (i in seq_len(n)) {
    x <- rnorm(k)
    y[i] <- sum(x^2)
  }
  y
}
```

```{r rchisq2-implementation-faster, include = FALSE}
my_fast_rchisq <- function(n, k) {
  x <- matrix(rnorm(k * n), k)
  colSums(x)
}
```

. . .

### Step 2

Make another function `my_other_rchisq()` that uses inverse sampling.

```{r rchisq2, include = FALSE}
my_other_rchisq <- function(n, k) {
  x <- rnorm(n)
  qchisq(x, k)
}
```

. . .

### Step 3

Benchmark your implementations against one another and `rchisq()`.

```{r rchisq-bench, include = FALSE}
n <- 1e4
k <- 5
bench::mark(
  rchisq(n, k),
  my_rchisq(n, k),
  my_fast_rchisq(n, k),
  my_other_rchisq(n, k),
  check = FALSE
) |>
  plot()
```
