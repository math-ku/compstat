---
title: "Random Number Generation and Rejection Sampling"
---

{{< include _common.qmd >}}

```{r setup, include=FALSE}
library(profvis)

knitr::read_chunk(here::here("R", "vMSim.R"))

source(here::here("R", "vMSim.R"), keep.source = TRUE)
```

## Today

### Leftovers From Last Lecture

Efficient computations of smoothing splines

. . .

### Pseudo-Random Numbers

How do you simulate random numbers in R?

. . .

### Rejection Sampling

General and useful method for sampling from a target distribution

## Smoothing Splines

Recall, that our smoothing spline estimate is
$$
\symbf{f} = \symbf{\Phi}\symbf{\beta}
$$
with $\symbf{\Phi}_{ij} = \varphi_j(x_i)$, and \pause
$$
\begin{aligned}
  L(\symbf{f}) & = (\symbf{y} - \symbf{f})^T (\symbf{y} - \symbf{f}) + \lambda \lVert f''\rVert_2^2 \\
               & = (\symbf{y} - \symbf{\Phi}\symbf{\beta})^T (\symbf{y} - \symbf{\Phi}\symbf{\beta}) + \lambda \symbf{\beta}^T \symbf{\Omega} \symbf{\beta}
\end{aligned}
$$
with
$$
\symbf{\Omega}_{jk} = \int \varphi_j''(z) \varphi_k''(z) \text{d}z.
$$

\pause

### Solution

The solution is
$$
\hat{\symbf{\beta}} = (\symbf{\Phi}^T \symbf{\Phi} + \lambda \symbf{\Omega})^{-1} \symbf{\Phi}^T \symbf{y}.
$$


## Efficient Computations of Smoothing Splines

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
library(Matrix)
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splines::splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splines::splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) /
    6
}

omega <- pen_mat(
  seq(0, 1, 0.1)
)

nuuk <- read_table(
  here::here("data", "nuuk.txt"),
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) |>
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) |>
  mutate(Temperature = Temperature / 10) |>
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) |>
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

When $n$ is large, computing $\symbf{S}_{\lambda} \symbf{y}$ directly is
expensive.

\medskip\pause

In practice, we therefore often use fewer basis functions than data points, $p
< n$.

\pause

### Singular Value Decomposition

Using the singular value decomposition
$$
\symbf{\Phi} = \symbf{U} \symbf{D} \symbf{V}^T
$$
\pause we can rewrite the smoother matrix as
$$
\symbf{S}_{\lambda} = \widetilde{\symbf{U}}  (I + \lambda \operatorname{diag}(\symbf{\gamma}))^{-1} \widetilde{\symbf{U}}^T
$$
where $\widetilde{\symbf{U}} = \symbf{U} \symbf{W}$ and \pause
$$
\symbf{D}^{-1} \symbf{V}^T \symbf{\Omega} \symbf{V} \symbf{D}^{-1} = \symbf{W} \operatorname{diag}(\symbf{\gamma}) \symbf{W}^T.
$$

\medskip\pause

The columns of $\widetilde{\symbf{U}}$ form the **Demmler-Reinsch basis**.

\pdfpcnote{
  Can be done whether we use all or a subset of our basis functions

  S_\lambda is diagonalized

  Derive $\symbf{S}_\lambda$ on blackboard.
}

## Interpretation

Project the data $y$ onto the Demmler-Reinsch basis (columns of
$\widetilde{\symbf{U}}$) to get coefficients $\hat{\symbf{\beta}} =
\widetilde{\symbf{U}}^T\symbf{y}$.

\pause\medskip

Each coefficient is shrunk according to the corresponding eigenvalue $\gamma_i$
and smoothing parameter $\lambda$:
$$
\hat{\symbf{\beta}}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.
$$

\pause\medskip

The final smoothed values are reconstructed by combining the shrunk coefficients
with the basis functions.
$$
\hat{\symbf{f}} = \widetilde{\symbf{U}} \hat{\symbf{\beta}}(\lambda)
$$

\pdfpcnote{
  \hat\beta are the original coefficients (if lambda = 0).
}

## The Demmler-Reinsch Basis (Columns of $\widetilde{\symbf{U}}$)

```{r spline-diagonalization}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
inner_knots <- seq(1867, 2013, length.out = 18)
Phi <- splines::splineDesign(
  c(rep(range(inner_knots), 3), inner_knots),
  nuuk_year$Year
)
omega <- pen_mat(inner_knots)
Phi_svd <- svd(Phi)
omega_tilde <- t(
  t(
    crossprod(Phi_svd$v, omega %*% Phi_svd$v)
  ) /
    Phi_svd$d
) /
  Phi_svd$d

Omega_tilde_svd <- svd(omega_tilde)
U_tilde <- Phi_svd$u %*% Omega_tilde_svd$u

colnames(U_tilde) <- paste0(rep("u", 20), 1:20)
bind_cols(select(nuuk_year, Year), as_tibble(U_tilde)) |>
  gather(key = "term", value = "value", -Year, factor_key = TRUE) |>
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

##

```{r spline-gamma}
#| echo: false
#| fig-width: 5.5
#| fig-height: 3
#| fig-cap: The eigenvalues $\gamma_i$ of the penalty matrix in the
#|   Demmler-Reinsch basis.
library(patchwork)

d <- Omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2 + plot_layout(axes = "collect")
```

## Pseudo-Random Numbers

Computers have no concept of "true" randomness and instead generate
**pseudo-random** numbers.

\medskip\pause


Not really random, but appear to be so.

\medskip\pause

A research field in itself, see `?RNG` in R for available algorithms.

. . .

### Mersenne Twister

The default in R, which generates integers in the range
$$
\{0, 1, \ldots, 2^{32} -1\}.
$$

\medskip\pause

Long period: all combinations of consecutive integers up to dimension 623 occur
equally often in a period.

. . .

### Uniform Random Variables i n R

`runif()` samples by just scaling these pseduo-random integers to the unit
interval (with some tweaks to avoid division by 0).

\pdfpcnote{
  Random numbers are used in many applications.
}

## Transformation Methods

What if we want to sample from a different distribution?

\medskip\pause

We could use **the transformation method**, which relies on the following fact:

\medskip

If $T : \mathcal{Z} \to \mathbb{R}$ is a map and $Z \in \mathcal{Z}$ is a random
variable we can sample, then we can sample $X = T(Z).$

\begin{figure}
\centering
\begin{tikzpicture}[node distance=1cm, auto, >=stealth, thick]
  \node[draw, rectangle, rounded corners, fill=blue!10] (A) {Sample $Z$};
  \node[draw, rectangle, rounded corners, right = of A, fill=green!10] (B) {Apply $T$};
  \node[draw, rectangle, rounded corners, right = of B, fill=orange!10] (C) {Get $X = T(Z)$};
  \draw[->] (A) -- (B);
  \draw[->] (B) -- (C);
\end{tikzpicture}
\end{figure}

\pause

Transformation methods let us generate samples from complex distributions by
applying a function to samples from a simpler distribution.

\medskip

### Example {.example}

To sample $X \sim \mathrm{Exp}(\lambda)$, use $X = -\log(U)/\lambda$ with $U
\sim \mathrm{Uniform}(0,1)$.


## Inversion Sampling

A type of **transformation** sampling.

\medskip\pause

If $F^{-1} : (0,1) \mapsto \mathbb{R}$ is the generalized inverse of a
distribution function and $U$ is uniformly distributed on $(0, 1)$ then
$$
F^{-1}(U)
$$
has distribution function $F$.

\medskip\pause

Computing $F^{-1}$: easy for discrete distributions, but hard for continuous
ones.

## Gaussian Random Variables

### Box--Muller Method

A transformation of two independent uniforms into two independent Gaussian
random variables (polar coordinates).

. . .

### Inverse Transform

$X = \Phi^{-1}(U)$ where $\Phi$ is the distribution function for the Gaussian
distribution.

. . .

### Rejection Sampling

See [Exercise 5.1 in
CSwR](https://cswr.nrhstat.org/random-number-generation#univariate:ex) or the
[Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm).


## Computing $\Phi^{-1}$

Recall that
$$
\Phi(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-z^2/2} \mathrm{d} z.
$$

. . .


But $\Phi^{-1}$ (the quantile function) does not have a closed-form expression.  

\medskip\pause

We need to approximate $\Phi^{-1}$ numerically.

\medskip\pause

[This](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/snorm.c#L265),
together with [this technical
approximation](https://github.com/wch/r-source/blob/af7f52f70101960861e5d995d3a4bec010bc89e6/src/nmath/qnorm.c#L52)
of $\Phi^{-1}$ is how R generates samples from $\mathcal{N}(0,1)$.

\medskip\pause


## Sampling in R

R has three basic options for customizing the behavior,
set through `RNGkind()`.

```{r rng-kind}
RNGkind()
```

. . .

### Options

- `kind`: type of pseudo-random number generator
- `normal.kind`: type of sampler for Normal samples
- `sample.kind`: type of sampler for `sample()`

##


\begin{figure}
  \begin{tikzpicture}[node distance=1cm, auto, >=stealth, thick]
    % Nodes
    \node[draw, rectangle, rounded corners, fill=gray!20] (A) {Integers from Pseudo-RNG};
    \node[draw, rectangle, rounded corners, below=of A, fill=green!10] (B) {$\mathcal{U}(0, 1)$};
    \node[draw, rectangle, rounded corners, below right=of B, fill=orange!10] (D1) {Transformation methods};
    \node[draw, rectangle, rounded corners, below left=of B, fill=red!10] (D2) {Rejection sampling};

    % Arrows
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (D1);
    \draw[->] (B) -- (D2);
  \end{tikzpicture}
  \caption{Main categories of sampling methods (in this course).}
\end{figure}


## Example: Sampling from a $t$-Distribution

If we let
$$
T(z,w) = \frac{z}{\sqrt{w/k}},
$$
and take $Z \sim \mathcal{N}(0,1)$ and $W \sim \chi^2_k$, then
$$
X = T(Z, W) = \frac{Z}{\sqrt{W/k}} \sim t_k.
$$

. . .

This transformation is used in R and other software to simulate $t$-distributed random variables.

\medskip\pause

\begin{algorithm}[H]
  \caption{Transformation sampling for the $t$-distribution}
  $z \gets $ sample from $\mathcal{N}(0, 1)$\;
  $w \gets $ sample from $\chi^2_k$\;
  \KwRet{$z / \sqrt{w / k}$}\;
\end{algorithm}


## Von Mises Distribution

The density on $(-\pi, \pi]$ is
$$
f(x) = \frac{1}{2 \pi I_0(\kappa)} \ e^{\kappa \cos(x - \mu)}
$$
for $\kappa > 0$ and $\mu \in (-\pi, \pi]$ parameters and $I_0$ is the modified
Bessel function.

\medskip

```{r}
#| fig-cap: The von Mises density for $\mu = 2$ and $\kappa = 9$.
#| echo: false
#| fig-width: 5
#| fig-height: 2
library(ggplot2)

dvm <- function(x, k) exp(k * cos(x - 2)) / (2 * pi * besselI(k, 0))

xy <- tibble(
  x = seq(0, 2 * pi, length.out = 400),
  y = dvm(x, 9)
)


p1 <- ggplot(xy, aes(x, y)) +
  geom_line() +
  coord_fixed(ratio = 2 * pi / 1.2) +
  labs(y = "Density") +
  theme_minimal()

p2 <- ggplot(xy, aes(x = x, y = y)) +
  geom_line() +
  coord_polar(
    theta = "x",
    start = 0,
    direction = 1
  ) +
  labs(
    x = "Angle (radians)",
    y = NULL
  ) +
  theme_minimal()

p1 + p2
```


## Sampling from the von Mises Distribution

### Using movMF

```{r vMsim}
library(movMF)
xy <- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5)))

# rmovMF represents samples as elements on the unit circle
x <- acos(xy[, 1]) * sign(xy[, 2])
```

. . .

### Problem {.alert}

Can we sample via the transformation method? Yes, but not easily. There is no
closed-form expression for the quantile function.

##


```{r vMhist}
#| fig-height: 2.5
#| fig-width: 5.5
#| echo: false
#| fig-cap: Histogram of samples from rmovMF()
ggplot(tibble(x = x), aes(x)) +
  geom_rug(alpha = 0.1) +
  geom_histogram(
    aes(y = ..density..),
    bins = 15,
    fill = "grey70",
    color = "white"
  ) +
  stat_function(
    aes(color = "Theoretical Density"),
    fun = function(x) exp(0.5 * cos(x + 1.5)) / (2 * pi * besselI(0.5, 0))
  ) +
  geom_density(
    aes(color = "SJ density estimate"),
    bw = "sj",
    key_glyph = "path"
  ) +
  labs(y = "Density", x = "x", color = NULL)
```

## Rejection Sampling

We want to sample from a target distribution $f(y)$ but have no readily
available transform $T$ for our proposal $g(y)$.

\medskip\pause

Let $Y_1, Y_2, \ldots$ be i.i.d. with density $g$ on $\mathbb{R}$ and $U_1, U_2,
\ldots$ be i.i.d. uniform and independent of $Y_i$ for all $i$.

\medskip\pause

Define
$$
k = \inf\left\{n \geq 1 \mid U_n \leq \alpha \frac{f(Y_n)}{g(Y_n)}\right\},
$$
for $\alpha \in (0, 1]$, where $f$ is a density.

. . .

### Theorem

If $f(y) \leq g(y)/\alpha$ for all $y \in \mathbb{R}$, then the distribution of
$Y_k$ has density $f$.

\medskip\pause

$\alpha$ is the **acceptance probability** and $g(y)/\alpha$ the **envelope** of $f$.

##

```{r rejection-sampling-ggplot}
#| echo: false
#| fig-height: 2.5
#| fig-width: 5
#| fig-cap: Illustration of rejection sampling for a normal target density,
#|   using a uniform proposal.
set.seed(1)
n <- 1000
y <- runif(n, 0, 9)
u <- runif(n, 0, 3.2)
target <- function(x) 3 * exp(-0.5 * (x - 5)^2)
envelope <- function(x) rep(3.2, length(x))

accepted <- u <= target(y)
df <- data.frame(y = y, u = u, accepted = accepted)

curve_df <- data.frame(
  x = seq(0, 9, length.out = 200),
  target = target(seq(0, 9, length.out = 200)),
  envelope = envelope(seq(0, 9, length.out = 200))
)

ggplot() +
  geom_line(
    data = curve_df,
    aes(x = x, y = envelope)
  ) +
  geom_area(
    data = curve_df,
    aes(x = x, y = target),
    col = "black",
    fill = "grey90"
  ) +
  geom_point(
    data = df,
    aes(x = y, y = u, color = accepted),
    size = 0.5
  ) +
  scale_color_manual(
    values = c("red", "steelblue2"),
    labels = c("Rejected", "Accepted")
  ) +
  labs(x = "y", y = "Density / U", color = NULL) +
  theme_minimal() +
  theme(legend.position = "right")
```

## Normalizing Constants

Rejection sampling works with unnormalized densities!

\medskip\pause

Suppose:

- Target: $f(y) = c q(y)$ (unknown $c$)
- Proposal: $g(y) = d p(y)$ (unknown $d$)

\medskip\pause

Acceptance test:
$$
U \leq \frac{\alpha f(y)}{g(y)} = \frac{\alpha c q(y)}{d p(y)}
$$

\medskip\pause

If $\alpha = \frac{\alpha' d}{c}$, then
$$
\frac{\alpha f(y)}{g(y)} = \frac{\alpha' q(y)}{p(y)}
$$

\pause

### Takeaway

You do not need to know $c$ or $d$!

## How to Find $\alpha'$?

Given functions $q$ and $p$, we need to find $\alpha'$ such that
$$
\alpha' q(y) \leq p(y) \quad \text{for all } y.
$$

\medskip\pause

### How?

We can try to solve the problem
$$
\alpha' = \inf_{y: q(y) > 0} \frac{p(y)}{q(y)} > 0.
$$

\medskip\pause

Optimally, find the minimum analytically.

\medskip\pause

Otherwise, numerically via optimization (e.g. `optimize()` in R).

## Von Mises Rejection Sampling

Rejection sampling using the uniform proposal, $g(y) \propto 1$.

\medskip\pause

Since
$$
e^{\kappa(\cos(y) - 1)} = \alpha' e^{\kappa \cos(y)} \leq 1,
$$
where $\alpha' = \exp(-\kappa)$, we reject if
$$
U > e^{\kappa(\cos(Y) - 1)}.
$$

. . .

\begin{algorithm}[H]
  \caption{Rejection sampling for the von Mises distribution}
  \Repeat{sample is accepted}{
    Generate $Y \sim \mathrm{Uniform}(-\pi, \pi)$\;
    Generate $U \sim \mathrm{Uniform}(0, 1)$\;
    \If{$U \leq e^{\kappa(\cos(Y) - 1)}$}{
      Accept $Y$\;
    }
  }
\end{algorithm}

## Implementation

```{r}
get_one_sample_vmises <- function(kappa) {
  repeat {
    Y <- runif(1, -pi, pi)
    U <- runif(1)
    if (U <= exp(kappa * (cos(Y) - 1))) {
      break
    }
  }
  Y
}
```

. . .

```{r}
sample_vonmises_slow <- function(n, kappa) {
  replicate(n, get_one_sample_vmises(kappa))
}
```


##

```{r vMsim2}
#| fig-width: 5.4
#| fig-height: 3
#| echo: false
#| fig-cap: Histogram of samples from `sample_vonmises_slow()`, with
#|   theoretical density overlaid.
f <- function(y, kappa) exp(kappa * cos(y)) / (2 * pi * besselI(kappa, 0))

# Simulate data
x1 <- sample_vonmises_slow(100000, 0.5)
x2 <- sample_vonmises_slow(100000, 2)

df <- tibble(
  x = c(x1, x2),
  kappa = factor(rep(c(0.5, 2), each = 100000))
)

ggplot(df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 30,
    fill = "grey60",
    color = "white"
  ) +
  stat_function(
    fun = f,
    args = list(kappa = 0.5),
    color = "steelblue3",
    data = data.frame(x = seq(-pi, pi, length.out = 200), kappa = 0.5)
  ) +
  stat_function(
    fun = f,
    args = list(kappa = 2),
    color = "steelblue3",
    data = data.frame(x = seq(-pi, pi, length.out = 200), kappa = 2)
  ) +
  labs(y = "Density", x = "x") +
  facet_wrap(~kappa, ncol = 2, labeller = "label_both")
```

##

```{r profile-vsim}
#| echo: false
#| eval: false
source(here::here("R", "vmises-sim-slow.R"))
profvis(sample_vonmises_slow(100000, 5))
```

![A line profile of the naive von Mises sampling
implementation.](../images/vmises-slow-profiling.png){width=82%}

## Findings from Profiling

The bottleneck is the repeated calls to `runif(1)`.

\medskip\pause

Both because of the for loop overhead, but also because calling the RNG for a
single number is inefficient.

\medskip\pause

It is faster to generate all random numbers once and store them in a vector.

\medskip\pause

But how to do that for rejection sampling with an \alert{unknown} number of
rejections?


## Vectorized (but Random Length) Rejection Sampler

We can speed up the rejection sampler by generating $m$ proposal samples at
once.

```{r vM-rejection-vec}
fixed_vonmises_sampler <- function(m, kappa) {
  Y <- runif(m, -pi, pi)
  u <- runif(m)
  accept <- u <= exp(kappa * (cos(Y) - 1))
  Y[accept]
}
```

. . .

Simple and fast, but returns a **random** number of samples!

\medskip\pause

We need a function that returns exactly $n$ samples. But we don't know $\alpha$
in advance.

##

\begin{algorithm}[H]
  \caption{Sampling exactly $n$ values via rejection sampling with unknown
    acceptance probability $\alpha$.}
  $\alpha \gets 1$\;
  $\symbf{y} \gets$ empty vector\;
  $n_\text{accepted} \gets 0$\;
  \While{$n_\text{accepted} < n$}{
    $m \gets \lceil (n - n_\text{accepted}) / \alpha \rceil$\;
    $\symbf{y}_\text{new} \gets$ Generate $m$ candidate samples $x_1, \ldots, x_m$\;
    $n_\text{accepted} \gets n_\text{accepted} +$ length$(\symbf{y}_\text{new})$\;
    \If{first batch}{
      $\alpha \gets (n_\text{accepted} + 1) / (m + 1)$ \tcp*[f]{estimate acceptance probability}
    }
    Append $\symbf{y}_\text{new}$ to $\symbf{y}$\;
  }
  \KwRet{$\symbf{y}_{1:n}$}
\end{algorithm}

## Function Factory for Rejection Sampling

```{r new_rejection_sampler}
new_rejection_sampler <- function(generator) {
  function(n, ...) {
    alpha <- 1
    y <- numeric(0)
    n_accepted <- 0
    while (n_accepted < n) {
      m <- ceiling((n - n_accepted) / alpha)
      y_new <- generator(m, ...)
      n_accepted <- n_accepted + length(y_new)
      if (length(y) == 0) {
        alpha <- (n_accepted + 1) / (m + 1)
      }
      y <- c(y, y_new)
    }
    list(x = y[seq_len(n)], alpha = alpha)
  }
}
```

##

```{r vmises-bench}
#| fig-width: 5
#| fig-height: 1.5
sample_vonmises <- new_rejection_sampler(fixed_vonmises_sampler)

bench::mark(
  sample_vonmises(1e5, 5),
  sample_vonmises_slow(1e5, 5),
  check = FALSE
) |>
  plot()
```

## Adaptive Envelopes

:::: {.columns}

::: {.column width="47%"}

When $f$ is _log-concave_ on $I$ we can construct bounds of the form
$$
f(x) \leq e^{V(x)}
$$
where
$$
V(x) = \sum_{i=1}^m  (a_i x + b_i) \symbf{1}_{I_i}(x)
$$
for intervals $I_i$ forming a partition of $I$.


:::

::: {.column width="47%"}

```{r log-concave-envelope-general}
#| echo: false
library(ggplot2)

curve_fun <- function(x) exp(-0.5 * x^2)
log_curve <- function(x) -0.5 * x^2

# Tangent points
x1 <- -1.5
x2 <- 1.5

# Compute tangent lines in log-space
slope1 <- -x1
intercept1 <- log_curve(x1) - slope1 * x1
slope2 <- -x2
intercept2 <- log_curve(x2) - slope2 * x2

xseq <- seq(-3, 3, length.out = 400)
envelope_log <- ifelse(
  xseq < 0,
  slope1 * xseq + intercept1,
  slope2 * xseq + intercept2
)
envelope <- exp(envelope_log)

df <- data.frame(
  x = xseq,
  curve = curve_fun(xseq),
  envelope = envelope
)

ggplot(df, aes(x)) +
  geom_line(aes(y = curve), color = "grey60") +
  geom_line(aes(y = envelope)) +
  geom_point(aes(x = x1, y = curve_fun(x1))) +
  geom_point(aes(x = x2, y = curve_fun(x2))) +
  scale_y_continuous(
    transform = scales::log_trans(),
    labels = scales::label_math(e^.x, format = log)
  ) +
  labs(y = "Density") +
  theme_minimal()
```

:::

::::

\bigskip\pause

Typically, $a_i x + b_i$ is tangent to the graph of $\log(f)$ at $x_i \in I_i =
(z_{i-1}, z_i]$ for
$$
z_0 < x_1 < z_1 < x_2 < \ldots < z_{m-1} < x_m < z_m.
$$


## Beta Distribution

```{r}
#| echo: false
#| fig-width: 5.5
#| fig-height: 2.5
fixed_beta_sampler <- function(m, x1, x2, alpha, beta) {
  lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x)
  lf_deriv <- function(x) (alpha - 1) / x - (beta - 1) / (1 - x)
  a1 <- lf_deriv(x1)
  a2 <- lf_deriv(x2)
  if (a1 == 0 || a2 == 0 || a1 - a2 == 0) {
    stop(
      "\nThe implementation requires a_1 and a_2 different and both different from zero. Choose different values of x_1 and x_2.",
      call. = FALSE
    )
  }
  b1 <- lf(x1) - a1 * x1
  b2 <- lf(x2) - a2 * x2
  z1 <- (b2 - b1) / (a1 - a2)
  Q1 <- exp(b1) * (exp(a1 * z1) - 1) / a1
  c <- Q1 + exp(b2) * (exp(a2 * 1) - exp(a2 * z1)) / a2

  y <- ratio <- numeric(m)
  uy <- c * runif(m)
  u <- runif(m)
  i <- uy < Q1
  y[i] <- z <- log(a1 * exp(-b1) * uy[i] + 1) / a1
  y[!i] <- zz <- log(a2 * exp(-b2) * (uy[!i] - Q1) + exp(a2 * z1)) / a2
  ratio[i] <- exp(lf(z) - a1 * z - b1)
  ratio[!i] <- exp(lf(zz) - a2 * zz - b2)
  accept <- u <= ratio
  y[accept]
}

sample_beta <- new_rejection_sampler(fixed_beta_sampler)
```

```{r Beta-fig}
#| fig-cap: Histograms of samples from an adaptive envelope sampler with theoretical
#|   densities (blue) and envelopes (orange) overlaid.
#| fig-width: 5.5
#| fig-height: 3
#| echo: false
params <- data.frame(
  x1 = c(0.3, 0.3, 0.2, 0.2),
  x2 = c(0.7, 0.7, 0.5, 0.5),
  alpha = c(4, 1.8, 4, 1.8),
  beta = c(2, 2.4, 2, 2.4),
  label = c(
    "x1=0.3, x2=0.7, alpha=4, beta=2",
    "x1=0.3, x2=0.7, alpha=1.8, beta=2.4",
    "x1=0.2, x2=0.5, alpha=4, beta=2",
    "x1=0.2, x2=0.5, alpha=1.8, beta=2.4"
  )
)

make_beta_df <- function(n, x1, x2, alpha, beta, label) {
  x <- sample_beta(n, x1 = x1, x2 = x2, alpha = alpha, beta = beta)$x
  data.frame(x = x, label = label)
}

sample_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_beta_df(100000, x1, x2, alpha, beta, label))
  })
)

xseq <- seq(0, 1, length.out = 400)

make_density_df <- function(xseq, alpha, beta, label) {
  data.frame(
    x = xseq,
    density = xseq^(alpha - 1) * (1 - xseq)^(beta - 1) / beta(alpha, beta),
    label = label
  )
}

make_envelope_df <- function(xseq, x1, x2, alpha, beta, label) {
  envelope <- function(x, x1, x2, alpha, beta) {
    lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x)
    lf_deriv <- function(x) (alpha - 1) / x - (beta - 1) / (1 - x)
    a1 <- lf_deriv(x1)
    a2 <- lf_deriv(x2)
    b1 <- lf(x1) - a1 * x1
    b2 <- lf(x2) - a2 * x2
    z1 <- (b2 - b1) / (a1 - a2)
    ifelse(x < z1, exp(a1 * x + b1), exp(a2 * x + b2))
  }
  data.frame(
    x = xseq,
    envelope = envelope(xseq, x1, x2, alpha, beta) / beta(alpha, beta),
    label = label
  )
}

dens_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_density_df(xseq, alpha, beta, label))
  })
)
env_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_envelope_df(xseq, x1, x2, alpha, beta, label))
  })
)

ggplot(sample_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 50,
    fill = "grey80"
  ) +
  geom_line(
    data = dens_df,
    aes(x = x, y = density),
    color = "steelblue3"
  ) +
  geom_line(
    data = env_df,
    aes(x = x, y = envelope),
    color = "dark orange"
  ) +
  ylim(0, 3) +
  facet_wrap(~label, ncol = 2)
```

##

```{r beta-envelopes}
#| echo: false
#| warning: false
#| fig-width: 5.5
#| fig-height: 3
#| fig-cap: Theoretical densities (blue) and envelopes (orange) for
#|   different parameter values and choices of $x_1$ and $x_2$.
params <- data.frame(
  alpha = c(4, 1.8),
  beta = c(2, 2.4),
  x1 = c(0.3, 0.2),
  x2 = c(0.7, 0.5),
  label = c(
    "alpha=4, beta=2, x1=0.3, x2=0.7",
    "alpha=1.8, beta=2.4, x1=0.2, x2=0.5"
  )
)

xseq <- seq(0.05, 0.95, length.out = 400)

make_df <- function(xseq, alpha, beta, x1, x2, label) {
  envelope_fun <- function(x, x1, x2, alpha, beta) {
    lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x)
    lf_deriv <- function(x) (alpha - 1) / x - (beta - 1) / (1 - x)
    a1 <- lf_deriv(x1)
    a2 <- lf_deriv(x2)
    b1 <- lf(x1) - a1 * x1
    b2 <- lf(x2) - a2 * x2
    z1 <- (b2 - b1) / (a1 - a2)
    ifelse(x < z1, exp(a1 * x + b1), exp(a2 * x + b2))
  }
  data.frame(
    x = xseq,
    density = xseq^(alpha - 1) * (1 - xseq)^(beta - 1) / beta(alpha, beta),
    envelope = envelope_fun(xseq, x1, x2, alpha, beta) / beta(alpha, beta),
    label = label
  )
}

plot_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_df(xseq, alpha, beta, x1, x2, label))
  })
)

ggplot(plot_df, aes(x = x)) +
  geom_line(aes(y = density), color = "steelblue3") +
  geom_line(aes(y = envelope), color = "dark orange") +
  scale_y_log10() +
  facet_wrap(~label, ncol = 2)
```

## Von Mises Adaptive Envelopes

:::: {.columns}

::: {.column width="47%"}

The von Mises density is unfortunately not globally log-concave.

\medskip\pause

But, it *is* 

- log-concave on $(-\pi, \pi/2)$ and $(\pi/2, \pi)$ and
- log-convex on $(-\pi, -\pi/2)$ and $(-\pi/2, \pi/2)$.

:::

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: The von Mises density with $\kappa = 0.5$.
theta <- 0.5 * c(cos(-1.5), sin(-1.5))

dvm <- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0))

xy <- tibble(
  x = seq(-pi, pi, length.out = 400),
  y = log(dvm(x, 0.5))
)

a <- seq(-2 * pi, 2 * pi, length.out = 100)

ggplot(xy, aes(x = x, y = y)) +
  geom_vline(
    xintercept = c(-pi, -pi / 2, pi / 2, pi),
    color = "grey60"
  ) +
  geom_line() +
  labs(y = "log-density") +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
```

:::

::::

##

```{r vonmises-adaptive}
#| echo: false
dvm <- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0))

von_mises_adapt_random <- function(N, x1, x2, kappa) {
  lf <- function(x) kappa * cos(x)
  lf_deriv <- function(x) -kappa * sin(x)
  a1 <- 2 * kappa / pi
  a2 <- lf_deriv(x1)
  a3 <- lf_deriv(x2)
  a4 <- -a1

  b1 <- kappa
  b2 <- lf(x1) - a2 * x1
  b3 <- lf(x2) - a3 * x2
  b4 <- kappa

  z0 <- -pi
  z1 <- -pi / 2
  z2 <- (b3 - b2) / (a2 - a3)
  z3 <- pi / 2
  z4 <- pi

  Q1 <- exp(b1) * (exp(a1 * z1) - exp(a1 * z0)) / a1
  Q2 <- Q1 + exp(b2) * (exp(a2 * z2) - exp(a2 * z1)) / a2
  Q3 <- Q2 + exp(b3) * (exp(a3 * z3) - exp(a3 * z2)) / a3
  c <- Q3 + exp(b4) * (exp(a4 * z4) - exp(a4 * z3)) / a4

  u0 <- c * runif(N)
  u <- runif(N)

  I1 <- (u0 < Q1)
  I2 <- (u0 >= Q1) & (u0 < Q2)
  I3 <- (u0 >= Q2) & (u0 < Q3)
  I4 <- (u0 >= Q3)

  x <- numeric(N)
  accept <- logical(N)
  x[I1] <- log(a1 * exp(-b1) * u0[I1] + exp(a1 * z0)) / a1
  accept[I1] <- u[I1] <= exp(lf(x[I1]) - a1 * x[I1] - b1)
  x[I2] <- log(a2 * exp(-b2) * (u0[I2] - Q1) + exp(a2 * z1)) / a2
  accept[I2] <- u[I2] <= exp(lf(x[I2]) - a2 * x[I2] - b2)
  x[I3] <- log(a3 * exp(-b3) * (u0[I3] - Q2) + exp(a3 * z2)) / a3
  accept[I3] <- u[I3] <= exp(lf(x[I3]) - a3 * x[I3] - b3)
  x[I4] <- log(a4 * exp(-b4) * (u0[I4] - Q3) + exp(a4 * z3)) / a4
  accept[I4] <- u[I4] <= exp(lf(x[I4]) - a4 * x[I4] - b4)

  x[accept]
}

von_mises_adapt <- new_rejection_sampler(von_mises_adapt_random)

envelope <- function(x, x1, x2, kappa) {
  lf <- function(x) kappa * cos(x)
  lf_deriv <- function(x) -kappa * sin(x)
  a1 <- 2 * kappa / pi
  a2 <- lf_deriv(x1)
  a3 <- lf_deriv(x2)
  a4 <- -a1

  b1 <- kappa
  b2 <- lf(x1) - a2 * x1
  b3 <- lf(x2) - a3 * x2
  b4 <- kappa

  z0 <- -pi
  z1 <- -pi / 2
  z2 <- (b3 - b2) / (a2 - a3)
  z3 <- pi / 2
  z4 <- pi

  env <- numeric(length(x))
  i1 <- x < z1
  i2 <- x >= z1 & x < z2
  i3 <- x >= z2 & x < z3
  i4 <- x >= z3

  env[i1] <- exp(a1 * x[i1] + b1)
  env[i2] <- exp(a2 * x[i2] + b2)
  env[i3] <- exp(a3 * x[i3] + b3)
  env[i4] <- exp(a4 * x[i4] + b4)

  env
}
```

```{r vonmises-adaptive-plot}
#| fig-cap: Histograms of samples from an adaptive envelope sampler for the von Mises distribution,
#|   with theoretical densities (blue) and envelopes (orange) overlaid.
#| fig-width: 5.5
#| fig-height: 3
#| echo: false
params <- data.frame(
  x1 = c(-0.4, -0.1, -0.4, -1),
  x2 = c(0.4, 0.1, 0.4, 1),
  kappa = c(5, 5, 2, 2),
  label = c(
    "kappa=5, x1=-0.4, x2=0.4",
    "kappa=5, x1=-0.1, x2=0.1",
    "kappa=2, x1=-0.4, x2=0.4",
    "kappa=2, x1=-1, x2=1"
  )
)

make_vm_df <- function(n, x1, x2, kappa, label) {
  x <- von_mises_adapt(n, x1, x2, kappa)$x
  data.frame(x = x, label = label)
}

sample_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_vm_df(100000, x1, x2, kappa, label))
  })
)

xseq <- seq(-pi, pi, length.out = 400)

make_density_df <- function(xseq, kappa, label) {
  data.frame(
    x = xseq,
    density = dvm(xseq, kappa),
    label = label
  )
}

make_envelope_df <- function(xseq, x1, x2, kappa, label) {
  env <- envelope(xseq, x1, x2, kappa) / (2 * pi * besselI(kappa, 0))
  data.frame(
    x = xseq,
    envelope = env,
    label = label
  )
}

dens_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_density_df(xseq, kappa, label))
  })
)
env_df <- do.call(
  rbind,
  lapply(seq_len(nrow(params)), function(i) {
    with(params[i, ], make_envelope_df(xseq, x1, x2, kappa, label))
  })
)

ggplot(sample_df, aes(x)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 30,
    fill = "grey80"
  ) +
  geom_line(
    data = dens_df,
    aes(x = x, y = density),
    color = "steelblue3"
  ) +
  geom_line(
    data = env_df,
    aes(x = x, y = envelope),
    color = "dark orange"
  ) +
  ylim(0, 1.5) +
  facet_wrap(~label, ncol = 2)
```

## Caveats about Adaptive Envelopes

Adaptive envelopes is a general and powerful technique, but tricky to implement! 

\medskip\pause

Efficiency depends on number of rejections and also quality of implementation.

\medskip\pause

A good implementation should make use of new points to update envelope.

## Transformation Methods vs. Rejection Sampling

:::: {.columns}

::: {.column width="47%"}

### Transformation Methods

Transform samples from a simple distribution to the target distribution.

\medskip\pause

Efficient if the transformation is available
and cheap to compute.

\medskip\pause

But may not work for complex or unnormalized distributions!

:::


. . .


::: {.column width="47%"}

### Rejection Sampling

Uses a proposal distribution and accepts/rejects samples.

\medskip\pause

Works even if quantile function is unavailable or expensive.

\medskip\pause

Often less efficient, but very general.

\medskip\pause

Useful for complex, high-dimensional, or unnormalized distributions.

:::

::::

. . .


### Takeaway {.example}

Use transformation methods when possible for efficiency; use rejection sampling
when transformation is impractical or impossible.

## Exercises

### Step 1

Use the fact that 
$$
W = X_1^2 + \ldots + X_k^2 \sim \chi^2_k \quad \text{for} \quad X_1, \ldots, X_k \sim_{\text{i.i.d.}} \mathcal{N}(0, 1)
$$
to implement a function, `my_rchisq()`, such that
`my_rchisq(n, k)` returns a sample of $n$ i.i.d. observations from $\chi^2_k$.

```{r rchisq2-implementation, include = FALSE}
my_rchisq <- function(n, k) {
  y <- numeric(n)
  for (i in seq_len(n)) {
    x <- rnorm(k)
    y[i] <- sum(x^2)
  }
  y
}
```

```{r rchisq2-implementation-faster, include = FALSE}
my_fast_rchisq <- function(n, k) {
  x <- matrix(rnorm(k * n), k)
  colSums(x)
}
```

. . .

### Step 2

Make another function `my_other_rchisq()` that uses inverse sampling.

```{r rchisq2, include = FALSE}
my_other_rchisq <- function(n, k) {
  x <- rnorm(n)
  qchisq(x, k)
}
```

. . .

### Step 3

Benchmark your implementations against one another and `rchisq()`.

```{r rchisq-bench, include = FALSE}
n <- 1e4
k <- 5
bench::mark(
  rchisq(n, k),
  my_rchisq(n, k),
  my_fast_rchisq(n, k),
  my_other_rchisq(n, k),
  check = FALSE
) |>
  plot()
```
