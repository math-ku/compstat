---
title: "Monte Carlo Methods"
---

{{< include _common.qmd >}}

```{r setup-common2}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
theme_set(theme_minimal(base_size = 10))
options(width = 60)
```

```{r setup, include=FALSE}
library(patchwork)

old_options <- options(digits = 4)
```

## Rejection Sampling Recap

Suppose that $f(y) \propto q(y)$ is the **target** density, known up to
normalization.

\medskip\pause

Suppose that $g(y) \propto p(y)$ is another density, known up to normalization,
that we can simulate from, and suppose
$$
\alpha' q \leq p
$$
for some $\alpha' > 0$.

\medskip\pause

If $Y$ has density $g$ and $U$ is uniform on $(0,1)$ then the conditional
distribution
$$
Y \mid U \leq \alpha' q(Y) / p(Y)
$$
has density $f$.

## Basic Algorithm

To simulate one variable with density $f$, run the following code until it
returns a value:

. . .

- Simulate $y$ from $g$
- Simulate $u$ from $\operatorname{Uniform}(0,1)$
- If $u > \alpha' q(y) / p(y)$ return to step one
- Else return $y$

. . .

### Two Challenges

- Finding an envelope $p/\alpha'$, of $q$.
- Finding a **tight-enough** envelope.

## Today

### Monte Carlo Methods

General class of methods for estimating integrals and means

\medskip\pause

Rejection sampling is a type of Monte Carlo method.

. . .

### Importance Sampling

Focus on integrating functions and estimating means

\medskip\pause

Useful when we cannot analytically compute the integral

\medskip\pause

Variance reduction technique

## Monte Carlo Methods

### Basic, Simple Idea

Use random sampling to obtain numerical results

\medskip\pause

Useful when analytical solutions are infeasible

. . .

### Monte Carlo Integration

With $X_1, \ldots, X_n$ i.i.d. with density $f$
$$
\hat{\mu}_{\textrm{MC}} := \frac{1}{n} \sum_{i=1}^n h(X_i) \rightarrow \mu := \mathbb{E} h(X_1) = \int h(x) f(x) \ dx
$$
for $n \to \infty$ by the law of large numbers (LLN).

## Monte Carlo Integration

By the central limit theorem (CLT), we know that
$$
\frac{1}{n} \sum_{i=1}^n h(X_i) \overset{\textrm{approx}} \sim
\mathcal{N}(\mu, \sigma^2_{\textrm{MC}} / n)
$$
where
$$
\sigma^2_{\textrm{MC}} = \mathbb{V} h(X_1) = \int (h(x) - \mu)^2 f(x) \ dx.
$$

## Monte Carlo Integration

We can estimate $\sigma^2_{\textrm{MC}}$ using the empirical variance
$$
\hat{\sigma}^2_{\textrm{MC}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i) - \hat{\mu}_{\textrm{MC}})^2,
$$
then the variance of $\hat{\mu}_{\textrm{MC}}$ is estimated as
$\hat{\sigma}^2_{\textrm{MC}} / n$ and a standard 95% confidence interval for
$\mu$ is
$$
\hat{\mu}_{\textrm{MC}} \pm 1.96 \frac{\hat{\sigma}_{\textrm{MC}}}{\sqrt{n}}.
$$

## Example: Gamma Distribution

:::: {.columns}

::: {.column width="47%"}

We want to estimate the mean of a $\operatorname{Gamma}(8,1)$ distribution.

. . .

```{r gamma-sim, echo=-1}
set.seed(1234)
B <- 1000
x <- rgamma(B, 8)

mu_hat <- (cumsum(x) / (1:B))
sigma_hat <- sd(x)
mu_hat[B] # Theoretical: 8
sigma_hat # Theoretical: âˆš8
```

:::

::: {.column width="47%"}

```{r gamma-fig, echo = FALSE}
#| fig-cap: "Density of a Gamma(8,1) distribution with mean indicated."
x <- seq(0, 20, length.out = 100)
y <- dgamma(x, 8)

tibble(x, y) |>
  ggplot(aes(x, y)) +
  geom_line() +
  geom_vline(xintercept = 8, linetype = "dashed", color = "steelblue") +
  annotate("text", x = 7, y = 0.02, label = "E(X)", hjust = -0.5)
```

:::

::::

##

```{r gamma-fig1, fig.height = 3, fig.width=5}
#| echo: false
#| fig-cap: "Convergence of the Monte Carlo estimate of the mean of a Gamma(8,1) distribution."
p <- ggplot(tibble(x = 1:B, y = mu_hat), aes(x, y)) +
  geom_hline(yintercept = 8, linetype = "dashed", col = "steelblue4") +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10))

p +
  geom_ribbon(
    aes(
      ymin = mu_hat - 1.96 * sigma_hat / sqrt(1:B),
      ymax = mu_hat + 1.96 * sigma_hat / sqrt(1:B)
    ),
    fill = "dark orange",
    alpha = 0.4
  ) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10))
```

## When to Stop?

Can use concentration inequalities to get a stopping rule

\medskip\pause

But need variance estimate for Chebychev-based bound and moment generating
functions for Chernoff-based bounds.

\medskip\pause

If you overshoot then you waste computer resources, and if you undershoot then
you get a bad estimate.

# Importance Sampling

## Importance Sampling

When we are only interested in Monte Carlo integration, we do not need to sample
from the target distribution.

. . .

### Basic Idea

- Usually, some regions of the target distribution are more important
- So let's oversample these regions
- But we need to correct for this oversampling

\medskip\pause

We want to find $\mu = \operatorname{E} h(X)$. Observe that
$$
\mu = \int h(x) f(x) \ dx = \int h(x) \frac{f(x)}{g(x)} g(x) \ dx = \int h(x) w^*(x) g(x) \ dx
$$
whenever $g$ is a density fulfilling that
$$
g(x) = 0 \Rightarrow f(x) = 0.
$$

## Weights

With $X_1, \ldots, X_n$ i.i.d. with density $g$ define the _weights_
$$
w^*(X_i) = \frac{f(X_i)}{g(X_i)}.
$$

. . .

- $f/g$ is called the _likelihood ratio_.
- $f$ is the _nominal_ density
- $g$ is the _importance_ density

. . .

### Estimator

$$
\hat{\mu}_{\textrm{IS}}^* := \frac{1}{n} \sum_{i=1}^n h(X_i)w^*(X_i).
$$

It has mean $\mu$.

##

```{r}
#| echo: false
#| fig-cap: "Target (dashed blue) and proposal (orange) densities
#|  with corresponding importance weights."
#| fig-width: 5
#| fig-height: 3
x <- seq(-5, 5, length.out = 200)
target <- dnorm(x, mean = 0, sd = 1)
proposal <- dnorm(x, mean = 1, sd = 1)

p1 <- tibble(x, target, proposal) |>
  pivot_longer(
    cols = c(target, proposal),
    names_to = "Density",
    values_to = "value"
  ) |>
  ggplot(aes(x, value, color = Density, linetype = Density)) +
  geom_line() +
  scale_color_manual(
    values = c(target = "steelblue", proposal = "darkorange")
  ) +
  scale_linetype_manual(values = c(target = "dashed", proposal = "solid")) +
  theme(legend.position.inside = c(0.87, 0.82)) +
  guides(
    color = guide_legend(position = "inside"),
    linetype = guide_legend(position = "inside")
  ) +
  labs(
    y = "Density",
    x = "x",
    color = "Distribution",
    linetype = "Distribution"
  )

w <- target / proposal

p2 <- tibble(x, w) |>
  ggplot(aes(x, w)) +
  geom_line() +
  labs(
    x = "x",
    y = "Weight"
  ) +
  theme_minimal(base_size = 10)

p1 / p2 + plot_layout(heights = c(2, 1), axes = "collect")
```

## Gamma Importance Sampling

:::: {.columns}

::: {.column width="47%"}

```{r gamma-sim-IS, echo=-1}
set.seed(1234)
B <- 1000
x <- rnorm(B, 10, 3)
w_star <-
  dgamma(x, 8) / dnorm(x, 10, 3)
mu_hat_IS <-
  cumsum(x * w_star) / (1:B)
mu_hat_IS[B]
```

Close to theoretical value (8).

:::

. . .

::: {.column width="47%"}

```{r gammasim-fig, echo = FALSE}
#| fig-height: 3.2
#| fig-width: 2.7
z <- seq(-12, 12, length.out = 100) + 10
f_x <- dgamma(z, 8)
g_x <- dnorm(z, 10, 3)
w <- f_x / g_x

p1 <- tibble(x = z, f_x, g_x) |>
  pivot_longer(c(f_x, g_x)) |>
  ggplot(aes(x, value, color = name)) +
  geom_line() +
  guides(color = guide_legend(position = "inside")) +
  theme(legend.position.inside = c(0.8, 0.8)) +
  labs(y = "Density", color = NULL)

p2 <- tibble(x = z, w) |>
  ggplot(aes(x, w)) +
  geom_line()

p1 / p2 + plot_layout(axes = "collect")
```

:::

::::

##

```{r gamma-IS-fig1, echo = FALSE}
#| fig-cap: "Convergence of the importance sampling estimate of the mean of a
#|   Gamma(8,1) distribution using a N(10,3) proposal."
#| fig-width: 5
#| fig-height: 2.5
p <- ggplot(
  tibble(x = 1:B, y = mu_hat_IS),
  aes(x, y)
) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10)) +
  geom_hline(yintercept = 8, linetype = "dashed", col = "steelblue4")
p
```

## Importance Sampling Variance

Again by the LLN
$$
\hat{\mu}_{\textrm{IS}}^* \rightarrow \mathbb{E} h(X_1) w^*(X_1) = \mu,
$$
and by the CLT
$$
\hat{\mu}_{\textrm{IS}}^* \overset{\textrm{approx}} \sim
\mathcal{N}(\mu, \sigma^{*2}_{\textrm{IS}} / n)
$$
where
$$
\sigma^{*2}_{\textrm{IS}} = \mathbb{V} h(X_1)w^*(X_1) = \int (h(x) w^*(x) - \mu)^2 g(x) \ dx.
$$

## Importance Sampling Variance

The importance sampling variance can be estimated just as the MC variance
$$
\hat{\sigma}^{*2}_{\textrm{IS}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i)w^*(X_i) - \hat{\mu}_{\textrm{IS}}^*)^2,
$$

. . .

And a 95% standard confidence interval is
$$
\hat{\mu}^*_{\textrm{IS}} \pm 1.96 \frac{\hat{\sigma}^*_{\textrm{IS}}}{\sqrt{n}}.
$$

\medskip\pause

We may have $\sigma^{*2}_{\textrm{IS}} > \sigma^2_{\textrm{MC}}$ or
$\sigma^{*2}_{\textrm{IS}} < \sigma^2_{\textrm{MC}}$ depending on $h$ and $g$.

. . .

### Goal

Choose $g$ so that $h(x) w^*(x)$ becomes as constant as possible.

## Gamma Importance Sampling

```{r gamma-IS-sd, echo=TRUE}
sigma_hat_IS <- sd(x * w_star)
sigma_hat_IS # Theoretical value?
```

. . .

```{r gamma-IS-fig2, echo=FALSE}
#| fig-width: 4.5
#| fig-cap: "Importance sampling estimate of the mean of a Gamma(8,1)."
p +
  geom_ribbon(
    aes(
      ymin = mu_hat_IS - 1.96 * sigma_hat_IS / sqrt(1:B),
      ymax = mu_hat_IS + 1.96 * sigma_hat_IS / sqrt(1:B)
    ),
    fill = "gray"
  ) +
  geom_line() +
  geom_point(size = 0.5)
```

```{r test, echo=FALSE, eval=FALSE}
mu_hat_IS <- replicate(10000, {
  x <- rnorm(B, 10, 3)
  w_star <- dgamma(x, 8) / dnorm(x, 10, 3)
  mean(x * w_star)
})
sd(mu_hat_IS) * sqrt(B)
```

## Standardized Weights

If $f = c^{-1} q$ with $c$ unknown then
$$
c = \int q(x) dx = \int \frac{q(x)}{g(x)} g(x) d x.
$$

. . .

And
$$
\mu = \frac{\int h(x) w^*(x) g(x) \ d x}{\int w^*(x) g(x) \ d x},
$$
where
$$
w^*(x) = \frac{q(x)}{g(x)}.
$$

## IS with Standardized Weights

An importance sampling estimate of $\mu$ is thus
$$
\begin{aligned} \hat{\mu}_{\textrm{IS}} & = \frac{\sum_{i=1}^n h(X_i) w^*(X_i)}{\sum_{i=1}^n w^*(X_i)} \\ \pause
                                        & = \sum_{i=1}^n h(X_i) w(X_i),
\end{aligned}
$$
where $w^*(X_i) = q(X_i) / g(X_i)$ and
$$
w(X_i) = \frac{w^*(X_i)}{\sum_{i=1}^n w^*(X_i)}
$$
are the _standardized weights_.

\medskip\pause

Works irrespectively of the value of the normalizing constant $c$, and also if
an unnormalized $g$ is used.

## Gamma Standardized Weights

```{r gamma-sim-ISw, echo=-1}
set.seed(1234)
B <- 1000
x <- rnorm(B, 10, 3)
w_star <- numeric(B)
x_pos <- x[x > 0]
w_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))

mu_hat_IS <- cumsum(x * w_star) / cumsum(w_star)
mu_hat_IS[B] # Theoretical value 8
```

##

```{r gamma-ISw-fig1, echo = FALSE}
#| fig-width: 4.5
#| fig-height: 3
#| fig-cap: "Convergence of the importance sampling estimate of the mean of a
#|   Gamma(8,1) distribution using standardized weights."
p <- tibble(x = 1:B, y = mu_hat_IS) |>
  ggplot(aes(x, y)) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10)) +
  labs(y = expression(hat(mu)[IS]), x = "Number of samples")
p
```

## Importance Sampling Variance

The variance of the IS estimator with standardized weights is a little more
complicated, because the estimator is a \alert{ratio of random variables}.

\medskip\pause

From the multivariate CLT, we have
$$
\frac{1}{n} \sum_{i=1}^n \begin{bmatrix} h(X_i) w^*(X_i) \\ w^*(X_i) \end{bmatrix}
\overset{\textrm{approx}}{\sim}
\mathcal{N}\left(
  c \begin{bmatrix} \mu \\ 1\end{bmatrix},
  \frac{1}{n} \begin{bmatrix} \sigma^{2}_{\textrm{IS}} & \gamma \\ \gamma & \sigma^2_{w^*} \end{bmatrix}
\right),
$$
where
$$
\begin{aligned}
  \sigma^2_{\textrm{IS}} & = \mathbb{V}(h(X_1) w^*(X_1)) \\
  \gamma & = \mathrm{cov}(h(X_1) w^*(X_1), w^*(X_1)) \\
  \sigma^2_{w^*} & = \mathbb{V}(w^*(X_1))
\end{aligned}
$$

## Importance Sampling Variance

We can then apply the $\Delta$-method with $t(x, y) = x / y$.

\medskip\pause

Observe that $\nabla t(x, y) = \begin{bmatrix}1 / y \\ - x / y^2\end{bmatrix}$,
whence

$$
\nabla t(c\mu, c)^T \left(\begin{array}{cc} \hat{\sigma}^{2}_{\textrm{IS}} & \gamma \\ \gamma & \sigma^2_{w^*}
\end{array} \right) \nabla t(c\mu, c) = \frac{\sigma^{2}_{\textrm{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma}{c^{2}}.
$$

. . .

By the $\Delta$-method,
$$
\hat{\mu}_{\textrm{IS}} \overset{\textrm{approx}}{\sim}
\mathcal{N}\left(\mu, \frac{\sigma^{2}_{\textrm{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma}{c^2 n} \right).
$$

. . .

For $c = 1$, the asymptotic variance can be estimated by plugging in empirical
estimates. For $c \neq 1$ it is necessary to estimate $c$ as
$$
\hat{c} = \frac{1}{n} \sum_{i=1}^n w^*(X_i)
$$

##

```{r delta-method-plot3D}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-cap: Tangent plane approximation of the function $t(x,y) = x/y$ at the point
#|   $(2,3)$.
library(plot3D)

# Define function and means
t_fun <- function(x, y) x / y
mu_x <- 2
mu_y <- 3

# Grid for surface
x_seq <- seq(1, 3, length.out = 26)
y_seq <- seq(2, 4, length.out = 26)
z <- outer(x_seq, y_seq, t_fun)

# Tangent plane at (mu_x, mu_y)
grad_x <- 1 / mu_y
grad_y <- -mu_x / mu_y^2
tangent_plane <- function(x, y) {
  t_fun(mu_x, mu_y) + grad_x * (x - mu_x) + grad_y * (y - mu_y)
}
z_tangent <- outer(x_seq, y_seq, tangent_plane)

# Plot both surfaces together
persp3D(
  x_seq,
  y_seq,
  z,
  col = "steelblue",
  xlab = "x",
  ylab = "y",
  zlab = "t(x, y)",
  theta = 30,
  phi = 10,
  border = "black"
)

# Add tangent plane
persp3D(
  x_seq,
  y_seq,
  z_tangent,
  add = TRUE,
  col = "grey",
  alpha = 0.6
)

# Add mean point
points3D(
  mu_x,
  mu_y,
  t_fun(mu_x, mu_y),
  col = "darkorange",
  pch = 19,
  add = TRUE,
  cex = 1
)
```

## Gamma Standardized Weights, Variance

```{r sigma_hat_is_w}
c_hat <- mean(w_star)
sigma_hat_IS <- sd(x * w_star)
sigma_hat_w_star <- sd(w_star)
gamma_hat <- cov(x * w_star, w_star)
sigma_hat_IS_w <- sqrt(
  sigma_hat_IS^2 +
    mu_hat_IS[B]^2 * sigma_hat_w_star^2 -
    2 * mu_hat_IS[B] * gamma_hat
) /
  c_hat
sigma_hat_IS_w
```

```{r test-2, echo=FALSE, eval=FALSE}
mu_hat_IS <- replicate(10000, {
  x <- rnorm(B, 10, 3)
  w_star <- numeric(B)
  x_pos <- x[x > 0]
  w_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))
  sum(x * w_star) / sum(w_star)
})
sd(mu_hat_IS) * sqrt(B)
```

##

```{r gamma-ISw-fig2}
#| echo: false
#| fig-width: 4.5
#| fig-cap: "Importance sampling estimate of the mean of a Gamma(8,1)
#|   distribution with 95% confidence bands using standardized weights."
p +
  geom_ribbon(
    aes(
      ymin = mu_hat_IS - 1.96 * sigma_hat_IS_w / sqrt(1:B),
      ymax = mu_hat_IS + 1.96 * sigma_hat_IS_w / sqrt(1:B)
    ),
    fill = "gray"
  ) +
  geom_line() +
  geom_point(size = 0.5)
```

## When to Use Importance Sampling?

- **Rare Event Estimation:** When the event of interest has low probability
  under the target distribution, importance sampling can efficiently estimate
  probabilities or expectations.
- **High Variance Monte Carlo:** If standard Monte Carlo methods yield high
  variance, importance sampling can reduce variance by focusing samples where
  the integrand is large.
- **Difficult-to-Sample Targets:** When direct sampling from the target
  distribution is hard, but sampling from a related proposal is easy.
- **Integrals with Heavy Tails:** For integrals dominated by regions with low
  probability under the target, importance sampling can improve accuracy.
- **Bayesian Computation:** Useful for marginal likelihood estimation and
  posterior expectations when the posterior is complex.

## Poor Proposals

If the proposal distribution does not cover regions where the target is large,
weights become highly variable and estimates unreliable.


\medskip\pause

### Remedies

Redesign proposal to better match the target.

\medskip\pause

Use adaptive or mixture proposals.

\medskip\pause

Increase sample size (if feasible).


## Diagnostics for Importance Sampling

### Weight Distribution

A histogram of weights visualizes the distribution of importance weights. This
reveals weight degeneracy, where many weights are near zero and a few are very
large.

\medskip\pause

### Effective Sample Size (ESS)

Effective sample size (ESS) measures how many samples are effectively
contributing to the estimate. A low ESS indicates poor sampling efficiency and
high variance.

\medskip\pause

### Comparison with Standard Monte Carlo

Comparing importance sampling estimates and variances with standard Monte Carlo
shows whether importance sampling actually improves efficiency.

## Weight Degeneracies

:::: {.columns}

::: {.column width="47%"}

Suppose $X \sim \mathcal{N}(0, I_{10})$ and we want to estimate
$\mathbb{E}[f(X)]$ where $f(X)$ is large near the origin.

\medskip\pause

If we use a proposal $g(x) = \mathcal{N}(\mu, I_{10})$ with large $|\mu|$, most
samples are far from the origin, so $f(X)$ is nearly zero and weights are highly
variable.

:::

::: {.column width="47%"}


```{r poor-proposal-demo}
#| echo: false
#| fig-height: 2.3
#| fig-cap: "Target (blue) and proposal (orange) densities
#|   with samples from the proposal."
library(mvtnorm)

set.seed(1)
d <- 2
n <- 1000
mu <- rep(2, d) # Poor proposal: mean far from origin
x <- matrix(rnorm(n * d, mean = mu), ncol = d)

colnames(x) <- c("x1", "x2")

grid <- expand.grid(
  x1 = seq(-5, 8, length.out = 100),
  x2 = seq(-5, 8, length.out = 100)
)
grid_mat <- as.matrix(grid)

grid$target <- dmvnorm(grid_mat, mean = rep(0, d))
grid$proposal <- dmvnorm(grid_mat, mean = mu)

tibble(
  x1 = grid$x1,
  x2 = grid$x2,
  target = grid$target,
  proposal = grid$proposal
) |>
  ggplot(aes(x1, x2)) +
  geom_contour(aes(z = target), color = "steelblue", linetype = "dashed") +
  geom_contour(aes(z = proposal), color = "darkorange") +
  geom_point(aes(x1, x2), data = as_tibble(x), alpha = 0.3, size = 0.5) +
  labs(
    x = expression(X[1]),
    y = expression(X[2])
  ) +
  coord_fixed() +
  theme_minimal(base_size = 10)
```

:::

::::

##

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| echo: false
#| fig-cap: "Histogram of importance weights showing weight degeneracy."
target_density <- function(x) dmvnorm(x, mean = rep(0, d))
proposal_density <- function(x) dmvnorm(x, mean = mu)
f <- function(x) exp(-sum(x^2) / 2) # Large near origin

weights <- apply(x, 1, function(row) {
  target_density(row) / proposal_density(row)
})
values <- apply(x, 1, f)
estimate <- sum(values * weights) / sum(weights)

ggplot(tibble(weights = weights), aes(weights)) +
  geom_histogram() +
  labs(
    x = "Importance Weights",
    y = "Frequency"
  )
```

Most weights are near zero, with a few large weights.

\medskip\pause

\pdfpcnote{
This indicates weight degeneracy and poor sampling efficiency.
}

## Effective Sample Size (ESS)

The effective sample size (ESS) quantifies how many samples effectively
contribute to the estimate.

\medskip\pause

It is defined as
$$
\text{ESS} = \frac{(\sum_{i=1}^n w_i)^2}{\sum_{i=1}^n w_i^2}
$$
where $w_i$ are the importance weights.

\medskip\pause

If all weights are equal, $\text{ESS} = n$. If one weight dominates, $\text{ESS}
\approx 1$, and the estimate is unreliable.

. . .

```{r ess-demo}
ess <- (sum(weights))^2 / sum(weights^2)
ess
```

## Example: Network Failure

![A network graph. Different edges "fail" independently with probability
$p$.](../images/networkfig.png){ width=70% }

. . .

**Problem:** What is the probability that nodes 1 and 10 are disconnected?

## Representing Graphs

We can represent a graph using an adjacency matrix.

```{r network_adj, echo=13}
options(width = 65)
A <- matrix(0, 10, 10)
A[1, c(2, 4, 5)] <- 1
A[2, c(1, 3, 6)] <- 1
A[3, c(2, 6, 7, 8, 10)] <- 1
A[4, c(1, 5, 8)] <- 1
A[5, c(1, 4, 8, 9)] <- 1
A[6, c(2, 3, 7, 10)] <- 1
A[7, c(3, 6, 10)] <- 1
A[8, c(3, 4, 5, 9)] <- 1
A[9, c(5, 8, 10)] <- 1
A[10, c(3, 6, 7, 9)] <- 1
A # Graph adjacency matrix
```

. . .

```{r}
Aup <- A
Aup[lower.tri(Aup)] <- 0
```

## Check Connectivity

```{r discon-def}
discon <- function(Aup) {
  A <- Matrix::forceSymmetric(Aup, "U")
  i <- 3
  Apow <- A %*% A %*% A # A^3

  while (Apow[1, 10] == 0 && i < 9) {
    Apow <- Apow %*% A
    i <- i + 1
  }

  Apow[1, 10] == 0 # TRUE if nodes 1 and 10 are not connected
}
```

## Sampling a Graph

```{r simNet, cache = TRUE}
sim_net <- function(Aup, p) {
  ones <- which(Aup == 1)
  Aup[ones] <- sample(
    c(0, 1),
    length(ones),
    replace = TRUE,
    prob = c(p, 1 - p)
  )
  Aup
}
```

. . .

```{r sim-bench, cache = TRUE, dependson = "simNet"}
bench::bench_time(replicate(1e5, sim_net(Aup, 0.5)))
```

## Estimating Probability of Nodes 1 and 10 Disconnected

```{r sim, cache = TRUE}
set.seed(27092016)
n <- 1e5
tmp <- replicate(n, discon(sim_net(Aup, 0.05)))

mu_hat <- mean(tmp)
mu_hat
```

. . .

Estimate with confidence interval using $\sigma^2 = \mu (1 - \mu)$.

```{r sim-confint, dependson = "sim"}
mu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)
```

## Importance Sampling

We will simulate with failure probability $p_0$ and compute the importance
weights.

```{r impw, cache = TRUE}
weights <- function(Aup, Aup0, p0, p) {
  w <- discon(Aup0)

  if (w) {
    s <- sum(Aup0)
    w <- (p / p0)^18 * (p0 * (1 - p) / (p * (1 - p0)))^s
  }

  as.numeric(w)
}
```

. . .

For the IS estimator the weights will be multiplied by the indicator that 1 and
10 are disconnected.

## Estimating Probability of Nodes 1 and 10 Disconnected

```{r sim2, cache = TRUE, dependson = "impw"}
tmp <- replicate(n, weights(Aup, sim_net(Aup, 0.2), 0.2, 0.05))
mu_hat_IS <- mean(tmp)
mu_hat_IS
```

. . .

Confidence interval using empirical variance estimate $\hat{\sigma}^2$.

```{r, dependson = "sim2"}
mu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)
```

. . .

```{r}
c(sd(tmp), sqrt(mu_hat * (1 - mu_hat))) # Estimated standard deviations
```

## Comparison

The ratio of variances is estimated as

```{r}
mu_hat * (1 - mu_hat) / var(tmp)
```

. . .

Need about `r round(mu_hat * (1 - mu_hat) / var(tmp))` times more naive samples
compared to importance sampling for same precision.

\medskip\pause

Benchmarking would should that the extra computing time for importance sampling
is small compared to the reduction of variance.

\medskip\pause

It is worth the coding effort if used repeatedly, but not if it is a one-off
computation.

## Enumeration

There are $2^{18} = 262,144$ different networks with any number of the edges
failing, so complete enumeration is possible.

```{r enumeration, cache = TRUE}
ones <- which(Aup == 1)
Atmp <- Aup
p <- 0.05
prob <- numeric(2^18)

for (i in 0:(2^18 - 1)) {
  on <- as.numeric(intToBits(i)[1:18])
  Atmp[ones] <- on

  if (discon(Atmp)) {
    s <- sum(on)
    prob[i + 1] <- p^(18 - s) * (1 - p)^s
  }
}
```

## Probability of 1 and 10 Being Disconnected

Here is the true value:

```{r disconnect-prob}
sum(prob)
```

. . .

And here's the MC estimate with confidence interval:

```{r}
mu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)
```

. . .

And the IS estimate with confidence interval:

```{r}
mu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)
```


```{r reset-options}
#| echo: false
options(digits = old_options$digits)
```

## Summary

- Monte Carlo methods are useful for estimating integrals and means
- Importance sampling can reduce variance considerably
- But the proposal distribution must be chosen carefully

## Exercise: Von Mises Importance Sampling

Implement an importance sampling algorithm for computing the mean of the von
Mises distribution.

```{r vonmises-density}
dvmises <- function(x, mu = 0, kappa = 2) {
  exp(kappa * cos(x - mu)) / (2 * pi * besselI(kappa, 0))
}
```

Try various proposals.

If you have time, create a function that takes parameters of the von Mises
distribution and adjusts the proposal distribution based on these.

Plot the result and check convergence.

```{r vmises-importance}
#| include: false
vonmises_importance <- function(n, mu = 0, kappa = 2) {
  x <- rnorm(n, 0, pi / 2)
  w <- dvmises(x, mu, kappa) / dnorm(x, 0, pi / 2)
  cumsum(x * w) / 1:n
}

x <- seq(-pi, pi, length.out = 100)
plot(x, dvmises(x))
lines(x, dnorm(x, 0, pi / 2))
```

### Exercise

Implement a heuristic stopping rule that stops when the confidence interval is
0.1 units wide.

```{r excercise-stop, include = FALSE}
# TO DO
```
$$
$$
