---
title: "Monte Carlo Methods"
---

{{< include _common.qmd >}}

```{r setup-common2}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(patchwork)
theme_set(theme_minimal(base_size = 10))
options(width = 60)
```

```{r setup, include=FALSE}
old_options <- options(digits = 4)
```

## Rejection Sampling Recap

Suppose that $f(y) \propto q(y)$ is the **target** density, known up to
normalization.

\medskip\pause

Suppose that $g(y) \propto p(y)$ is another density, known up to normalization,
that we can simulate from, and suppose
$$
\alpha' q \leq p
$$
for some $\alpha' > 0$.

\medskip\pause

If $Y$ has density $g$ and $U$ is uniform on $(0,1)$ then the conditional
distribution
$$
Y \mid U \leq \alpha' q(Y) / p(Y)
$$
has density $f$.

## Basic Rejection Sampling Algorithm

\begin{algorithm}[H]
  \caption{Rejection sampling to simulate from a random variable $X$ with density
           $f \propto q$ using proposal $Z$ with density $g \propto p$.}
  \Repeat{$x$ is accepted}{
    Generate $x \sim Z$\;
    Generate $u \sim \mathsf{Uniform}(0, 1)$\;
    \If{$u \leq \frac{\alpha' q(x)}{p(x)}$}{
      Accept $x$\;
    }
  }
\end{algorithm}

. . .

### Two Challenges

- Finding an envelope $p/\alpha'$ of $q$.\pause
- Finding a **tight-enough** envelope.

## Today

### Monte Carlo Methods

General class of methods for estimating integrals and means.

\medskip\pause

Rejection sampling is a type of Monte Carlo method.

. . .

### Importance Sampling

Focus on integrating functions and estimating means.

\medskip\pause

Useful when we cannot analytically compute the integral.

\medskip\pause

It is also a variance reduction technique.

## Monte Carlo Methods

### Basic, Simple Idea

Use random sampling to obtain numerical results.

\medskip\pause

Useful when analytical solutions are infeasible.

. . .

### Monte Carlo Integration

With $X_1, \ldots, X_n$ i.i.d. with density $f$,
$$
\hat{\mu}_{\mathsf{MC}} = \frac{1}{n} \sum_{i=1}^n h(X_i) \rightarrow \mu = \E h(X_1) = \int h(x) f(x) \ dx
$$
for $n \to \infty$ by the law of large numbers (LLN).

## Monte Carlo Integration

By the central limit theorem (CLT), we know that
$$
\frac{1}{n} \sum_{i=1}^n h(X_i) \overset{\mathsf{approx}} \sim
\mathcal{N}(\mu, \sigma^2_{\mathsf{MC}} / n)
$$
where
$$
\sigma^2_{\mathsf{MC}} = \var h(X_1) = \int (h(x) - \mu)^2 f(x) \ dx.
$$

## Monte Carlo Integration

We can estimate $\sigma^2_{\mathsf{MC}}$ using the empirical variance
$$
\hat{\sigma}^2_{\mathsf{MC}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i) - \hat{\mu}_{\mathsf{MC}})^2,
$$
then the variance of $\hat{\mu}_{\mathsf{MC}}$ is estimated as
$\hat{\sigma}^2_{\mathsf{MC}} / n$ and a standard 95% confidence interval for
$\mu$ is
$$
\hat{\mu}_{\mathsf{MC}} \pm 1.96 \frac{\hat{\sigma}_{\mathsf{MC}}}{\sqrt{n}}.
$$

## Example: Gamma Distribution

:::: {.columns}

::: {.column width="47%"}

We want to estimate the mean of a $\mathsf{Gamma}(8,1)$ distribution.

. . .

```{r gamma-sim, echo=-1}
set.seed(1234)
B <- 1000
x <- rgamma(B, 8)

mu_hat <- (cumsum(x) / (1:B))
sigma_hat <- sd(x)
mu_hat[B] # Theoretical: 8
sigma_hat # Theoretical: âˆš8
```

:::

::: {.column width="47%"}

```{r gamma-fig, echo = FALSE}
#| fig-cap: "Density of a Gamma(8,1) distribution with mean indicated."
x <- seq(0, 20, length.out = 100)
y <- dgamma(x, 8)

tibble(x, y) |>
  ggplot(aes(x, y)) +
  geom_line() +
  geom_vline(xintercept = 8, linetype = "dashed", color = "steelblue") +
  annotate("text", x = 7, y = 0.02, label = "E(X)", hjust = -0.5)
```

:::

::::

##

```{r gamma-fig1}
#| fig-height: 2.8
#| fig-width: 5
#| echo: false
#| fig-cap: "Convergence of the Monte Carlo estimate of the mean of a Gamma(8,1) distribution."
p <- ggplot(tibble(x = 1:B, y = mu_hat), aes(x, y)) +
  geom_hline(yintercept = 8, linetype = "dashed", col = "steelblue4") +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10))

p +
  geom_ribbon(
    aes(
      ymin = mu_hat - 1.96 * sigma_hat / sqrt(1:B),
      ymax = mu_hat + 1.96 * sigma_hat / sqrt(1:B)
    ),
    fill = "dark orange",
    alpha = 0.4
  ) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10))
```

## When to Stop?

We could use concentration inequalities (Chebyshev/Chernoff) to get a stopping
rule.

\medskip\pause

But need variance estimate for Chebychev-based bound and moment generating
functions for Chernoff-based bounds.

\medskip\pause

More commonly, just run until the confidence interval is sufficiently narrow.

. . .

### Trade-Offs

If you overshoot then you waste computer resources, and if you undershoot then
you get a bad estimate.

## Importance Sampling

When we are only interested in Monte Carlo integration, we do not need to sample
from the target distribution.

. . .

### Basic Idea

Some regions of the target distribution are often more important than others.

\medskip\pause

So let's oversample these regions, but correct for this by \alert{weighting}
the samples.

\medskip\pause

We want to find $\mu = \E h(X)$. Observe that
$$
\mu = \int h(x) f(x) \ dx = \int h(x) \frac{f(x)}{g(x)} g(x) \ dx = \int h(x) w^*(x) g(x) \ dx
$$
whenever $g$ is a density fulfilling that
$$
g(x) = 0 \Rightarrow f(x) = 0.
$$

## Weights

With $X_1, \ldots, X_n$ i.i.d. with density $g$ define the _weights_
$$
w^*(X_i) = \frac{f(X_i)}{g(X_i)}.
$$

. . .

$g$is the _importance_ density and $f/g$ is called the _likelihood ratio_.

. . .

### Estimator

$$
\hat{\mu}_{\mathsf{IS}}^* := \frac{1}{n} \sum_{i=1}^n h(X_i)w^*(X_i).
$$

It has mean $\mu$.

##

```{r}
#| echo: false
#| fig-cap: "Target (dashed blue) and importance (orange) densities
#|  with corresponding importance weights."
#| fig-width: 5
#| fig-height: 3
x <- seq(-5, 5, length.out = 200)
target <- dnorm(x, mean = 0, sd = 1)
importance <- dnorm(x, mean = 1, sd = 1)

p1 <- tibble(x, target, importance) |>
  pivot_longer(
    cols = c(target, importance),
    names_to = "Density",
    values_to = "value"
  ) |>
  ggplot(aes(x, value, color = Density, linetype = Density)) +
  geom_line() +
  scale_color_manual(
    values = c(target = "black", importance = "steelblue")
  ) +
  scale_linetype_manual(values = c(target = "solid", importance = "dashed")) +
  theme(legend.position.inside = c(0.87, 0.82)) +
  guides(
    color = guide_legend(position = "inside"),
    linetype = guide_legend(position = "inside")
  ) +
  labs(
    y = "Density",
    x = "x",
    color = "Distribution",
    linetype = "Distribution"
  )

w <- target / importance

p2 <- tibble(x, w) |>
  ggplot(aes(x, w)) +
  geom_line() +
  labs(x = "x", y = "Weight")

p1 / p2 + plot_layout(heights = c(2, 1), axes = "collect")
```

## Gamma Importance Sampling

:::: {.columns}

::: {.column width="47%"}

```{r gamma-sim-IS, echo=-1}
set.seed(1234)
B <- 1000
x <- rnorm(B, 10, 3)
w_star <-
  dgamma(x, 8) / dnorm(x, 10, 3)
mu_hat_IS <-
  cumsum(x * w_star) / (1:B)
mu_hat_IS[B]
```

Close to theoretical value (8).

:::

. . .

::: {.column width="47%"}

```{r gammasim-fig, echo = FALSE}
#| fig-height: 3.2
#| fig-width: 2.7
z <- seq(-12, 12, length.out = 100) + 10
f_x <- dgamma(z, 8)
g_x <- dnorm(z, 10, 3)
w <- f_x / g_x

p1 <- tibble(x = z, target = f_x, importance = g_x) |>
  pivot_longer(c(target, importance)) |>
  ggplot(aes(x, value, color = name, linetype = name)) +
  geom_line() +
  guides(color = guide_legend(position = "inside")) +
  scale_color_manual(
    values = c(target = "black", importance = "steelblue")
  ) +
  scale_linetype_manual(
    values = c(target = "solid", importance = "dashed")
  ) +
  theme(legend.position.inside = c(0.8, 0.8)) +
  labs(y = "Density", color = "Density", linetype = "Density")

p2 <- tibble(x = z, w) |>
  ggplot(aes(x, w)) +
  geom_line()

p1 / p2 + plot_layout(axes = "collect")
```

:::

::::

##

```{r gamma-IS-fig1, echo = FALSE}
#| fig-cap: "Convergence of the importance sampling estimate of the mean of a
#|   Gamma(8,1) distribution using a N(10,3) importance distribution."
#| fig-width: 5
#| fig-height: 2.5
mu_hat_IS <- cumsum(x * w_star) / (1:B)
sigma_hat_IS <- sd(x * w_star)

p <- ggplot(
  tibble(x = 1:B, y = mu_hat_IS),
  aes(x, y)
) +
  geom_ribbon(
    aes(
      ymin = mu_hat_IS - 1.96 * sigma_hat_IS / sqrt(1:B),
      ymax = mu_hat_IS + 1.96 * sigma_hat_IS / sqrt(1:B)
    ),
    fill = "dark orange",
    alpha = 0.4
  ) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10)) +
  geom_hline(yintercept = 8, linetype = "dashed", col = "steelblue4")
p
```

## Importance Sampling Variance

Again by the LLN
$$
\hat{\mu}_{\mathsf{IS}}^* \rightarrow \E h(X_1) w^*(X_1) = \mu,
$$
and by the CLT
$$
\hat{\mu}_{\mathsf{IS}}^* \overset{\mathsf{approx}} \sim
\mathcal{N}(\mu, \sigma^{*2}_{\mathsf{IS}} / n)
$$
where
$$
\sigma^{*2}_{\mathsf{IS}} = \var \big( h(X_1)w^*(X_1)\big) = \int (h(x) w^*(x) - \mu)^2 g(x) \ dx.
$$

## Importance Sampling Variance

The importance sampling variance can be estimated just as the MC variance
$$
\hat{\sigma}^{*2}_{\mathsf{IS}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i)w^*(X_i) - \hat{\mu}_{\mathsf{IS}}^*)^2,
$$

. . .

And a 95% standard confidence interval is
$$
\hat{\mu}^*_{\mathsf{IS}} \pm 1.96 \frac{\hat{\sigma}^*_{\mathsf{IS}}}{\sqrt{n}}.
$$

\medskip\pause

We may have $\sigma^{*2}_{\mathsf{IS}} > \sigma^2_{\mathsf{MC}}$ or
$\sigma^{*2}_{\mathsf{IS}} < \sigma^2_{\mathsf{MC}}$ depending on $h$ and $g$.

. . .

### Goal

Choose $g$ so that $h(x) w^*(x)$ becomes as constant as possible.

## Gamma Importance Sampling

```{r gamma-IS-sd, echo=TRUE}
sigma_hat_IS <- sd(x * w_star)
sigma_hat_IS # Theoretical value?
```

. . .

```{r gamma-IS-fig2, echo=FALSE}
#| fig-width: 4.5
#| fig-cap: "Importance sampling estimate of the mean of a Gamma(8,1)."
p +
  geom_ribbon(
    aes(
      ymin = mu_hat_IS - 1.96 * sigma_hat_IS / sqrt(1:B),
      ymax = mu_hat_IS + 1.96 * sigma_hat_IS / sqrt(1:B)
    ),
    fill = "gray"
  ) +
  geom_line() +
  geom_point(size = 0.5)
```

```{r test, echo=FALSE, eval=FALSE}
mu_hat_IS <- replicate(10000, {
  x <- rnorm(B, 10, 3)
  w_star <- dgamma(x, 8) / dnorm(x, 10, 3)
  mean(x * w_star)
})
sd(mu_hat_IS) * sqrt(B)
```

## Standardized Weights

If $f = c^{-1} q$ with $c$ unknown then
$$
c = \int q(x) dx = \int \frac{q(x)}{g(x)} g(x) d x.
$$

. . .

And
$$
\mu = \frac{\int h(x) w^*(x) g(x) \ d x}{\int w^*(x) g(x) \ d x},
$$
where
$$
w^*(x) = \frac{q(x)}{g(x)}.
$$

## IS with Standardized Weights

An importance sampling estimate of $\mu$ is thus
$$
\begin{aligned} \hat{\mu}_{\mathsf{IS}} & = \frac{\sum_{i=1}^n h(X_i) w^*(X_i)}{\sum_{i=1}^n w^*(X_i)} \\ \pause
                                        & = \sum_{i=1}^n h(X_i) w(X_i),
\end{aligned}
$$
where $w^*(X_i) = q(X_i) / g(X_i)$ and
$$
w(X_i) = \frac{w^*(X_i)}{\sum_{i=1}^n w^*(X_i)}
$$
are the _standardized weights_.

\medskip\pause

Works irrespectively of the value of the normalizing constant $c$, and also if
an unnormalized $g$ is used.

## Gamma Standardized Weights

```{r gamma-sim-ISw, echo=-1}
set.seed(1234)
B <- 1000
x <- rnorm(B, 10, 3)
w_star <- numeric(B)
x_pos <- x[x > 0]
w_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))

mu_hat_IS <- cumsum(x * w_star) / cumsum(w_star)
mu_hat_IS[B] # Theoretical value 8
```

##

```{r gamma-ISw-fig1, echo = FALSE}
#| fig-width: 4.5
#| fig-height: 3
#| fig-cap: "Convergence of the importance sampling estimate of the mean of a
#|   Gamma(8,1) distribution using standardized weights."
p <- tibble(x = 1:B, y = mu_hat_IS) |>
  ggplot(aes(x, y)) +
  geom_line() +
  geom_point(size = 0.5) +
  coord_cartesian(ylim = c(6, 10)) +
  labs(y = expression(hat(mu)[IS]), x = "Number of samples")
p
```

## Importance Sampling Variance

The variance of the IS estimator with standardized weights is a little more
complicated, because the estimator is a \alert{ratio of random variables}.

\medskip\pause

From the multivariate CLT, we have
$$
\frac{1}{n} \sum_{i=1}^n \begin{bmatrix} h(X_i) w^*(X_i) \\ w^*(X_i) \end{bmatrix}
\overset{\mathsf{approx}}{\sim}
\mathcal{N}\left(
  c \begin{bmatrix} \mu \\ 1\end{bmatrix},
  \frac{1}{n} \begin{bmatrix} \sigma^{2}_{\mathsf{IS}} & \gamma \\ \gamma & \sigma^2_{w^*} \end{bmatrix}
\right),
$$
where
$$
\begin{aligned}
  \sigma^2_{\mathsf{IS}} & = \var\big(h(X_1) w^*(X_1)\big) \\
  \gamma & = \cov (h(X_1) w^*(X_1), w^*(X_1)) \\
  \sigma^2_{w^*} & = \var (w^*(X_1))
\end{aligned}
$$

## Importance Sampling Variance

We can then apply the $\Delta$-method with $t(x, y) = x / y$.

\medskip\pause

Observe that $\nabla t(x, y) = \begin{bmatrix}1 / y \\ - x / y^2\end{bmatrix}$,
whence

$$
\nabla t(c\mu, c)^T \left(\begin{array}{cc} \hat{\sigma}^{2}_{\mathsf{IS}} & \gamma \\ \gamma & \sigma^2_{w^*}
\end{array} \right) \nabla t(c\mu, c) = \frac{\sigma^{2}_{\mathsf{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma}{c^{2}}.
$$

. . .

By the $\Delta$-method,
$$
\hat{\mu}_{\mathsf{IS}} \overset{\mathsf{approx}}{\sim}
\mathcal{N}\left(\mu, \frac{\sigma^{2}_{\mathsf{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma}{c^2 n} \right).
$$

. . .

For $c = 1$, the asymptotic variance can be estimated by plugging in empirical
estimates. For $c \neq 1$ it is necessary to estimate $c$ as
$$
\hat{c} = \frac{1}{n} \sum_{i=1}^n w^*(X_i)
$$

##

```{r delta-method-plot3D}
#| echo: false
#| fig-width: 5
#| fig-height: 4.5
#| fig-cap: Tangent plane approximation of the function $t(x,y) = x/y$ at the point
#|   $(2,3)$.
library(plot3D)

# Define function and means
t_fun <- function(x, y) x / y
mu_x <- 2
mu_y <- 3

# Grid for surface
x_seq <- seq(1, 3, length.out = 26)
y_seq <- seq(2, 4, length.out = 26)
z <- outer(x_seq, y_seq, t_fun)

# Tangent plane at (mu_x, mu_y)
grad_x <- 1 / mu_y
grad_y <- -mu_x / mu_y^2
tangent_plane <- function(x, y) {
  t_fun(mu_x, mu_y) + grad_x * (x - mu_x) + grad_y * (y - mu_y)
}
z_tangent <- outer(x_seq, y_seq, tangent_plane)

# Plot both surfaces together
persp3D(
  x_seq,
  y_seq,
  z,
  col = "steelblue",
  xlab = "x",
  ylab = "y",
  zlab = "t(x, y)",
  theta = 30,
  phi = 10,
  border = "black"
)

# Add tangent plane
persp3D(
  x_seq,
  y_seq,
  z_tangent,
  add = TRUE,
  col = "grey",
  alpha = 0.6
)

# Add mean point
points3D(
  mu_x,
  mu_y,
  t_fun(mu_x, mu_y),
  col = "darkorange",
  pch = 19,
  add = TRUE,
  cex = 1
)
```

## Gamma Standardized Weights, Variance

```{r sigma_hat_is_w}
c_hat <- mean(w_star)
sigma_hat_IS <- sd(x * w_star)
sigma_hat_w_star <- sd(w_star)
gamma_hat <- cov(x * w_star, w_star)

sigma_hat_IS_w <- sqrt(
  sigma_hat_IS^2 +
    mu_hat_IS[B]^2 * sigma_hat_w_star^2 -
    2 * mu_hat_IS[B] * gamma_hat
) /
  c_hat

sigma_hat_IS_w
```

```{r test-2, echo=FALSE, eval=FALSE}
mu_hat_IS <- replicate(10000, {
  x <- rnorm(B, 10, 3)
  w_star <- numeric(B)
  x_pos <- x[x > 0]
  w_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))
  sum(x * w_star) / sum(w_star)
})
sd(mu_hat_IS) * sqrt(B)
```

##

```{r gamma-ISw-fig2}
#| echo: false
#| fig-width: 4.5
#| fig-cap: "Importance sampling estimate of the mean of a Gamma(8,1)
#|   distribution with 95% confidence bands using standardized weights."
p +
  geom_ribbon(
    aes(
      ymin = mu_hat_IS - 1.96 * sigma_hat_IS_w / sqrt(1:B),
      ymax = mu_hat_IS + 1.96 * sigma_hat_IS_w / sqrt(1:B)
    ),
    fill = "gray"
  ) +
  geom_line() +
  geom_point(size = 0.5)
```

## When to Use Importance Sampling?

### Rare Events

Estimating probabilities or expectations involving rare events.

. . .

### High-Variance Monte Carlo

Importance sampling can
reduce variance by focusing samples where the integrand is large.

. . .

### Difficult-to-Sample Targets

When direct sampling from the target
  distribution is hard, but sampling from a related importance distribution
  is easy.

## Poor Importance Distribution

If the importance distribution does not cover regions where the target is large,
weights become highly variable and estimates unreliable.

\medskip\pause

### Remedies

Redesign importance density to better match the target density.

\medskip\pause

Increase sample size (if feasible).

## Diagnostics for Importance Sampling

### Weight Distribution

A histogram of weights visualizes the distribution of importance weights. This
reveals weight degeneracy, where many weights are near zero and a few are very
large.

. . .

### Effective Sample Size (ESS)

Effective sample size (ESS) measures how many samples are effectively
contributing to the estimate. A low ESS indicates poor sampling efficiency and
high variance.

. . .

### Coefficient of Variation (CV)

The coefficient of variation (CV) of the weights quantifies their relative
variability. A high CV indicates that a few samples dominate the estimate,
leading to high variance.

. . .

### Comparison with Standard Monte Carlo

Comparing importance sampling estimates and variances with standard Monte Carlo
shows whether importance sampling actually improves efficiency.

## Weight Degeneracies

:::: {.columns}

::: {.column width="47%"}

Suppose that
$$
X \sim \mathcal{N}(0, I_{10})
$$
and that we want to estimate $\operatorname{E}[f(X)]$
where $f(X)$ is large near the origin.

\medskip\pause

If we use a importance density $g(x)$ from
$$
\mathcal{N}(\mu, I_{10})
$$
with large $|\mu|$, most samples are far from the origin, so $f(X)$ is nearly zero and
weights are highly variable.

:::

::: {.column width="47%"}


```{r poor-importance-demo}
#| echo: false
#| fig-height: 2.3
#| fig-cap: "Target (blue) and importance (orange) densities
#|   with samples from the importance distribution."
library(mvtnorm)

set.seed(1)
d <- 2
n <- 1000
mu <- rep(2, d)
x <- matrix(rnorm(n * d, mean = mu), ncol = d)

colnames(x) <- c("x1", "x2")

grid <- expand.grid(
  x1 = seq(-5, 8, length.out = 100),
  x2 = seq(-5, 8, length.out = 100)
)
grid_mat <- as.matrix(grid)

grid$target <- dmvnorm(grid_mat, mean = rep(0, d))
grid$importance <- dmvnorm(grid_mat, mean = mu)

tibble(
  x1 = grid$x1,
  x2 = grid$x2,
  target = grid$target,
  importance = grid$importance
) |>
  ggplot(aes(x1, x2)) +
  geom_contour(aes(z = target), color = "steelblue", linetype = "dashed") +
  geom_contour(aes(z = importance), color = "darkorange") +
  geom_point(aes(x1, x2), data = as_tibble(x), alpha = 0.3, size = 0.5) +
  labs(
    x = expression(X[1]),
    y = expression(X[2])
  ) +
  coord_fixed()
```

:::

::::

##

```{r}
#| fig-width: 5
#| fig-height: 2.5
#| echo: false
#| fig-cap: "Histogram of importance weights showing weight degeneracy."
target_density <- function(x) dmvnorm(x, mean = rep(0, d))
importance_density <- function(x) dmvnorm(x, mean = mu)
f <- function(x) exp(-sum(x^2) / 2) # Large near origin

weights <- apply(x, 1, function(row) {
  target_density(row) / importance_density(row)
})
values <- apply(x, 1, f)
estimate <- sum(values * weights) / sum(weights)

ggplot(tibble(weights = weights), aes(weights)) +
  geom_histogram() +
  labs(
    x = "Importance Weights",
    y = "Frequency"
  )
```

Most weights are near zero, with a few large weights.

\medskip\pause

\pdfpcnote{
This indicates weight degeneracy and poor sampling efficiency.
}

## Effective Sample Size (ESS)

The effective sample size (ESS) quantifies how many samples effectively
contribute to the estimate.

\medskip\pause

It is defined as
$$
\mathrm{ESS} = \frac{1}{\sum_{i=1}^n w_i^2}
$$
where $w_i$ are the \alert{normalized} importance weights.

\medskip\pause

If all weights are equal, $\mathrm{ESS} = n$. If one weight dominates,
$\mathrm{ESS} \approx 1$, and the estimate is unreliable.

. . .

```{r ess-demo}
ess <- 1 / sum(weights^2)
ess
```

## Coefficient of Variation (CV)

The coefficient of variation (CV) of the weights measures their relative
variability.

\medskip\pause

It is defined as
$$
\mathrm{CV} = \frac{\sqrt{\frac{1}{n} \sum_{i=1}^n (w_i - \bar{w})^2}}{\bar{w}}
$$
where $\bar{w} = \frac{1}{n} \sum_{i=1}^n w_i$ is the mean weight.

. . .

### Interpretation

A high CV indicates that a few samples dominate the estimate, leading to high
variance.

## Choosing the Importance Distribution

We want low-variance estimates of
$$
I = \int h(x) f(x) \ dx
$$

. . .

The importance distribution determines the weights
$$
w^*(x) = \frac{f(x)}{g(x)}
$$
and the variance of these weights drives the variance of the IS estimator.

## Principles for Choosing the Importance Distribution

### Cover the Support

Ensure the importance distribution covers all regions where the target
distribution is significant.

. . .

### Match the Bulk

The mean and spread of $q$ shuold roughly match the regions of large
$|h(x)|f(x)$.

. . .

### Give it Heavy-Enough Tails

The importance distribution should have heavier tails than the target to avoid
extreme weights.

## Common Strategies

### Laplace Approximation

Use a Gaussian approximation centered at the mode of the target distribution.

. . .


### Adaptive Importance Sampling

Iteratively refine the importance distribution based on pilot runs.

. . .


### Defensive Mixtures

Combine a well-chosen importance distribution with a broader distribution to
ensure coverage of the entire target support.

. . .

### Mixture Distributions

Use a mixture of distributions to capture multiple modes or complex shapes.


## Laplace Approximation

### Basic Idea

Take a Gaussian approximation of the target density at its mode $x^*$ as the importance
distribution. 

. . .

### Motivation

Connection comes from the second-order Taylor expansion of $\log f(x)$ at the mode $x^*$,
$$
\log f(x) \approx \log f(x^*) - \frac{1}{2}(x - x^*)^T H (x - x^*)
$$
where $H$ is the Hessian of $-\log f(x)$ at $x^*$. 

\medskip\pause

Exponentiating both sides gives:
$$
f(x) \approx f(x^*) \exp\left(-\frac{1}{2}(x - x^*)^T H (x - x^*)\right)
$$


## Laplace Approximation


### Steps

1. Find the mode $x^* = \arg\max_x f(x)$.\pause
2. Compute the Hessian $H$ of the first-order Taylor approximation of $-\log
   f(x)$ around $x^*$.\pause
3. Use $\mathcal{N}(x^*, H^{-1})$ as importance distribution.

. . .

### When to Use

Best for unimodal, smooth, and symmetric target densities.

## Example: Gamma Distribution

:::: {.columns}

::: {.column width="47%"}

The density of the $\mathsf{Gamma}(3,1)$ distribution is
$$
f(x) = \frac{1}{2} x^2 e^{-x}, \quad x > 0.
$$

. . .

The mode is at $x^* = 2$ and the Hessian of $-\log f(x)$ at $x^*$ is 
$$
H = \frac{\alpha - 1}{(x^*)^2} = \frac 1 2.
$$

\medskip\pause

So the Laplace approximation is $\mathcal{N}(2, 2)$.

:::

::: {.column width="47%"}


```{r}
#| echo: false
#| fig-cap: "Target (solid black) and Laplace approximation (dashed blue)
#|   densities for a Gamma(3,1) distribution."
f_density <- function(x) dgamma(x, shape = 3, rate = 1) # Gamma(3,1)
log_f <- function(x) log(f_density(x))

x_star <- (3 - 1) / 1
hessian <- 2 / (x_star^2)
sd_laplace <- 1 / sqrt(hessian)

x_grid <- seq(-2, 10, length.out = 400)

df <- tibble(
  x = x_grid,
  f = f_density(x_grid),
  laplace = dnorm(x_grid, mean = x_star, sd = sd_laplace),
) |>
  pivot_longer(
    cols = c(f, laplace),
    names_to = "Density",
    values_to = "value"
  ) |>
  filter(value > 0)

# Plot
ggplot(df, aes(x = x, y = value, color = Density, linetype = Density)) +
  geom_line() +
  scale_color_manual(
    values = c(f = "black", laplace = "steelblue")
  ) +
  scale_linetype_manual(values = c(f = "solid", laplace = "dashed")) +
  theme(legend.position.inside = c(0.8, 0.8)) +
  guides(
    color = guide_legend(position = "inside"),
    linetype = guide_legend(position = "inside")
  ) +
  labs(
    y = "Density"
  )
```

:::

::::


## Moment-Matching Importance Sampling

Adaptive importance sampling iteratively refines the importance distribution
based on the samples obtained.

. . .

### Basic Idea

Start with an initial importance distribution and iteratively refine it based on
the samples and weights obtained.

. . .

### Steps

1. Choose an initial importance distribution $g_0(x)$.
2. Draw samples from $g_0$ and compute weights.
3. Update the importance distribution parameters based on weighted samples.
4. Repeat steps 2-3 for several iterations.

### Mathematically

At iteration $t$, draw samples $X_1, \ldots, X_n \sim g_t(x)$ and compute
weights
$$
w_i^* = \frac{f(X_i)}{g_t(X_i)}.
$$  

##

```{r}
#| echo: false
library(ggplot2)
library(tibble)

# Target density: bimodal mixture
f_density <- function(x) 0.4 * dnorm(x, -2, 1) + 0.6 * dnorm(x, 3, 0.5)

# Adaptive importance sampling function
adaptive_is <- function(
  n = 1000,
  n_iter = 5,
  q_init_mean = 0,
  q_init_sd = 1
) {
  # Store proposal params and samples
  proposals <- list()
  samples <- list()
  weights <- list()

  mean_q <- q_init_mean
  sd_q <- q_init_sd

  for (i in 1:n_iter) {
    # Draw from current proposal
    x <- rnorm(n, mean = mean_q, sd = sd_q)
    w <- f_density(x) / dnorm(x, mean = mean_q, sd = sd_q)
    w_norm <- w / sum(w)

    # Store current step
    proposals[[i]] <- c(mean = mean_q, sd = sd_q)
    samples[[i]] <- x
    weights[[i]] <- w_norm

    # Update proposal using weighted moments
    mean_q <- sum(w_norm * x)
    sd_q <- sqrt(sum(w_norm * (x - mean_q)^2))
  }

  return(list(proposals = proposals, samples = samples, weights = weights))
}

# Run adaptive importance sampling
set.seed(123)
res <- adaptive_is(n = 1000, n_iter = 5)

# Plotting function for a given iteration
plot_step <- function(iter) {
  proposal <- res$proposals[[iter]]
  x_grid <- seq(-6, 6, length.out = 400)
  df <- tibble(
    x = x_grid,
    target = f_density(x_grid),
    importance = dnorm(x_grid, mean = proposal["mean"], sd = proposal["sd"])
  ) |>
    pivot_longer(
      cols = c(target, importance),
      names_to = "Density",
      values_to = "value"
    )

  # Calculate ESS and CV for current iteration
  w <- res$weights[[iter]]
  n <- length(w)
  ess <- (sum(w))^2 / sum(w^2)
  cv <- sqrt(mean((w - mean(w))^2)) / mean(w)

  ggplot(df, aes(x = x, y = value, color = Density, linetype = Density)) +
    geom_line() +
    scale_color_manual(
      values = c(target = "black", importance = "steelblue")
    ) +
    scale_linetype_manual(values = c(target = "solid", importance = "dashed")) +
    lims(x = c(-6, 6), y = c(0, 0.6)) +
    labs(
      y = "Density",
      subtitle = sprintf("ESS = %.1f, CV = %.2f", ess, cv)
    )
}
```

```{r}
#| echo: false
#| fig-width: 5.4
#| fig-height: 2.5
#| fig-cap: "Adaptive importance sampling step 1."
plot_step(1)
```

##

```{r}
#| echo: false
#| fig-width: 5.4
#| fig-height: 2.5
#| fig-cap: "Adaptive importance sampling, step 2."
plot_step(2)
```

##

```{r}
#| echo: false
#| fig-width: 5.4
#| fig-height: 2.5
#| fig-cap: "Adaptive importance sampling, step 3."
plot_step(3)
```

##

```{r}
#| echo: false
#| fig-width: 5.4
#| fig-height: 2.5
#| fig-cap: "Adaptive importance sampling, step 4."
plot_step(4)
```

##

```{r}
#| echo: false
#| fig-width: 5.4
#| fig-height: 2.5
#| fig-cap: "Adaptive importance sampling, step 5."
plot_step(5)
```

## Defensive Importance Sampling


Defensive importance sampling uses a mixture of a well-chosen importance
distribution and a broader distribution to ensure coverage of the entire target
support.

\medskip\pause

Recall the Gamma(8,1) example where we used $g(x) = \mathcal{N}(10, 3^2)$.

\medskip\pause


## 

:::: {.columns}

::: {.column width="47%"}

```{r}
#| echo: false
#| fig-cap: "Target (black) and importance (blue) densities with samples from
#|   the importance distribution."
#| ref-label: gammasim-fig
#| fig-height: 3
#| fig-width: 2.7
```

:::

. . .

::: {.column width="47%"}


```{r gamma-defensive-fig}
#| fig-height: 3
#| fig-width: 2.7
#| fig-cap: "Target (black) and importance (orange) densities with samples from
#|   the defensive importance mixture distribution."
#| echo: false
set.seed(1234)
B <- 1000
mix_indicator <- rbinom(B, 1, 0.8)
x <- rnorm(B, mean = 10, sd = 3)
x[mix_indicator == 0] <- rnorm(sum(mix_indicator == 0), mean = 0, sd = 6)
g_mix <- 0.8 * dnorm(x, 10, 3) + 0.2 * dnorm(x, 10, 10)
w_star <- dgamma(x, 8) / g_mix
mu_hat_defensive <- cumsum(x * w_star) / (1:B)

z <- seq(-12, 12, length.out = 100) + 10
f_x <- dgamma(z, 8)
g_mix <- 0.8 * dnorm(z, 10, 3) + 0.2 * dnorm(z, 10, 10)
w <- f_x / g_mix

p1 <- tibble(x = z, target = f_x, mixture = g_mix) |>
  pivot_longer(c(target, mixture)) |>
  ggplot(aes(x, value, color = name, linetype = name)) +
  geom_line() +
  guides(color = guide_legend(position = "inside")) +
  scale_color_manual(
    values = c(target = "black", mixture = "darkorange")
  ) +
  scale_linetype_manual(
    values = c(target = "solid", mixture = "dashed")
  ) +
  theme(legend.position.inside = c(0.8, 0.8)) +
  labs(y = "Density", color = "Density", linetype = "Density")

p2 <- tibble(x = z, w) |>
  ggplot(aes(x, w)) +
  geom_line()

p1 / p2 + plot_layout(axes = "collect")
```

:::

::::

## Case Study: Network Failure

![A network graph. Different edges "fail" independently with probability
$p$.](../images/networkfig.png){ width=70% }

. . .

**Problem:** What is the probability that nodes 1 and 10 are disconnected?

## Representing Graphs

We can represent a graph using an adjacency matrix.

```{r network_adj, echo=13}
options(width = 65)
A <- matrix(0, 10, 10)
A[1, c(2, 4, 5)] <- 1
A[2, c(1, 3, 6)] <- 1
A[3, c(2, 6, 7, 8, 10)] <- 1
A[4, c(1, 5, 8)] <- 1
A[5, c(1, 4, 8, 9)] <- 1
A[6, c(2, 3, 7, 10)] <- 1
A[7, c(3, 6, 10)] <- 1
A[8, c(3, 4, 5, 9)] <- 1
A[9, c(5, 8, 10)] <- 1
A[10, c(3, 6, 7, 9)] <- 1
A # Graph adjacency matrix
```

. . .

```{r}
Aup <- A
Aup[lower.tri(Aup)] <- 0
```

## Check Connectivity

We begin with a function that checks if nodes 1 and 10 are disconnected.

. . .

```{r discon-def}
discon <- function(Aup) {
  A <- Matrix::forceSymmetric(Aup, "U")
  i <- 3
  Apow <- A %*% A %*% A # A^3

  while (Apow[1, 10] == 0 && i < 9) {
    Apow <- Apow %*% A
    i <- i + 1
  }

  Apow[1, 10] == 0 # TRUE if nodes 1 and 10 are not connected
}
```

## Sampling a Graph

```{r simNet}
sim_net <- function(Aup, p) {
  ones <- which(Aup == 1)
  Aup[ones] <- sample(
    c(0, 1),
    length(ones),
    replace = TRUE,
    prob = c(p, 1 - p)
  )
  Aup
}
```

. . .

```{r sim-bench}
bench::bench_time(replicate(1e5, sim_net(Aup, 0.5)))
```

## Estimating Probability of Nodes 1 and 10 Disconnected

```{r sim}
set.seed(27092016)
n <- 1e5
tmp <- replicate(n, discon(sim_net(Aup, 0.05)))

mu_hat <- mean(tmp)
mu_hat
```

. . .

Estimate with confidence interval using $\sigma^2 = \mu (1 - \mu)$.

```{r sim-confint}
mu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)
```

## Importance Sampling

We will simulate with failure probability $p_0$ and compute the importance
weights.

```{r impw}
weights <- function(Aup, Aup0, p0, p) {
  w <- discon(Aup0)

  if (w) {
    s <- sum(Aup0)
    w <- (p / p0)^18 * (p0 * (1 - p) / (p * (1 - p0)))^s
  }

  as.numeric(w)
}
```

. . .

For the IS estimator the weights will be multiplied by the indicator that 1 and
10 are disconnected.

## Estimating Probability of Nodes 1 and 10 Disconnected

```{r sim2}
#| cache: true
tmp <- replicate(n, weights(Aup, sim_net(Aup, 0.2), 0.2, 0.05))
mu_hat_IS <- mean(tmp)
mu_hat_IS
```

. . .

Confidence interval using empirical variance estimate $\hat{\sigma}^2$.

```{r}
mu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)
```

. . .

```{r}
c(sd(tmp), sqrt(mu_hat * (1 - mu_hat))) # Estimated standard deviations
```

## Comparison

The ratio of variances is estimated as

```{r}
mu_hat * (1 - mu_hat) / var(tmp)
```

. . .

Need about `r round(mu_hat * (1 - mu_hat) / var(tmp))` times more naive samples
compared to importance sampling for same precision.

\medskip\pause

Benchmarking would should that the extra computing time for importance sampling
is small compared to the reduction of variance.

\medskip\pause

It is worth the coding effort if used repeatedly, but not if it is a one-off
computation.

## Enumeration

There are $2^{18} = 262,144$ different networks with any number of the edges
failing, so complete enumeration is possible.

```{r enumeration}
#| cache: true
ones <- which(Aup == 1)
Atmp <- Aup
p <- 0.05
prob <- numeric(2^18)

for (i in 0:(2^18 - 1)) {
  on <- as.numeric(intToBits(i)[1:18])
  Atmp[ones] <- on

  if (discon(Atmp)) {
    s <- sum(on)
    prob[i + 1] <- p^(18 - s) * (1 - p)^s
  }
}
```

## Probability of 1 and 10 Being Disconnected

Here is the true value:

```{r disconnect-prob}
sum(prob)
```

. . .

And here's the MC estimate with confidence interval:

```{r}
mu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)
```

. . .

And the IS estimate with confidence interval:

```{r}
mu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)
```


```{r reset-options}
#| echo: false
options(digits = old_options$digits)
```

## Summary

- Monte Carlo methods are useful for estimating integrals and means
- Importance sampling can reduce variance considerably
- But the proposal distribution must be chosen carefully

## Exercise: Von Mises Importance Sampling

### Step 1

Implement a standard Monte Carlo algorithm for computing the mean of the von
Mises distribution. Plot the result and check convergence towards the $\mu$.

```{r vonmises-density}
dvmises <- function(x, mu = 0, kappa = 2) {
  exp(kappa * cos(x - mu)) / (2 * pi * besselI(kappa, 0))
}
```

. . .

### Step 2

Implement importance sampling using an importance
density of your choice. 

\medskip\pause

Compute the estimate of the mean and its standard error. Compare with the
standard Monte Carlo estimate.

. . .

### Step 3

Compute the effective sample size and coefficient of variation of the
importance weights. Experiment with different importance densities.


```{r vmises-importance}
#| include: false
vonmises_importance <- function(n, mu = 0, kappa = 2) {
  x <- rnorm(n, 0, pi / 2)
  w <- dvmises(x, mu, kappa) / dnorm(x, 0, pi / 2)
  cumsum(x * w) / 1:n
}

x <- seq(-pi, pi, length.out = 100)
plot(x, dvmises(x))
lines(x, dnorm(x, 0, pi / 2))
```

