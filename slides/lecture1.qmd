---
title: "Introduction"
---

{{< include _common.qmd >}}

```{r init, echo=FALSE, message=FALSE, warning=FALSE}
load(here::here("data", "top100dih.RData"))
phipsi2 <- na.omit(dataset)
phipsi <- read.table(here::here("data", "phipsi.tsv"), header = TRUE)
phipsi[, c("phi", "psi")] <- pi * phipsi[, c("phi", "psi")] / 180

options(width = 50)
```

## What is Computational Statistics?

It is a broad field, where meaning depends on context.

. . .

One definition is that it is **the use of computational methods to solve
statistical problems**, for instance

. . .

- simulation,
- optimization,
- numerical integration,
- data analysis, and
- visualization.

## A Running Example

:::: {.columns}

::: {.column width="47%"}

Let's try to get a bit of flavor of what we will be doing in the course.

\bigskip

. . .

Throughout the course we will use a data set of amino acid angles, $\Phi$ and
$\Psi$, from protein structures.

:::

::: {.column width="47%"}

![Amino Acid Angles](../images/PhiPsi_creative.jpg){width=90%}

:::

::::

## Histograms

A simple way to analyze the distributions of the angles $\Phi$ and $\Psi$ is the
**histogram**.

\bigskip

. . .

:::: {.columns}

::: {.column width="47%"}

```{r hist1-src}
#| eval: false
#| fig-cap: A histogram of the $\Psi$ variable in the amino acid data
ggplot(phipsi, aes(x = phi)) +
  geom_histogram() +
  geom_rug(alpha = 0.5) +
  labs(
    x = expression(Phi),
    y = "Density"
  )
```

:::

::: {.column width="47%"}

```{r hist1-show}
#| ref-label: hist1-src
#| fig-height: 2.1
#| echo: false
```

:::

::::

## Density Estimation

Histograms are not very smooth. If we allow ourselves to make stronger
assumptions, we can get a smoother estimate of the distribution, using **kernel
density estimation** (KDE).

\bigskip

. . .

:::: {.columns}

::: {.column width="47%"}

```{r dens-src}
#| eval: false
ggplot(phipsi, aes(x = phi)) +
  geom_density() +
  geom_rug(alpha = 0.5) +
  labs(
    x = expression(Phi),
    y = "Density"
  )
```

:::

::: {.column width="47%"}

```{r dens-fig}
#| echo: false
#| ref-label: dens
#| fig-height: 2

```

:::

::::

. . .

But how is this KDE actually computed? Doing this efficiently is a
**computational statistics** problem.

## Statistical Topics of the Course

The course can be broken down into a number of **statistical** and **computational**
topics.

. . .

There are three statistical topics in the course:

\pause 

### Smoothing

We will learn how to compute efficient kernel density estimates and
scatterplot smoothers.

. . .

### Simulation

We will learn to efficiently simulate from probability distributions 
using inversion, rejection, and importance sampling.

. . .

### Optimization

We will learn to solve optimization problems that arise in statistics, for
instance in maximum likelihood estimation (MLE), using the EM algorithm and
gradient-based optimization.

## Computational Topics of the Course

### Implementation

We will learn how to implement statistical methods in R, using
object-oriented programming and functional programming.

. . .

### Correctness

We will learn how to ensure that our code is correct, using
testing and debugging.

. . .

### Efficiency

We will learn how measure performance and find bottlenecks in our code using
profiling and benchmarking, and how to optimize it.

## Teaching Staff

:::: {.columns}

::: {.column width="47%"}

### Instructor

Johan Larsson, postdoctoral researcher

![Johan](../images/johan.jpg){width=50%}

#### Contact

Use Absalon for course-related questions and email (see Absalon) for personal
matters.

:::

. . .

::: {.column width="47%"}

### Teaching Assistant

Jinyang Liu, PhD student in machine learning

![Jin](../images/jinyang.jpg){width=50%}

:::

::::

## Assignments

Four assignments make up the bulk of the course work.

. . .

\bigskip

For each assignment, there are two alternatives (A and B). You will pick one.

. . .

\bigskip

Each assignment is tied to a particular **topic**:

1. Smoothing
2. Univariate simulation
3. The EM algorithm
4. Stochastic optimization

## Presentations

There will be four presentation sessions (week 3, 4, 6, 7)^[Not counting the
potato harvesting week when you are off.]

. . .

\bigskip

You will divide into groups of 2-3 students and present 
your solution to one of the assignments during one of the sessions.

. . .

\bigskip

You will register for groups and assignments in [Absalon](https://absalon.ku/dk).

. . .

\bigskip

Presentation is compulsory but not graded. We expect solutions to be
work in progress.


. . .

## Oral Examination

The main examination is an oral exam based on your assignments.

\bigskip 

. . .

You will prepare four presentations, one for each assignment you picked.

\bigskip

. . .

At the exam, you will present one of these at random.

## Schedule

:::: {.columns}

::: {.column width="47%"}

### Lectures

- Tuesdays and Thursdays, 10:15–12:00 (Johan)

. . .

### Exercise Sessions

- Thursdays, 08:15–10:00 (Jinyang)

. . .

### Presentations

- Thursdays, 13:15–15:00 (Johan)
- Only weeks 3, 4, 6, and 7

:::

. . .

::: {.column width="47%"}

### Examination

- November 6-8 (8.15-17.30, tentative)
- Rooms to be announced

:::

::::

## Course Literature

### Computational Statistics with R

Main textbook for the course, written by Niels Richard Hansen.

- Available online at <https://cswr.nrhstat.org/>
- Not yet complete, but we only use parts that are.
- [Companion package](https://github.com/nielsrhansen/CSwR/tree/master/CSwR_package):
  install with `pak::pak("github::nielsrhansen/CSwR/CSwR_package")`.

. . .

### Advanced R

Auxiliary textbook, written by Hadley Wickham.

- Available online at <https://adv-r.hadley.nz/>
- Covers more advanced R programming topics.
- We will use selected chapters.

## Online Resources

:::: {.columns}

::: {.column width="70%"}

### Absalon

Main source for information and communication about the course.
Accessed at [absalon.ku.dk](https://absalon.ku.dk/).

. . .

### CompStat Web Page

Course content will be uploaded to [github.io/math-ku/compstat](https://math-ku.github.io/compstat/).

\medskip

This is where you will a detailed schedule of the course, slides, and
the assignments.

:::

::: {.column width="25%"}

![Absalon](../images/absalon.jpg){width=100%}

:::

::::


## Generative AI

Generative AI (e.g. ChatGPT, Copilot, Bard, etc.) are powerful tools.

\bigskip

. . .

You are allowed to use them in this course, but with some caveats:

- You must understand the results.
- You must acknowledge their use in your assignments and how you used them.

. . .

\bigskip

:::: {.columns}

::: {.column width="47%"}

### Copilot
Access to GitHub Copilot is available for free to students, via 
[GitHub Education](https://education.github.com).

:::

::: {.column width="47%"}

![](../assets/images/github-education.png){width=100%}

:::

::::


# Programming in R

## Prerequisite R Knowledge

We expect knowledge of

- data structures (vectors, lists, data frames),
- control structures (loops, if-then-else),
- function calling,
- interactive and script usage (`source`) of R.

. . .

All of this is covered in chapters 1-5 of
[Advanced R](https://adv-r.hadley.nz/).

. . .

\bigskip

But you **do not** need to be an experienced programmer.

# Functions

## Functions in R

- Everything that happens in R is the result of a function call. Even `+`, `[`
  and `<-` are functions.
- An R function takes a number of _arguments_, and when a function call is
  evaluated it computes a _return value_.
- Functions can return any R object, including functions!
- Implementations of R functions are collected into source files, which can be
  organized into packages.
- The order of your functions in the script does not matter.

## Components of a Function

:::: {.columns}

::: {.column width="47%"}

### Arguments

```{r}
f <- function(x, y) {
  x + y
}
```

. . .

```{r}
formals(f)
```

:::

. . .

::: {.column width="47%"}

### Body

```{r}
body(f)
```

. . .

### Environment

```{r}
environment(f)
```

:::

::::

## Naming Functions

> There are only two hard things in Computer Science: cache invalidation and
> naming things.
>
> \medskip
> 
> \hfill_--Phil Karlton_

. . .

:::: {.columns}

::: {.column width="47%"}

### Favor Descriptive Names

Begin to see if you can use a _verb_. Better long and descriptive than short and
cryptic.

. . .

### Honor Common Conventions

Avoid `.` in names; it is used for **methods** (upcoming).

. . .

:::

::: {.column width="47%"}

### Use a Consistent Style

- `lowercase`
- `snake_case` (tidyverse)
- `camelCase`
- `UpperCamelCase`

. . .

### Namespace Clashes

- Avoid names of existing functions.

:::

::::

## Example: Counting Zeros

Count data is often modeled using a Poisson distribution. R can simulate count
data using the function `rpois()`.

```{r, echo=2}
set.seed(1234)
rpois(10, 2) # n = 10 variables from a Poisson(2) distribution
```

. . .

There are two zeros in this sequence.

. . .

Let's write a function that counts the number of zeros: checks for zero
inflation.

## A First Attempt

```{r}
count_zeros <- function(x) {
  n_zeros <- 0
  for (i in 1:length(x)) {
    if (x[i] == 0) {
      n_zeros <- n_zeros + 1
    }
  }
  n_zeros
}
```

. . .

```{r}
count_zeros(c(3, 2, 0))
```

. . .

```{r}
count_zeros(c(0, 0, 0))
```

## Testing

It is critical to ensure that your code does what you think it does,
and that it continues to do so as you modify it.

. . .

\bigskip

Testing is the process of writing code that checks that your code is
correct.

. . .

\bigskip

Some people even think that the **first thing** you should do is
to write a test.

. . .

\bigskip

As you modify your code, your tests will catch these **regressions** for you.

## testthat

[testthat](https://testthat.r-lib.org/) is the most popular testing framework for R.

. . .

```{r unit-testing, eval = FALSE, error = TRUE}
# In file tests/test_count_zeros.R
test_that("count_zeros work on various input", {
  expect_equal(count_zeros(c(0, 0, 1e-9, 25)), 2)
  expect_equal(count_zeros(c(-0, 1.1, -2)), 1)
  expect_equal(count_zeros(c()), 0)
})
```

```{r testthat, error = TRUE}
testthat::test_dir("tests")
```

## A Second Attempt

```{r}
count_zeros <- function(x) {
  n_zeros <- 0
  for (i in seq_along(x)) {
    if (x[i] == 0) {
      n_zeros <- n_zeros + 1
    }
  }
  n_zeros
}
```

```{r}
testthat::test_dir("tests")
```

## Debugging

- Sometimes hard to identify the offending piece of code.
- Helpful to use a debugging tool. R studio comes with a helpful interface for
  this.
- We will talk more about debugging in week 5.

## Functional Programming

In functional programming, functions are **first-class citizens**: they can be passed as
arguments to other functions, returned as values from functions, and assigned to
variables.

\bigskip

. . .

This allows for a high degree of abstraction and code reuse, for instance
through the use of the `apply` family of functions.

. . .

Let's write our own apply function.

\bigskip

```{r own-apply}
our_apply <- function(x, fun) {
  val <- numeric(length(x))
  for (i in seq_along(x)) {
    val[i] <- fun(x[[i]])
  }
  val
}
```

## Testing Our Apply Function

```{r}
sapply(1:10, exp)
```

. . .

```{r}
our_apply(1:10, exp)
```

. . .

### Assumptions

`x` is a list, `fun()` takes a single argument, and `fun()` returns a numeric.

. . .

## What if `fun()` Needs Additional Arguments?

Then we get an error:

```{r, error=TRUE, echo=-1}
set.seed(1)
our_apply(1:10, rpois)
```

. . .

### Anonymous Functions

We can use an anonymous function to pass additional arguments to `fun()`.

```{r}
our_apply(
  1:10,
  function(lambda) rpois(1, lambda = 0.9)
)
```

## `...`

More general functionality can be achieved wit the  `...` (ellipsis) argument,
which passes arguments forward.

. . .

```{r own-apply-2}
our_apply <- function(x, fun, ...) { # <1>
  val <- numeric(length(x))
  for (i in seq_along(x)) {
    val[i] <- fun(x[[i]], ...) # <2>
  }
  val
}
```

1. `...` in the argument list of `our_apply()` collects additional arguments.
2. `...` in the call to `fun()` passes these additional arguments to `fun()`.

. . .

```{r}
our_apply(1:10, rpois, n = 1)
```

# Benchmarking

## R Is Slow ...

... when used like a low-level language.

- R is an **interpreted** (as opposed to _compiled_) language.
- It was written mainly for specifying statistical models (not for developing
  new numerical methods).
- It is suitable for high-level programming where most low-level computations
  are implemented in a compiled language (e.g. `lm()` and `qr()`.)
- It is also quite old.

## R Is Fast ...

... when most computations are carried out by calls to compiled code.

```{r}
x <- rnorm(1e4)
bench_res <- bench::mark(
  loop = {
    y <- numeric(length(x))
    for (i in seq_along(x)) {
      y[i] <- 10 * x[i]
    }
    y
  },
  vectorized = 10 * x
)
```

## Plot Benchmark Results

```{r bench-plot}
#| fig-width: 5
#| fig-cap: Benchmark results for a loop vs. a vectorized computation
autoplot(bench_res)
```

## Vectorization

Vectorization is the process of rewriting code to use vectorized operations,
which operate on entire vectors at once, instead of using loops to operate on
individual elements.

. . .

The term is somewhat misleading, since it does not necessarily involve
vector processors.

. . .

### Example: Counting Zeros, Vectorized

```{r}
count_zeros_vec <- function(x) {
  sum(x == 0)
}
```

. . .

- `x == 0` checks if each entry of `x` is 0 and returns a vector of logicals.
- `sum()` computes and returns the sum of all elements in a vector. Logicals are
  coerced to integers.
- In this case the vectorized implementation is cohesive and clear.
- The vectorized computations are performed by compiled code (C/C++/Fortran),
  which run faster than pure R code.
- Writing vectorized code requires a larger knowledge of R functions.

## Beware of Loops in Disguise

Just because you ran a function, it does not mean that it is vectorized.

. . .

```{r}
#| fig-width: 5
#| fig-height: 2
bench::mark(
  sapply(x, function(x_i) 10 * x_i),
  10 * x
) |>
  plot()
```

## Development Cycle Sketch

- Is there a good-enough existing implemention for your problem? If yes, then
  you are done.
- If not, implement a solution and test it. Does it solve your problem
  sufficiently well? If yes, then you're done.
- If not, then profile (next week!), benchmark, and debug (week 5). Then
  refactor and optimize.

. . .

### The Root of All Evil

\medskip

> We _should_ forget about small efficiencies, say about 97% of the time:
> premature optimization is the root of all evil. Yet we should not pass up our
> opportunities in that critical 3%.
>
> _—Donald Knuth_

## Example: Density Estimation

```{r kernDens}
kern_dens <- function(x, h, m = 512) {
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)

  for (i in seq_along(xx)) {
    for (j in seq_along(x)) {
      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))
    }
  }

  y <- y / (sqrt(2 * pi) * h * length(x))

  list(x = xx, y = y)
}
```

## Vectorizing Our Density Estimator

```{r kernDens-vec}
kern_dens_vec <- function(x, h, m = 512) {
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)
  const <- (sqrt(2 * pi) * h * length(x))

  for (i in seq_along(xx)) {
    y[i] <- sum(exp(-(xx[i] - x)^2 / (2 * h^2))) / const
  }

  list(x = xx, y = y)
}
```

## Benchmarking

```{r kern-bench}
kern_bench <- bench::mark(
  kern_dens(phipsi$psi, 0.2),
  kern_dens_vec(phipsi$psi, 0.2)
)
```

## Plot Benchmark Results

```{r kern-bench-autoplot}
#| message: false
#| fig-height: 2.5
#| fig-width: 5
plot(kern_bench)
```

## Parameterized Benchmarking

```{r kern-bench-grid, cache = TRUE, message = FALSE}
kern_benchmarks <- bench::press(
  n = 2^(6:9),
  m = 2^(5:11),
  {
    bench::mark(
      loop = kern_dens(x[1:n], h = 0.2, m = m),
      vec = kern_dens_vec(x[1:n], h = 0.2, m = m)
    )
  }
)
```

## Plotting Results

```{r kern-bench-fig}
#| message: false
#| warning: false
#| fig-width: 7
library(tidyverse)
mutate(kern_benchmarks, expression = as.character(expression)) |>
  ggplot(aes(m, median, color = expression)) +
  geom_point() +
  geom_line() +
  facet_grid(cols = vars(n))
```

## Getting Help with R

### Google It

Especially good for error messages.

. . .

### Generative AI

- Also great for error messages and debugging
- _Caution_: You need to understand the results, especially when you ask it to
  create something for you.

. . .

### Absalon Discussion Forum

Use the fact that there are twenty other people in the course with exactly the
same problem.

# Exercises

### Exercise 1

Can you list three ways to access element `a` in this list?

```{r}
l <- list(a = 1, b = 2)
```

```{r, include=FALSE}
l[[1]]
l$a
l[["a"]]
```

. . .

### Exercise 2

Write a for loop that prints "even" if the loop variable is even, "odd" if the
loop variable is odd, and exits if is larger than 10.
