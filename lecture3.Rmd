---
title: "Benchmarking and Profiling"
subtitle: Computational Statistics
author: "Johan Larsson, Niels Richard Hansen"
date: "September 10, 2024"
---

```{r setup, echo=FALSE}
source(file.path("R", "kernel.R"))

knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.8,
  fig.retina = 3,
  fig.align = "center",
  cache = FALSE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

library(ggplot2)

source("R/kernel.R")

theme_set(theme_grey(base_size = 16))
```

## Today


### Measuring Performance

#### Profiling

Identifying bottlenecks in code

#### Benchmarking

Comparing performance of different implementations

--

### Improving Performance

Writing efficient code


---
## Profiling

A profiler quantifies how much time each part of a function takes up.

--

R uses a **sampling profiler**.

--

### profvis

The R package [profvis](https://CRAN.R-project.org/package=profvis) provides useful visualization tools. 
Can also be called activated through the RStudio IDE.

```{r load_profvis}
library(profvis)
```

--

### Remember!

> We *should* forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.
> Yet we should not pass up our opportunities in that critical 3%.
> 
> *—Donald Knuth*

---

## Example

We profile the following reimplementation of `gauss()`.

```{r gauss_step, eval=FALSE, out.width = 600}
gauss_step <- function(x, h = 1) {
  exponent <- x^2 / (2 * h^2)
  numerator <- exp(-exponent)
  denominator <- h * sqrt(2 * pi)
  numerator / denominator
}
```

---
## Usage

### Simple to Use

- First, make sure the code is sourced (`source()`).
- Run `profvis()` on the expression you want to profile.

```{r source_profvis, cache = TRUE, eval = FALSE}
source("R/lecture-3.R", keep.source = TRUE)
x <- rnorm(1e7)
profvis(gauss_step(x))
```

- The result is an interactive webpage.

---

## Benchmarking Density Estimation

Let's profile our kernel density function.

```{r}
kern_dens
```

---

## Disecting Expressions

The one-liner in the loop did too much (for profiling). `kern_dens_detail()` spells out the steps.

```{r kern_dens_detail, echo=FALSE}
kern_dens_detail
```

---
## Line Profiling

```{r profile_kern_dens_detail, eval=FALSE}
profvis(kern_dens_detail(x, 0.2))
```

The result from profiling this call is more informative.

---

## Exercise

- Profile this implementation of matrix-vector multiplication for an example of $n=p=100$.
- Before starting: where do you think the bottleneck is?

```{r matvec-mul-ex}
matrix_vector_multiplication <- function(x, y) {
  n <- NROW(x)

  out <- c()

  for (i in seq_len(n)) {
    xi_y <- t(x[, i]) %*% y
    out <- c(out, xi_y)
  }

  out
}
```

---

## Caveats About Line Profiling

- Anonymous functions make profiling more difficult; prefer using named functions.
- Resolution may not be enough for small pieces of code (but do you really need to profile them then?)

---

## Benchmarking

The purpose of benchmarking is to measure and compare the computational resources 
used by one or more implementations of a computation.

--

### Microbenchmarking

Benchmarking small pieces of code.

Typical aporoach: find bottleneck through profiling and then (micro)benchmark alternatives.

### The bench Package

We use the R package [bench](https://cran.r-project.org/web/package=bench) for benchmarking.

```{r}
library(bench)
```

---
## Example

```{r}
x <- runif(100)

bench::mark(
  sqrt(x),
  x^0.5
)
```

---
## Exercise

Consider the following implementation of the Gaussian kernel.

```{r}
gauss <- function(x, h = 1) {
  exp(-x^2 / (2 * h^2)) / (h * sqrt(2 * pi))
}
```

Benchmark `gauss()` against `dnorm()`. 

```{r, include = FALSE}
x <- seq(-3, 3, length.out = 10000)

bench::mark(
  gauss(x),
  dnorm(x)
)
```

---

## Benchmarking Density Estimation

```{r message = FALSE, cache = FALSE}
res <- bench::press(
  n = 2^(5:13),
  {
    x <- rnorm(2^13)
    bench::mark(
      base = density(x[1:n], 0.2),
      loop = kern_dens(x[1:n], 0.2),
      vectorized = kern_dens_vec(x[1:n], 0.2),
      check = FALSE
    )
  }
)
```

---

```{r, echo=FALSE, warning=FALSE}
library(tidyverse)
mutate(
  res,
  expr = as.character(expression),
  median = as.numeric(median)
) |>
  ggplot(aes(n, median, color = expr)) +
  geom_point() +
  geom_line() +
  scale_y_log10() +
  labs(y = "time (µs)")
```

[Source for R code](R/kernel.R)

---

class: center, middle

# Improving Performance

---

## Many Ways

### General Computational Strategies

- Avoiding copies (passing by reference)
- Parallelization

### R-Specific Tricks

- Vectorization

### Context-Specific Tricks

- Density destimation: binning (up next)

---

## Binning

The line profiler revealed that most time is spent on the
kernel evaluation.

--

A trick to reduce the $nm$ kernel evaluations to a smaller number is
**binning**.

--

(On the slides and in my implementations, $m$ denotes the number of grid points 
for the evaluation of the estimate and $n$ the length of the data vector.)

If $n < m$, then binning may not be beneficial.

---

## Binning

```{r}
kern_bin
```

The `kern_bin()` function is a loop along the data vector, and 
arithmetic is used to determine which bin center is closest to a 
data point. 

--

The `kern_dens_bin()` function (see [the source file](R/kernel.R)) computes bin weights 
using `kern_bin()` with grid points as bin centers. 

---

```{r micro3, cache = TRUE, echo=FALSE, message=FALSE}
res2 <- bench::press(
  n = 2^(5:13),
  {
    h <- 0.2
    x <- rnorm(n)
    bench::mark(
      base = density(x, h),
      loop = kern_dens(x, h),
      vectorized = kern_dens_vec(x, h),
      binning = kern_dens_bin(x, h),
      check = FALSE
    )
  }
)

mutate(
  res2,
  expr = as.character(expression),
  median = as.numeric(median)
) |>
  ggplot(aes(n, median, color = expr)) +
  geom_point() +
  geom_line() +
  scale_y_log10() +
  labs(y = "time (µs)")
```

The relative benefit of binning increases with the size of the data.

---
## Testing

```{r plot-kern-dens-benchmark, echo=-c(1:2), fig.width = 7, fig.asp = 0.7}
library(tidyverse)
set.seed(123)
x <- rnorm(1e4) + rnorm(1e4, -3, 0.7)

plot(kern_dens(x, 0.2), type = "l", lwd = 4)
lines(kern_dens_bin(x, 0.2), col = "red", lwd = 2)
```

---
## Testing

```{r test-kern-dens-bench, echo=FALSE, fig.width=7, fig.asp = 0.65}
x0 <- kern_dens(x, 0.2)$x
plot(
  x0,
  kern_dens(x, 0.2)$y - kern_dens_bin(x, 0.2)$y,
  type = "l",
  ylim = c(-3e-3, 3e-3),
  lwd = 2,
  ylab = "Difference"
)
lines(
  x0,
  kern_dens(x[1:1024], 0.2)$y - kern_dens_bin(x[1:1024], 0.2)$y,
  col = "red",
  lwd = 2
)
lines(
  x0,
  kern_dens(x[1:128], 0.2)$y - kern_dens_bin(x[1:128], 0.2)$y,
  col = "blue",
  lwd = 2
)
```

The absolute errors due to binning are small but increasing with 
decreasing length of data sequence. Here $n = 8192$ is black, $n = 1024$ 
is red and $n = 128$ is blue. 

---
## Line Profiling

The `kern_dens_bin()` function is so much faster for long sequences that to get good 
profiling results we use a 512 times longer data sequence.

```{r eval = FALSE}
x <- rnorm(2^22)
profvis(kern_dens_bin(x, 0.2))
```

---

## Vectorization

- Vectorization is the process of replacing loops with vectorized operations.
- These vectorized operations are also loops, but they are written in C instead or R.
  - Examples of vectorized functions: `mean()`, `sd()`
  - Examples of non-vectorized functions: `apply()`, `Vectorize()`
- Vectorizing code is often about finding the right function in R.

---

## Exercise on Vectorization

How do you vectorize this matrix multiplication function?

```{r matvec-mul-ex-vectorize}
matrix_vector_multiplication <- function(x, y) {
  n <- NROW(x)

  out <- c()

  for (i in seq_len(n)) {
    xi_y <- t(x[, i]) %*% y
    out <- c(out, xi_y)
  }

  out
}
```

```{r ex2-solution, include = FALSE}
matrix_vector_multiplication_vectorized <- function(x, y) {
  t(x) %*% y
}
```

---

## Avoiding Copies

- In R, objects are passed by reference, but when an object is modified a copy is created.
--

- For instance, when subsetting a matrix, a copy is created. It's not possible to access for instance a column by reference.
--

- Growing vectors (`c()`) and matrices (`rbind()`, `cbind()`) also creates copies.


```{r}
x <- rnorm(100)
x <- c(x, 4) # 101 values are allocated
```

---

## Memory

### Memory in R

In R, everything is typically loaded into memory.

### Garbage Collection

R includes a garbage collector, which intermittently releases unused blocks in memory.

### Trade-Offs

Storing intermediate objects that are used multiple times will boost performance at the cost of additional memory storage.

---

## Exercise

Let's say we want to minimize the following function with gradient descent:
$$f(x) = \frac{1}{2}x^2 - x.$$
The gradient is $f'(x) = x - 1$.

```{r}
my_optim <- function(x) {
  gradient_history <- c()

  while (TRUE) {
    gradient <- x - 1
    gradient_history <- c(gradient_history, gradient)

    x <- x - gradient

    if (abs(gradient) < 1e-6) {
      break
    }
  }

  x
}
```


---

## Storage Modes

.pull-left[
- In R, matrices are stored in column-major order.
- This means that when you access a column of a matrix, 
  you are accessing a contiguous block of memory.
- Prefer to work with matrices instead of data frames.
]
]

.pull-right[

]

---

## Exercise

```{r}
mat_vec_mul1 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- t(x[i, ]) %*% y
  }
  z
}
```


```{r, include = FALSE}
n <- 1e3
p <- 1e1

x <- matrix(rnorm(n * p), n)
y <- matrix(rnorm(p), p, 1)

f1 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- t(x[i, ]) %*% y
  }
  z
}

f2 <- function(x, y) {
  n <- NROW(x)
  x_t <- t(x)
  z <- double(n)

  for (i in seq_len(n)) {
    z[i] <- x_t[, i] %*% y
  }

  z
}

f3 <- function(x, y) {
  n <- NROW(x)
  z <- double(n)

  for (i in seq_len(NCOL(x))) {
    z <- z + x[, i] * y[i]
  }

  z
}

bench::mark(f1(x, y), f2(x, y), f3(x, y))
```

---

## Parallelization and Asynchronous Programming

- Modern processors today have multiple (physical and virtual) cores
--

- But unless instructed otherwise, only a single core is going to be used.

--

- The computer doesn't automatically know that your computations are safe to do in parallel.

---

### Embarassingly Parallel Tasks

Trivial implementation of parallelization

### Examples

- Summing a vector (or matrix): `sum()`
- Linear algebra operations: `%*%` (`crossprod()`)

