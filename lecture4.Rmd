---
title: "Nearest Neighbors and Smoothing Splines"
subtitle: "Computational Statistics"
author: "Johan Larsson Niels Richard Hansen"
date: "September 12, 2024"
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 7,
  fig.height = 4,
  fig.retina = 3,
  fig.align = "center",
  cache = FALSE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

library(tidyverse)
theme_set(theme_grey(base_size = 16))
```

## Today

### Nearest neighbors

### Smoothing Splines

### S3 Class for Smoothing Splines

---

## Nuuk Temperature Data

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
nuuk <- read_table(
  "data/nuuk.dat.txt",
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) %>%
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) %>%
  mutate(Temperature = Temperature / 10) %>%
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) %>%
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

```{r}
library(tidyverse)

p <- ggplot(nuuk_year, aes(Year, Temperature)) +
  geom_point()
p
```

Data available [here](data/nuuk.dat.txt).

---
## Nearest Neighbor Estimation

Data: $(x_1, y_1), \ldots, (x_n, y_n)$

The $k$ nearest neighbor smoother in $x$ is defined as 
$$\hat{f}(x) = \frac{1}{k} \sum_{j \in N(x)} y_j$$
where $N(x)$ is the set of indices for the $k$ nearest neighbors of $x$. 

--

This is an estimator of
$$f(x) = E(Y \mid X = x).$$


---
## Nearest Neighbor Estimation

The estimator in $x_i$ is 
$$\hat{f}_i = \frac{1}{k} \sum_{j \in N_i} y_j$$
where $N_i = N(x_i)$. 
--


With $S_{ij} = \frac{1}{k} 1(j \in N_i)$ and $\mathbf{S} = (S_{ij})$ 
$$\hat{\mathbf{f}} = (\hat{f}_i) = \mathbf{S} \mathbf{y}.$$
--

$\hat{\mathbf{f}}$ is an estimator of the vector $(f(x_i))$. 

---
## Linear Smoothers

Any estimator of $(f(x_i))$ of the form $\mathbf{S} \mathbf{y}$ 
for a *smoother matrix* $\mathbf{S}$ is called a *linear smoother*.
--


The $k$ nearest neighbor smoother is a simple example of a linear 
smoother that works for $x$-values in any metric space. 
--


The representation of a linear smoother as a matrix-vector product,
$$\mathbf{S} \mathbf{y}$$
is theoretically useful, but often not the best way to actually
compute $\hat{\mathbf{f}}$.

---
## Running Mean

When $x_i \in \mathbb{R}$ we can sort data according to $x$-values and then use 
a *symmetric* neighbor definition:

$$N_i = \{i - (k - 1) / 2, i - (k - 1) / 2 + 1, \ldots, i - 1 , i, i + 1, \ldots,   i + (k - 1) / 2\}$$

(for $k$ odd.)

--

This simplifies computations: we don't need to keep track of metric comparisons,
only the order matters. 

---
## Running Mean 

Implementation (assuming $y$ in correct order) using the identity
$$\hat{f}_{i+1} = \hat{f}_{i} - y_{i - (k-1)/2} / k + y_{i + (k + 1)/2} / k.$$


```{r runMean}
# The vector 'y' must be sorted according to the x-values
run_mean <- function(y, k) {
  n <- length(y)
  m <- floor((k - 1) / 2)
  k <- 2 * m + 1 # Ensures k to be odd and m = (k - 1) / 2
  y <- y / k
  s <- rep(NA, n)
  s[m + 1] <- sum(y[1:k])
  for (i in (m + 1):(n - m - 1)) {
    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]
  }
  s
}
```

---
## Visualization

```{r nuuk-NN-plot2, warning=FALSE}
f_hat <- run_mean(nuuk_year$Temperature, 11)
p + geom_line(aes(y = f_hat), color = "blue")
```

---
## Using `filter()`

The R function `filter()` computes running means and moving averages 

```{r runMeanCheck}
f_hat_filter <- stats::filter(nuuk_year$Temperature, rep(1 / 11, 11))
range(f_hat_filter - f_hat, na.rm = TRUE)
```

--
```{r, echo=2}
op <- options(digits = 2)
f_hat_filter[c(1:10, 137:147)]
options(digits = op$digits)
```


---
## Benchmarking (Using `bench::press()`)

```{r runMeanBench, echo=FALSE, warning=FALSE, message = FALSE, cache = TRUE, fig.height = 5.5, fig.width = 12}
nn_bench <- bench::press(
  n = c(512, 1024, 2048, 4196),
  {
    k <- 11
    y <- rnorm(n)
    w <- c(rep(1 / k, k), rep(0, n - 10))
    S <- matrix(w, n - 10, n, byrow = TRUE)
    bench::mark(
      S %*% y,
      run_mean(y, k = k),
      stats::filter(y, rep(1 / k, k)),
      check = FALSE
    )
  }
)

plot(nn_bench)
```

The Matrix-vector multiplication is $O(n^2)$.

--

The two other algorithms are $O(n)$. 

---

## Bias-Variance Trade-Off

If data is i.i.d. with $V(Y \mid X) = \sigma^2$ and $f(x) = E(Y \mid X = x)$.

\begin{aligned}
\mathrm{MSE}(x) & =  \operatorname{E}\big((f(x) - \hat{f}(x))^2\big) \\
& = \operatorname{E}\big((f(x)-\operatorname{E}(\hat{f}(x)))^2\big) + 
\operatorname{E}\big((\hat{f}(x)-\operatorname{E}(\hat{f}(x)))^2\big)
\end{aligned}

--
Thus

\begin{aligned}
\operatorname{MSE}(x) & = \underbrace{\left[f(x) - \frac{1}{k} \sum_{l \in
    N(x)} f(x_l)\right]^2}_{\text{squared bias}} + 
    \underbrace{\frac{\sigma^2}{k}}_{\text{variance}}
\end{aligned}


--

- Small $k$ gives large variance and small bias (if $f$ is smooth).
- Large $k$ gives small variance and potentially large bias (if $f$ is not constant).



---

## LOOCV

The running mean / nearest neighbour smoother is a *linear smoother*,
$\hat{\mathbf{f}} = \mathbf{S} \mathbf{Y}.$

--

How to predict $y_i$ if $(x_i, y_i)$ is left out?

--

A *definition* for a linear smoother is 
$$\hat{f}^{-i}_i = \sum_{j \neq i} \frac{S_{ij}y_j}{1 - S_{ii}}.$$

--


For many smoothing procedures with a natural "out-of-sample" prediction method
the identity above holds. 


---
## LOOCV 

It follows that for *leave-one-out cross validation*
$$\mathrm{LOOCV} = \sum_{i} (y_i - \hat{f}^{-i}_i)^2 = 
\sum_{i} \left(\frac{y_i - \hat{f}_i}{1 - S_{ii}}\right)^2$$

--

```{r LOOCV-runMean}
loocv <- function(k, y) {
  f_hat <- run_mean(y, k)
  mean(((y - f_hat) / (1 - 1 / k))^2, na.rm = TRUE)
}
```
--

Note that the implementation removes the missing values that are due to the way 
we handle the boundary, and it uses `mean()` instead of `sum()` to correctly adjust 
for this. 

---
##LOOCV 

```{r nuuk-running-loocv}
k <- seq(3, 40, 2)
CV <- sapply(k, function(kk) loocv(kk, nuuk_year$Temperature))
```
--

```{r nuuk-running-loocv-plot}
k_opt <- k[which.min(CV)]
qplot(k, CV) + geom_line() + geom_vline(xintercept = k_opt, color = "red")
```


---
##LOOCV


The optimal choice of $k$ is `r k_opt`.

```{r nuuk-NN-plot3, warning=FALSE}
p + geom_line(aes(y = run_mean(nuuk_year$Temperature, k_opt)), color = "blue")
```

---
## Smoothing Splines 

The minimizer of
$$L(f) = \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \underbrace{\int f''(z)^2 \mathrm{d} z}_{\|f''\|_2^2}$$
is a cubic spline
--

with *knots* in the data points $x_i$, that is, a function 
$$f = \sum_j \beta_j \varphi_j$$
where $\varphi_j$ are basis functions for the $n$-dimensional space of such splines.

--

Cubic splines are piecewise degree 3 polynomials in between knots.

---
## Smoothing splines 

In vector notation 
$$\hat{\mathbf{f}} = \boldsymbol{\Phi}\hat{\beta}$$
with $\boldsymbol{\Phi}_{ij} = \varphi_j(x_i)$, 
--

and
\begin{aligned}
L(\mathbf{f}) & = (\mathbf{y} - \mathbf{f})^T (\mathbf{y} - \mathbf{f}) + \lambda \|f''\|_2^2 \\
& = ( \mathbf{y} -  \boldsymbol{\Phi}\beta)^T (\mathbf{y} -  \boldsymbol{\Phi}\beta) + \lambda \beta^T \mathbf{\Omega} \beta
\end{aligned}
--


with 
$$\mathbf{\Omega}_{jk} = \int \varphi_j''(z) \varphi_k''(z) \mathrm{d}z.$$

---
## Smoothing splines 

The minimizer is 
$$\hat{\beta} = (\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T \mathbf{y}$$

--


with resulting smoother
$$\hat{\mathbf{f}} = \underbrace{\boldsymbol{\Phi} ((\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T}_{\mathbf{S}_{\lambda}} \mathbf{y}.$$

--

We recognize this as a *linear smoother* with smoother matrix $\mathbf{S}_{\lambda}$.


---
## Splines in R

```{r splines, fig.width = 10, fig.height = 5, message = FALSE, warning = FALSE}
library(Matrix)
library(splines)
# Note the specification of repeated boundary knots
knots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)
xx <- seq(0, 1, 0.005)
B_splines <- splineDesign(knots, xx)
matplot(xx, B_splines, type = "l", lty = 1)
```

---
## Penalty Matrix

```{r Omega-Simpson, echo=FALSE}
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) / 6
}
```

```{r}
# See slide source for implementation of `pen_mat()`
Omega <- pen_mat(seq(0, 1, 0.1))
image(Matrix(Omega))
```

---

## Fitting a Smoothing Spline

We implement the matrix-algebra directly
for computing $\mathbf{S}_{\lambda} \mathbf{y}$.

```{r nuuk-smooth-spline}
inner_knots <- nuuk_year$Year
knots <- c(rep(range(inner_knots), 3), inner_knots)
Phi <- splineDesign(knots, inner_knots) #<<
```

--

```{r nuuk-smooth-spline2}
Omega <- pen_mat(inner_knots)
smoother <- function(lambda) {
  Phi %*% solve(
    crossprod(Phi) + lambda * Omega, # Phi^T Phi + lambda Omega
    crossprod(Phi, nuuk_year$Temperature) # Phi^T y
  )
}
```

---

class: middle

```{r fig.width = 10}
p + geom_line(aes(y = smoother(10)), color = "steelblue4") + # Undersmooth
  geom_line(aes(y = smoother(1000)), color = "red") + # Smooth
  geom_line(aes(y = smoother(100000)), color = "orange") # Oversmooth
```

---
## Generalized Cross-Validation

With $\mathrm{df} = \mathrm{trace}(\mathbf{S}) = \sum_{i=1}^n S_{ii}$
we replace $S_{ii}$ in LOOCV by $\mathrm{df} / n$ to get the
*generalized* cross-validation criterion
$$\mathrm{GCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - \mathrm{df} / n}\right)^2.$$
--


```{r gcv-smooth-spline}
gcv <- function(lambda, y) {
  S <- Phi %*% solve(crossprod(Phi) + lambda * Omega, t(Phi))
  df <- sum(diag(S)) # The trace of the smoother matrix
  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE)
}
```


---
## Generalized Cross-Validation

Then we apply this function to a grid of $\lambda$-values and 
choose the value of $\lambda$ that minimizes GCV.

```{r nuuk-spline-gcv}
lambda <- seq(50, 250, 2)
GCV <- sapply(lambda, gcv, y = nuuk_year$Temperature)
lambda_opt <- lambda[which.min(GCV)]
```

---
## Generalized Cross-Validation

```{r gc-plot, fig.width = 10, fig.height = 5}
tibble(lambda, GCV) |>
  ggplot(aes(lambda, GCV)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = lambda_opt, color = "dark orange")
```

---
## Optimal Smoothing Spline

An optimal smoother using our implementation

```{r nuuk-spline-opt, fig.width = 9, fig.height = 4.5}
smooth_opt <- smoother(lambda_opt)
p + geom_line(aes(y = smooth_opt), color = "blue")
```

---
## Using `smooth.spline()`

Using `smooth.spline()` without fast heuristic.

```{r nuuk-spline-opt2, fig.width = 9, fig.height = 4.5}
smooth <- smooth.spline(nuuk_year$Year, nuuk_year$Temperature, all.knots = TRUE)
p + geom_line(aes(y = smooth$y), color = "blue")
```

---
## Efficient Computations

In practice we use $p < n$ basis functions.

Using the singular value decomposition
$$\Phi = \mathbf{U} D \mathbf{V}^T$$
--

it holds that 
$$\mathbf{S}_{\lambda} = \widetilde{\mathbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\mathbf{U}}^T$$
where $\widetilde{\mathbf{U}} = \mathbf{U} \mathbf{W}$ and 
$$D^{-1} \mathbf{V}^T \mathbf{\Omega} \mathbf{V} D^{-1} = \mathbf{W} \Gamma \mathbf{W}^T.$$

---

## Efficient Computations

The interpretation of this representation is as follows. 

* First, the coefficients, $\hat{\beta} = \widetilde{\mathbf{U}}^Ty$, are computed for 
expanding $y$ in the basis given by the columns of $\widetilde{\mathbf{U}}$. 
--


* Second, the $i$-th coefficient is shrunk towards 0,
$$\hat{\beta}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.$$
--


* Third, the smoothed values, $\widetilde{\mathbf{U}} \hat{\beta}(\lambda)$, 
are computed as an expansion using the shrunken coefficients. 

---

### The Demmler-Reinsch Basis (Columns of $\widetilde{\mathbf{U}}$)

```{r spline-diagonalization, echo=FALSE, fig.width = 12, fig.height = 7}
inner_knots <- seq(1867, 2013, length.out = 18)
Phi <- splineDesign(c(rep(range(inner_knots), 3), inner_knots), nuuk_year$Year)
Omega <- pen_mat(inner_knots)
Phi_svd <- svd(Phi)
Omega_tilde <- t(t(crossprod(Phi_svd$v, Omega %*% Phi_svd$v)) / Phi_svd$d) / Phi_svd$d
# It is safer to use the numerical singular value decomposition ('svd')
# for diagonalizing a positive semidefinite matrix than to use a
# more general numerical diagonalization implementation such as 'eigen'.
Omega_tilde_svd <- svd(Omega_tilde)
U_tilde <- Phi_svd$u %*% Omega_tilde_svd$u

colnames(U_tilde) <- paste(rep("u", 20), 1:20, sep = "")
bind_cols(select(nuuk_year, Year), as_tibble(U_tilde)) %>%
  gather(key = "term", value = "value", -Year, factor_key = TRUE) %>%
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

---

### and the Eigenvalues $\gamma_i$

```{r spline-gamma, echo=FALSE, fig.height = 6, fig.width = 10}
library(patchwork)

# pp <- qplot(1:20, Omega_tilde_svd$d) + xlab("i") + ylab("Eigenvalues")
d <- Omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2
```


