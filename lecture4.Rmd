---
title: "Scatterplot Smoothing"
subtitle: "Computational Statistics"
author: "Johan Larsson, Niels Richard Hansen"
date: "September 12, 2024"
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 7,
  fig.height = 4,
  fig.retina = 3,
  fig.align = "center",
  cache = FALSE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

xaringanExtra::use_tile_view()

library(tidyverse)

theme_set(theme_grey(base_size = 16))
```

## Today

### Nearest neighbors

Simple algorithm for nonparametric smoothing.

### Smoothing Splines

A generalization of polynomial regression.

### S3 Smoother

We develop a S3 smoother for ggplot2.

---

## Nuuk Temperature Data

```{r looad-nuuk-data, echo=FALSE, message=FALSE}
nuuk <- read_table(
  "data/nuuk.dat.txt",
  col_names = c("Year", 1:12),
  na = "-999",
  skip = 1
) |>
  gather(key = "Month", value = "Temperature", -Year, convert = TRUE) |>
  mutate(Temperature = Temperature / 10) |>
  filter(Year > 1866)

nuuk_year <- group_by(nuuk, Year) %>%
  summarise(
    Temperature = mean(Temperature),
    Median = median(Temperature),
    High = max(Temperature),
    Low = min(Temperature)
  )
n <- nrow(nuuk_year)
```

```{r}
library(tidyverse)

p <- ggplot(nuuk_year, aes(Year, Temperature)) +
  geom_point()
p
```

Data available [here](data/nuuk.dat.txt).

???

- Annual average temperature in Nuuk, Greenland

---
## Nearest Neighbor Estimation

Data: $(x_1, y_1), \ldots, (x_n, y_n)$

The $k$ nearest neighbor smoother in $x$ is defined as 
$$\hat{f}(x) = \frac{1}{k} \sum_{j \in N(x)} y_j$$
where $N(x)$ is the set of indices for the $k$ nearest neighbors of $x$. 

--

This is an estimator of
$$f(x) = E(Y \mid X = x).$$

???

- $N(x)$ is the neighborhood of $x$.

---
## Nearest Neighbor Estimation

The estimator in $x_i$ is
$$\hat{f}_i = \frac{1}{k} \sum_{j \in N_i} y_j$$
where $N_i = N(x_i)$.
--


$S_{ij} = \frac{1}{k} \mathbf{1}_{N_i}(j)$, $\mathbf{S} = (S_{ij})$, and
$$\hat{\mathbf{f}} = (\hat{f}_i) = \mathbf{S} \mathbf{y}.$$

--

$\hat{\mathbf{f}}$ is an estimator of the vector $(f(x_i))$. 

???

- The vector $\hat{f}$ can be computed as a linear function of $\mathbf{y}$.
- $\mathbf{S}$ is a relatively sparse $n \times n$ matrix.

---
## Linear Smoothers

$(f(x_i))$ of the form $\mathbf{S} \mathbf{y}$ 
for a *smoother matrix* $\mathbf{S}$ is called a *linear smoother*.

--

The $k$ nearest neighbor smoother is a simple example of a linear
smoother that works for $x$-values in any metric space.

--

The representation of a linear smoother as a matrix-vector product,
$$\mathbf{S} \mathbf{y}$$
is theoretically useful, but often not the best way to actually
compute $\hat{\mathbf{f}}$.

---
## Running Mean

When $x_i \in \mathbb{R}$ we can sort data according to $x$-values and then use 
a *symmetric* neighbor definition:

$$N_i = \{i - (k - 1) / 2, i - (k - 1) / 2 + 1, \ldots, i - 1 , i, i + 1, \ldots,   i + (k - 1) / 2\}$$

(for $k$ odd.)

--

This simplifies computations: we don't need to keep track of metric comparisons,
only the order matters.

???

- Symmetric neighborhoods do not take distance into account.
- Only need to sort
- Only order matters

---

## Running Mean (Naive Implementation)

Assume $y$ is sorted and $k$ is odd.

```{r runmean-simple}
run_mean_naive <- function(y, k) {
  n <- length(y)
  m <- (k - 1) / 2
  y <- y / k
  s <- rep(NA, n)

  for (i in (m + 1):(n - m - 1)) {
    s[i] <- mean(y[(i - m):(i + m)])
  }

  s
}
```

???

- Can speed this up by replacing mean computation

---

## Running Mean

Implementation (assuming $y$ is sorted) using the identity
$$\hat{f}_{i+1} = \hat{f}_{i} - \frac{y_{i - (k-1)/2}}{k} + \frac{y_{i + (k + 1)/2}}{k}.$$

--

```{r run_mean_def}
run_mean <- function(y, k) {
  n <- length(y)
  m <- floor((k - 1) / 2)
  k <- 2 * m + 1 # Ensures k is odd and m = (k - 1) / 2
  y <- y / k
  s <- rep(NA, n)
  s[m + 1] <- sum(y[1:k])

  for (i in (m + 1):(n - m - 1)) {
    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]
  }
  s
}
```

???

- Ensures $k$ is odd


---

## Visualization

```{r nuuk-NN-plot2, warning=FALSE, fig.width = 10, fig.height = 5}
f_hat <- run_mean(nuuk_year$Temperature, 11)
p + geom_line(aes(y = f_hat), color = "blue")
```

???

- Takes 5 to the right and 5 to the left

---

## `stats::filter()`

The R function `stats::filter()` applies linear filtering (moving averages or autoregression).

--

```{r run-mean-nas, echo=2}
op <- options(digits = 2)
f_hat_filter <- stats::filter(nuuk_year$Temperature, rep(1 / 11, 11))
f_hat_filter[c(1:10, 137:147)]
options(digits = op$digits)
```

--

```{r runmean-test}
library(testthat)

test_that("filter() and run_mean() work the same", {
  f_hat <- run_mean(nuuk_year$Temperature, 11)
  f_hat_filter <- stats::filter(nuuk_year$Temperature, rep(1 / 11, 11))
  expect_equivalent(f_hat, as.numeric(f_hat_filter))
})
```

--

.footnote[
There is a `filter()` function in the **dplyr** package (part of the tidyverse), so look 
out for name clashes. Safest to call `stats::filter()`.
]

???

- `filter()` general linear filtering function
- Here, `rep(1, /11, 11)` are MA coefficients

---

## Benchmarking (Using `bench::press()`)

```{r runMeanBench, echo=FALSE, warning=FALSE, message = FALSE, cache = TRUE, fig.height = 5.5, fig.width = 12}
nn_bench <- bench::press(
  n = c(512, 1024, 2048, 4196),
  {
    k <- 11
    y <- rnorm(n)
    w <- c(rep(1 / k, k), rep(0, n - 10))
    S <- matrix(w, n - 10, n, byrow = TRUE)
    bench::mark(
      S %*% y,
      run_mean(y, k = k),
      stats::filter(y, rep(1 / k, k)),
      check = FALSE
    )
  }
)

plot(nn_bench)
```

The Matrix-vector multiplication is $O(n^2)$.

--

The two other algorithms are $O(n)$. 

---

## Mean-Squared Error

If data is i.i.d. with $V(Y \mid X) = \sigma^2$ and $f(x) = E(Y \mid X = x)$,

\begin{aligned}
\mathrm{MSE}(x) & = \operatorname{E}(f(x) - \hat{f}(x))^2 \\
                & = \operatorname{Var}\big(f(x)-\hat{f}(x)\big) + \big(\operatorname{E}(f(x)-\hat{f}(x))\big)^2\\
                & = \underbrace{\frac{\sigma^2}{k}}_{\text{variance}} + \underbrace{\left(f(x) - \frac{1}{k} \sum_{l \in N(x)} f(x_l)\right)^2}_{\text{squared bias}}.
\end{aligned}

--

### Bias-Variance Trade-Off

- Small $k$ gives large variance and small bias (if $f$ is smooth).
- Large $k$ gives small variance and potentially large bias (if $f$ is not constant).


---

## Leave-One Out Cross-Validation (LOOCV)

The running mean/nearest neighbour smoother is a *linear smoother*, $\hat{\mathbf{f}} = \mathbf{S} \mathbf{Y}$.

--

How to predict $y_i$ if $(x_i, y_i)$ is left out?

--

A *definition* for a linear smoother is 
$$\hat{f}^{-i}_i = \sum_{j \neq i} \frac{S_{ij}y_j}{1 - S_{ii}}.$$

--

For many smoothing procedures with a natural "out-of-sample" prediction method,
the identity above holds. 

???

- Can for running mean in principle compute this without leaving
  anything out.

---
## LOOCV 

It follows that for **leave-one-out cross validation,**
$$\mathrm{LOOCV} = \sum_{i} (y_i - \hat{f}^{-i}_i)^2 = 
\sum_{i} \left(\frac{y_i - \hat{f}_i}{1 - S_{ii}}\right)^2$$

--

### Implementation

```{r LOOCV-runMean}
loocv <- function(k, y) {
  f_hat <- run_mean(y, k)
  mean(((y - f_hat) / (1 - 1 / k))^2, na.rm = TRUE)
}
```

.footnote[
The implementation removes missing values due to the way 
we handle the boundaries, and it uses `mean()` instead of `sum()` to correctly adjust 
for this.
]

???

- For nearest neighbors, $S_{ii} = \frac{1}{k}$ for all $i$.
- **Derive the formula on the blackboard!**

---

## LOOCV 

```{r nuuk-running-loocv}
k <- seq(3, 40, 2)
cv_error <- sapply(k, function(kk) loocv(kk, nuuk_year$Temperature))
k_opt <- k[which.min(cv_error)]
```
--

```{r nuuk-running-loocv-plot, echo = FALSE, fig.width = 9, fig.height = 5}
ggplot(tibble(k, cv_error), aes(k, cv_error)) +
  geom_vline(xintercept = k_opt, color = "dark orange") +
  geom_line() +
  geom_point() +
  labs(y = "LOOCV Error", x = expression(k))
```

---

## LOOCV


The optimal choice of $k$ is `r k_opt`.

```{r nuuk-NN-plot3, warning=FALSE, fig.width = 9, fig.height = 5, echo = FALSE}
p + geom_line(aes(y = run_mean(nuuk_year$Temperature, k_opt)), color = "blue")
```

---

## Wiggliness

- Running mean is wiggly!
--

- We can use kernel smoothing (as we did last week) to fix this
--

- But another idea is to use smoothing splines instead.

---

class: center, middle

# Smoothing Splines

---

## Smoothing Splines

The minimizer of
$$L(f) = \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \underbrace{\int f''(z)^2 \mathrm{d} z}_{\|f''\|_2^2}$$
is a **cubic spline**
--

with **knots** in the data points $x_i$, that is, a function
$$f = \sum_j \beta_j \varphi_j$$
where $\varphi_j$ is a basis function for the $n$-dimensional space of such splines.

--

Cubic splines are piecewise degree 3 polynomials in between knots.

???

- Without penalty term, any interpolating function would work well.
- Penalty term penalizes solutions with large second derivatives (in norm).
- **Derive the minimizer on the blackboard!**

---

### Loss Function

In vector notation
$$\hat{\mathbf{f}} = \boldsymbol{\Phi}\hat{\beta}$$
with $\boldsymbol{\Phi}_{ij} = \varphi_j(x_i)$,
--

and
\begin{aligned}
L(\mathbf{f}) & = (\mathbf{y} - \mathbf{f})^T (\mathbf{y} - \mathbf{f}) + \lambda \|f''\|_2^2 \\
& = ( \mathbf{y} -  \boldsymbol{\Phi}\beta)^T (\mathbf{y} -  \boldsymbol{\Phi}\beta) + \lambda \beta^T \mathbf{\Omega} \beta
\end{aligned}
--

with 
$$\mathbf{\Omega}_{jk} = \int \varphi_j''(z) \varphi_k''(z) \mathrm{d}z.$$

???

- Can rewrite penalty term in quadratic form $\lambda \beta^T \mathbf{\Omega} \beta$.

---

### Solution

The minimizer is 
$$\hat{\beta} = (\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T \mathbf{y}$$

--

with resulting smoother
$$\hat{\mathbf{f}} = \underbrace{\boldsymbol{\Phi} (\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T}_{\mathbf{S}_{\lambda}} \mathbf{y}.$$

--

We recognize this as a *linear smoother* with smoother matrix $\mathbf{S}_{\lambda}$.

???

- Similar to ridge (Tikhonov) regression: OLS + squared l2 penalty.

---

## Splines in R

```{r splines, fig.width = 10, fig.height = 5, message = FALSE, warning = FALSE}
library(Matrix)
library(splines)
# Note the specification of repeated boundary knots
knots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)
xx <- seq(0, 1, 0.005)
b_splines <- splineDesign(knots, xx)
matplot(xx, b_splines, type = "l", lty = 1)
```

???

- Repeated knots at boundaries are necessary to ensure 
  function goes throgh control points and is smooth.

---
## Penalty Matrix

```{r Omega-Simpson, echo=FALSE}
pen_mat <- function(inner_knots) {
  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))
  d <- diff(inner_knots) # the vector of knot differences; b - a
  g_ab <- splineDesign(knots, inner_knots, derivs = 2)
  knots_mid <- inner_knots[-length(inner_knots)] + d / 2
  g_ab_mid <- splineDesign(knots, knots_mid, derivs = 2)
  g_a <- g_ab[-nrow(g_ab), ]
  g_b <- g_ab[-1, ]
  (crossprod(d * g_a, g_a) +
    4 * crossprod(d * g_ab_mid, g_ab_mid) +
    crossprod(d * g_b, g_b)) / 6
}
```

.middle[
.pull-left[
```{r Omega-Simpson2, eval=FALSE}
omega <- pen_mat(seq(0, 1, 0.1))

image(Matrix(omega))
```
]

.pull-right[
```{r Omega-Simpson3, echo=FALSE, ref.label="Omega-Simpson2"}
```
]
]

--

.footnote[
See [slide source](/lecture4.Rmd) for implementation of `pen_mat()`.
]

---

## Fitting a Smoothing Spline

We implement the matrix-algebra directly
for computing $\mathbf{S}_{\lambda} \mathbf{y}$.

```{r nuuk-smooth-spline}
inner_knots <- nuuk_year$Year

# Note that order does not matter
knots <- c(rep(range(inner_knots), 3), inner_knots)

phi <- splineDesign(knots, inner_knots) #<<
```

--

```{r nuuk-smooth-spline2}
omega <- pen_mat(inner_knots)
smoother <- function(lambda) {
  phi %*% solve(
    crossprod(phi) + lambda * omega, # Phi^T Phi + lambda Omega
    crossprod(phi, nuuk_year$Temperature) # Phi^T y
  )
}
```

---

```{r nuuk-smoother-plot, fig.width = 10, fig.height = 6}
p + geom_line(aes(y = smoother(10)), color = "steelblue4") + # Undersmooth
  geom_line(aes(y = smoother(1000)), color = "red") + # Smooth
  geom_line(aes(y = smoother(100000)), color = "orange") # Oversmooth
```

---
## Generalized Cross-Validation

With $\mathrm{df} = \mathrm{trace}(\mathbf{S}) = \sum_{i=1}^n S_{ii}$
we replace $S_{ii}$ in LOOCV by $\mathrm{df} / n$ to get the
**generalized** cross-validation criterion
$$\mathrm{GCV} = \sum_{i=1}^n \left(\frac{y_i - \hat{f}_i}{1 - \mathrm{df} / n}\right)^2.$$
--

### Implementation

```{r gcv-smooth-spline}
gcv <- function(lambda, y) {
  S <- phi %*% solve(crossprod(phi) + lambda * omega, t(phi))
  df <- sum(diag(S)) # The trace of the smoother matrix
  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE)
}
```


---
### GCV-Optimal $\lambda$

Apply `gcv()` across grid of $\lambda$-values and
choose $\lambda$ that minimizes GCV.

```{r nuuk-spline-gcv}
lambda <- seq(50, 250, 2)
gcv <- sapply(lambda, gcv, y = nuuk_year$Temperature)
lambda_opt <- lambda[which.min(gcv)]
```

```{r gc-plot, fig.width = 10, fig.height = 4.5, echo = FALSE}
tibble(lambda, gcv) |>
  ggplot(aes(lambda, gcv)) +
  geom_vline(xintercept = lambda_opt, color = "dark orange") +
  geom_line() +
  geom_point() +
  labs(y = "GCV Error", x = expression(lambda))
```

---

### GCV-Optimal Smoothing Spline

```{r nuuk-spline-opt, fig.width = 9, fig.height = 5}
smooth_opt <- smoother(lambda_opt)
p + geom_line(aes(y = smooth_opt), color = "blue")
```

---

### Using `smooth.spline()`

```{r nuuk-spline-opt2, fig.width = 9, fig.height = 5}
smooth <- smooth.spline(nuuk_year$Year, nuuk_year$Temperature, all.knots = TRUE)
p + geom_line(aes(y = smooth$y), color = "blue")
```

???

- Disable fast heuristic to have similar results as us.

---

## Efficient Computations

In practice we use $p < n$ basis functions.

Using the singular value decomposition
$$\Phi = \mathbf{U} D \mathbf{V}^T$$
--

it holds that 
$$\mathbf{S}_{\lambda} = \widetilde{\mathbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\mathbf{U}}^T$$
where $\widetilde{\mathbf{U}} = \mathbf{U} \mathbf{W}$ and 
$$D^{-1} \mathbf{V}^T \mathbf{\Omega} \mathbf{V} D^{-1} = \mathbf{W} \Gamma \mathbf{W}^T.$$

???

- Can be done whether we use all or a subset of our basis functions
- $S_\lambda$ is diagonalized
- DERIVE $\mathbf{S}_\lambda$ ON BLACKBOARD!

---

### Interpretation

* The coefficients, $\hat{\beta} = \widetilde{\mathbf{U}}^Ty$, are computed for 
  expanding $y$ in the basis given by the columns of $\widetilde{\mathbf{U}}$.
--

* The $i$-th coefficient is shrunk towards 0,
  $$\hat{\beta}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.$$
--

* The smoothed values, $\widetilde{\mathbf{U}} \hat{\beta}(\lambda)$, 
  are computed as an expansion using the shrunken coefficients. 

---

### The Demmler-Reinsch Basis (Columns of $\widetilde{\mathbf{U}}$)

```{r spline-diagonalization, echo=FALSE, fig.width = 12, fig.height = 7}
inner_knots <- seq(1867, 2013, length.out = 18)
phi <- splineDesign(c(rep(range(inner_knots), 3), inner_knots), nuuk_year$Year)
omega <- pen_mat(inner_knots)
phi_svd <- svd(phi)
omega_tilde <- t(t(
  crossprod(phi_svd$v, omega %*% phi_svd$v)
) / phi_svd$d) / phi_svd$d

# It is safer to use the numerical singular value decomposition ('svd')
# for diagonalizing a positive semidefinite matrix than to use a
# more general numerical diagonalization implementation such as 'eigen'.
omega_tilde_svd <- svd(omega_tilde)
u_tilde <- phi_svd$u %*% omega_tilde_svd$u

colnames(u_tilde) <- paste(rep("u", 20), 1:20, sep = "")
bind_cols(select(nuuk_year, Year), as_tibble(u_tilde)) %>%
  gather(key = "term", value = "value", -Year, factor_key = TRUE) %>%
  ggplot(aes(Year, value, color = term)) +
  geom_line() +
  facet_wrap(~term) +
  theme(legend.position = "none") +
  ylab("")
```

---

### The Eigenvalues $\gamma_i$

```{r spline-gamma, echo=FALSE, fig.height = 7, fig.width = 10}
library(patchwork)

d <- omega_tilde_svd$d

eigen_data <- tibble(i = 1:20, Eigenvalues = d)

p1 <- ggplot(eigen_data, aes(i, Eigenvalues)) +
  geom_point() +
  labs(y = expression(gamma[i]))

p2 <- p1 + scale_y_log10()

p1 / p2
```

---

class: middle, center

## S3 Smoother

---

## A LOESS Smoother

LOESS: locally weighted scatterplot smoothing

```{r loess, fig.height=4, fig.width = 10}
p + geom_smooth()
```

.footnote[
Default behavior in ggplot2 is to use LOESS for small data sets and
generalized additive models (GAMs) for larger data sets.
]

---

## LOESS

- A mix of nearest neighbors and smoothing splines.
--

- Implemented in `loess()`
--

- Does not automatically select the span (tuning parameter)
--

- Default span is 0.75, often too large
--

- Nonlinear, so the formulas for linear smoothers do not apply.
--

- Instead use 5- or 10-fold cross validation for tuning.
--

- Or use the GCV criterion (using `trace.hat` entry in the returned object).

--

Loess is a **robust** smoother (linear smoothers are not) and relatively insensitive 
to outliers.

---
## Another LOESS Smoother

```{r loess2, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "loess", span = 0.5)
```

---
## A Linear "Smoother"

```{r linear-smoother, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "lm")
```

---
## A Polynomial Smoother

```{r poly-smoother, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "lm", formula = y ~ poly(x, 5))
```

---
## Another Polynomial Smoother

```{r poly-smother2, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "lm", formula = y ~ poly(x, 20))
```

---
## A Spline Smoother

```{r spline-smoother, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "gam", formula = y ~ s(x))
```

---
## Another Spline Smoother

```{r spline-smoother2, fig.height = 5, fig.width = 10}
p + geom_smooth(method = "gam", formula = y ~ s(x, k = 100))
```



---
## Smoothing with ggplot2 

The `geom_smooth()` function easily adds miscellaneous model fits or scatter plot smoothers
to the scatter plot.

--

Spline smoothing is performed via the `gam()` function in the mgcv package, whereas
loess smoothing is via the `loess()` function in the stats package. 

--

Any "smoother" can be used that supports a formula interface and has a prediction 
function adhering to the standards of `predict.lm()`.

---

## Our Running Mean Implementation

```{r run_mean_def_again, ref.label="run_mean_def"}
run_mean
```

---

## An Interface for `geom_smooth()`

```{r}
running_mean <- function(..., data, k = 5) {
  ord <- order(data$x)
  s <- run_mean(data$y[ord], k = k)
  structure(
    list(x = data$x[ord], y = s),
    class = "running_mean"
  )
}
```

--

### Prediction Method

```{r}
predict.running_mean <- function(object, newdata, ...) {
  approx(object$x, object$y, newdata$x)$y # Linear interpolation
}
```

---
## A Running Mean

```{r, warning=FALSE, fig.height=5, fig.width = 10}
p + geom_smooth(method = "running_mean", se = FALSE, n = 200)
```

---
## Specifying Arguments to Our Method

```{r, warning=FALSE, fig.height=4, fig.width = 10}
p + geom_smooth(
  method = "running_mean",
  se = FALSE,
  n = 200,
  method.args = list(k = 13)
)
```

---

## Handling Boundary Values

```{r}
running_mean <- function(..., data, k = 5, boundary = NULL) {
  ord <- order(data$x)
  y <- data$y[ord]
  n <- length(y)
  m <- floor((k - 1) / 2)
  if (m > 0 && !is.null(boundary)) {
    if (boundary == "pad") {
      y <- c(rep(y[1], m), y, rep(y[n], m)) #<<
    }
    if (boundary == "rev") {
      y <- c(y[m:1], y, y[n:(n - m + 1)]) #<<
    }
  }
  s <- run_mean(y, k = k)
  if (!is.null(boundary)) {
    s <- na.omit(s)
  }
  structure(list(x = data$x[ord], y = s), class = "running_mean")
}
```

---

### No Boundary Adjustment

```{r, warning=FALSE, echo = FALSE, message=FALSE, fig.height = 5, fig.width = 10}
smooth1 <- geom_smooth(
  method = "running_mean", se = FALSE, n = 200,
  method.args = list(k = 13)
)
p + smooth1
```

---

### Padding

```{r, warning=FALSE, echo = FALSE, message=FALSE, fig.height = 5, fig.width = 10}
smooth2 <- geom_smooth(
  method = "running_mean", se = FALSE, n = 200,
  method.args = list(k = 13, boundary = "pad"),
  color = "red"
)
p + smooth2 + smooth1
```

---

### Reversion

```{r, warning=FALSE, echo = FALSE, message=FALSE, fig.height = 5, fig.width = 10}
smooth3 <- geom_smooth(
  method = "running_mean", se = FALSE, n = 200,
  method.args = list(k = 13, boundary = "rev"),
  color = "purple"
)
p + smooth3 + smooth2 + smooth1
```

