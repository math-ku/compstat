@article{nesterovMethodSolvingConvex1983,
  title         = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}}(1/K\textasciicircum 2)},
  author        = {Nesterov, Yuri},
  date          = {1983},
  journaltitle  = {Doklady Akademii Nauk SSSR},
  volume        = {269},
  number        = {3},
  pages         = {543--547},
  url           = {https://cir.nii.ac.jp/crid/1570572699326076416},
  urldate       = {2024-01-16},
  file          = {/home/jola/Zotero/storage/3R4RVMNF/nesterov_1983_a_method_for_unconstrained_convex_minimization_problem_with.pdf;/home/jola/Zotero/storage/WUZXKJHX/1570572699326076416.html}
}
@article{gohWhyMomentumReally2017,
  title         = {Why Momentum Really Works},
  author        = {Goh, Gabriel},
  date          = {2017},
  journaltitle  = {Distill},
  doi           = {10.23915/distill.00006},
  url           = {http://distill.pub/2017/momentum}
}
@article{lessardAnalysisDesignOptimization2016,
  title         = {Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author        = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  date          = {2016-01},
  journaltitle  = {SIAM Journal on Optimization},
  shortjournal  = {SIAM J. Optim.},
  volume        = {26},
  number        = {1},
  pages         = {57--95},
  publisher     = {{Society for Industrial and Applied Mathematics}},
  issn          = {1052-6234},
  doi           = {10.1137/15M1009597},
  url           = {https://epubs.siam.org/doi/abs/10.1137/15M1009597},
  urldate       = {2024-10-17},
  abstract      = {We provide a framework for computing the exact worst-case performance of any algorithm belonging to a broad class of oracle-based first-order methods for composite convex optimization, including those performing explicit, projected, proximal, conditional, and inexact (sub)gradient steps. We simultaneously obtain tight worst-case guarantees and explicit instances of optimization problems on which the algorithm reaches this worst-case. We achieve this by reducing the computation of the worst-case to solving a convex semidefinite program, generalizing previous works on performance estimation by Drori and Teboulle  [Math. Program., 145 (2014), pp. 451--482] and the authors [A. B. Taylor, J. M. Hendrickx, and F. Glineur, Math. Program., 161 (2017), pp. 307--345]. We use these developments to obtain a tighter analysis of the proximal point algorithm and of several variants of fast proximal gradient, conditional gradient, subgradient, and alternating projection methods. In particular, we present a new analytical worst-case guarantee for the proximal point algorithm that is twice better than previously known and improve the standard worst-case guarantee for the conditional gradient method by more than a factor of two. We also show how the optimized gradient method proposed by Kim and Fessler [Math. Program., 159 (2016), pp. 81--107] can be extended by incorporating a projection or a proximal operator, which leads to an algorithm that converges in the worst-case twice as fast as the standard accelerated proximal gradient method [A. Beck and M. Teboulle, SIAM J. Imaging Sci., 2 (2009), pp. 183--202].},
  keywords      = {momentum,optimization},
  file          = {/home/jola/Zotero/storage/5MYJDUU5/Lessard et al. - 2016 - Analysis and design of optimization algorithms via integral quadratic constraints.pdf}
}
