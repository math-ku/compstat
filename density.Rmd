---
title: "Density Estimation"
subtitle: Computational Statistics
author: "Johan Larsson, Niels Richard Hansen"
date: "September 5, 2024"
---

```{r init, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)

knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  fig.align = "center",
  cache = TRUE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

theme_set(theme_grey(base_size = 16))

load("top100dih.RData")

phipsi2 <- na.omit(dataset)
phipsi <- read.table("phipsi.tsv", header = TRUE)
phipsi[, c("phi", "psi")] <- pi * phipsi[, c("phi", "psi")] / 180
```


## Angle Data

```{r, fig.asp = 0.8}
hist(phipsi$psi, prob = TRUE, xlab = expression(psi), main = NULL)
rug(phipsi$psi)
```

---

## Density Estimation

Let $f_0$ denote the unknown desnity we want to estimate.

- If we fit a parameterize statistical model $(f_\theta)_\theta$ to
  data using the estimator $\hat{\theta}$, then $f_{\hat{\theta}}$ 
  is an estimate of $f_0$.
--

- The histogram is a nonparameteric density estimator, $\hat{f}$.
--

- We are interested in nonparametric estimators because
  * we want to compare data with the parametric estimate,
  * we don't know a suitable parametric model, and
  * we want aid in visualization.

---

## Density Estimation

Density estimation relies on the approximation
$$P(X \in (x-h, x+h)) = \int_{x-h}^{x+h} f_0(z) \ dz \simeq f_0(x) 2h.$$
--

Rearranging and using LLN gives

\begin{align*}
  f_0(x) & \simeq \frac{1}{2h}P(X \in (x-h, x+h)) \\
  & \simeq \frac{1}{2h} \frac{1}{n} \sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\
  & =  \frac{1}{2hn} \sum_{i=1}^n 1_{(-h, h)}(x - x_i) = \hat{f}_h(x)
\end{align*}

---

## Kernels 

We will consider *kernel estimators* 
$$\hat{f}_h(x) = \frac{1}{hn} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right).$$

--
The *uniform* or *rectangular kernel* is 
$$K(x) = \frac{1}{2} 1_{(-1,1)}(x).$$
--
The *Gaussian kernel* is 
$$K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.$$

---

## Show And Tell

---

## Implementation with Gaussian Kernel

```{r kern-dens-impl}
kern_dens <- function(x, h, m = 512) {
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)

  for (i in seq_along(xx)) {
    for (j in seq_along(x)) {
      y[i] <- y[i] + exp(-(xx[i] - x[j])^2 / (2 * h^2))
    }
  }

  y <- y / (sqrt(2 * pi) * h * length(x))

  list(x = xx, y = y)
}
```

---
## Test

```{r}
f_hat <- kern_dens(phipsi$psi, 0.2)
f_hat_dens <- density(phipsi$psi, 0.2)
```

.pull-left[
```{r test-dens, eval = FALSE}
plot(
  f_hat,
  type = "l",
  lwd = 4,
  xlab = "x",
  ylab = "Density"
)

lines(
  f_hat_dens,
  col = "red",
  lwd = 2
)
```
]

.pull-right[
```{r test-dens-output, ref.label="test-dens", echo=FALSE, fig.width = 6, fig.asp = 0.9}
```
]

---
## Test

```{r, fig.width = 7, fig.asp = 0.7}
plot(
  f_hat$x,
  f_hat$y - f_hat_dens$y,
  type = "l",
  lwd = 2,
  xlab = "x",
  ylab = "Difference"
)
```

---

---
## Density Estimation

For a parametric family we can use the MLE
$$\hat{\theta} = \text{arg max}_{\theta} \sum_{j=1}^n \log f_{\theta}(x_j).$$

--
For nonparametric estimation we can still introduce the log-likelihood:
$$\ell(f) = \sum_{j=1}^n \log f(x_j)$$
--
Let's see what happens for the Gaussian kernel density estimate
$$f(x) = f_h(x) = \frac{1}{nh \sqrt{2 \pi}} \sum_{j=1}^n e^{- \frac{(x - x_j)^2}{2 h^2} }.$$

---

## Density Estimation

```{r, fig.align='center'}
f_h <- function(x, h) mean(dnorm(x, phipsi$psi, h))
f_h <- Vectorize(f_h)
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 1), add = TRUE, col = "red")
```

---
## Density Estimation, $h = 1$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 1), add = TRUE, col = "red")
```

---
## Density Estimation, $h = 0.25$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.25), add = TRUE, col = "red", n = 1001)
```

---
## Density Estimation, $h = 0.01$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.1), add = TRUE, col = "red", n = 1001)
```

---
## Density Estimation, $h = 0.025$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.025), add = TRUE, col = "red", n = 10001)
```

---
## Density Estimation, $h = 0.01$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.01), add = TRUE, col = "red", n = 10001)
```

---
## Density Estimation, $h \to 0$

```{r, echo = FALSE}
hist(phipsi$psi, prob = TRUE)
rug(phipsi$psi)
curve(f_h(x, 0.0001), add = TRUE, col = "red", n = 10001)
```

---
## Log-Likelihood

If $x_i \neq x_j$ when $i \neq j$

\begin{aligned}
\ell(f_h) & = \sum_{i} \log\left(1 + \sum_{j \neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \right) - 
n \log(nh\sqrt{2 \pi}) \\
& \sim - n \log(nh\sqrt{2 \pi})
\end{aligned}

for $h \to 0$. 
--

Hence, $\ell(f_h) \to \infty$ for $h \to 0$ and there is no MLE in the set of distributions
with densities.

---
## Log-Likelihood

It actually holds that 
$$f_h \cdot m \overset{\mathrm{wk}}{\longrightarrow} 
\varepsilon_n = \frac{1}{n} \sum_{j=1}^n \delta_{x_j}$$
for $h \to 0$ (weak convergence). 
--


The *empirical measure* $\varepsilon_n$ can sensibly be regarded as 
the nonparametric MLE of the distribution. 


---
## ISE, MISE, and MSE

Quality of $\hat{f}_h$ can be quantified by the *integrated squared error*
$$\mathrm{ISE}(\hat{f}_h) = \int (\hat{f}_h(x) - f_0(x))^2 \ dx = ||\hat{f}_h - f_0||_2^2.$$ 
--

Quality of the estimation procedure can be quantified by the mean ISE,
$$\mathrm{MISE}(h) = E(\mathrm{ISE}(\hat{f}_h)),$$
where the expectation integral is over the data.
--

$$\mathrm{MISE}(h) = \int \mathrm{MSE}_h(x) \ dx$$ 
where $\mathrm{MSE}_h(x) = \mathrm{var}(\hat{f}_h(x)) + \mathrm{bias}(\hat{f}_h(x))^2$.


---
## AMISE

If $K$ is a square integrable probability density with mean 0,

$$\mathrm{MISE}(h) = \mathrm{AMISE}(h) + o((nh)^{-1} + h^4)$$

where the *asymptotic mean integrated squared error* is 

$$\mathrm{AMISE}(h) = \frac{\|K\|_2^2}{nh} + \frac{h^4 \sigma^4_K \|f_0''\|_2^2}{4}$$
with $\sigma_K^2 = \int t^2 K(t) \ dt.$

---
## Asymptotically Optimal Bandwidth

Optimizing $\mathrm{AMISE}(h)$ over $h$ gives the asymptotically optimal 
bandwidth 
$$h_n = \left( \frac{\|K\|_2^2}{ \|f_0''\|^2_2  \sigma_K^4} \right)^{1/5} n^{-1/5}.$$
--

and the asymptotically optimal error 
$$\mathrm{AMISE}(h_n) = C n^{-4/5}.$$

--

Using various plug-in estimates of $\|f_0''\|_2^2$, AMISE can be used to 
estimate the *asymptotically optimal bandwidth* in a mean integrated squared
error sense. 

---
## Asymptotically optimal bandwidth

If we pretend $f_0$ is the density for $\mathcal{N}(0, \sigma^2)$, the optimal
bandwidth becomes 
$$h_n = \left( \frac{8 \sqrt{\pi} \|K\|_2^2}{3 \sigma_K^4} \right)^{1/5} \sigma n^{-1/5}.$$

--

With the Gaussian kernel, $\|K\|_2^2 = 1/(2\sqrt{\pi})$ and $\sigma_K^4 = 1$, and 
the optimal bandwidth can be estimated as 

$$\hat{h}_n = \left(\frac{4}{3}\right)^{1/5} \tilde{\sigma} n^{-1/5}$$

with a (robust) estimate $\tilde{\sigma}$ of $\sigma.$  

--

*Silverman's rule of thumb* uses the bandwidth
$$\hat{h}_n = 0.9 \tilde{\sigma} n^{-1/5}.$$

---
## Amino Acid Angles (Silverman, the default)

.two-column-left[
```{r dens3, eval = FALSE}
density(phipsi$phi)
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens4, eval = FALSE}
density(phipsi$psi)
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi), col = "red", lwd = 2)
```
]

---
## Amino Acid Angles (Scott, using 1.06)

.two-column-left[
```{r dens5, eval = FALSE, fig.width = 5, fig.asp = 1}
density(phipsi$phi, bw = "nrd")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi, bw = "nrd"), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens6, eval = FALSE}
density(phipsi$psi, bw = "nrd")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi, bw = "nrd"), col = "red", lwd = 2)
```
]


---
## Amino Acid Angles (Sheather & Jones)

.two-column-left[
```{r dens7, eval = FALSE}
density(phipsi$phi, bw = "SJ")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$phi, prob = TRUE, main = NULL)
rug(phipsi$phi)
lines(density(phipsi$phi, bw = "SJ"), col = "red", lwd = 2)
```
]

.two-column-right[
```{r dens8, eval = FALSE}
density(phipsi$psi, bw = "SJ")
```

```{r, echo = FALSE, fig.width = 5, fig.asp = 1}
hist(phipsi$psi, prob = TRUE, main = NULL)
rug(phipsi$psi)
lines(density(phipsi$psi, bw = "SJ"), col = "red", lwd = 2)
```
]


