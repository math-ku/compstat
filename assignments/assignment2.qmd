---
title: Assignment 2
subtitle: Univariate Simulation
image: ../assets/images/assignment2.svg
description:
  Implement and compare rejection sampling and importance sampling algorithms
  for specific probability distributions.
---

```{r}
#| include: false
library(ggplot2)
dat <- read.csv(here::here("data", "poisson.csv"))
z <- dat$z
x <- dat$x

# Grid of y values
y_grid <- seq(0, 2, length.out = 200)

# Target log-density and density
log_f <- sapply(y_grid, function(y) sum(y * z * x - exp(y * x)))
log_f <- log_f - max(log_f)
f <- exp(log_f)

# Fit a Gaussian envelope to the target density
mu_hat <- y_grid[which.max(f)]
sigma_hat <- 0.25
gauss_env <- dnorm(y_grid, mean = mu_hat, sd = sigma_hat)
gauss_env <- gauss_env / max(gauss_env) * max(f) * 1.2 # scale for visibility

# Simulate a few samples from the envelope
set.seed(123)
samples <- rnorm(10, mean = mu_hat, sd = sigma_hat)
samples <- samples[samples >= 0 & samples <= 2] # keep in range
df_samples <- data.frame(
  y = samples,
  density = approx(y_grid, gauss_env, samples)$y
)

# Data frame for plotting
df <- data.frame(y = y_grid, density = f, envelope = gauss_env)

ggplot(df, aes(x = y)) +
  geom_area(aes(y = density), fill = "#6baed6", alpha = 0.7) +
  geom_line(
    aes(y = envelope),
    color = "grey40",
    size = 1.5,
    linetype = "dashed"
  ) +
  geom_point(
    data = df_samples,
    aes(y = 0),
    color = "black",
    size = 4,
    alpha = 0.5
  ) +
  theme_void()
ggsave(here::here("assets", "images", "assignment2.svg"))
```

The second assignment topic is univariate simulation. If you draw the topic
"Univariate Simulation" at the oral exam, you will have to present a solution of
one of the two assignments below.

Remember the five points:

- How can you test that your implementation is correct?
- Can you implement alternative solutions?
- Can the code be restructured e.g. by modularization, abstraction or object
  oriented programming to improve readability?
- How does the implementation perform (benchmarking)?
- Where are the bottlenecks (profiling), and what can you do about them?

When comparing simulation algorithms it may be necessary to use the same pseudo
random numbers (set the seed) or to compute results of comparable accuracy as
measured by e.g. the standard error.

Both of the two simulation assignments are made very concrete. You can solve the
assignments by writing code that is very specific, and this is a good way to get
started. But you should (re)write the final solution to be more generic.

# A: Rejection Sampling

This assignment uses the Poisson data available from [here](data/poisson.csv).
It is a table with 100 rows and two variables, $z$ and $x$, where $z$ takes
integer values and $x$ takes positive real values.

The purpose of this assignment is to sample from the probability distribution on
$[0, \infty)$ with density
$$f(y) \propto \prod_{i=1}^{100}  \exp\left(yz_ix_i - e^{yx_i}\right), \quad y \geq 0.$$
Find a Gaussian envelope of $f$ and implement rejection sampling from the
distribution with density $f$ using this envelope.

Implement the adaptive rejection sampling algorithm that uses a piecewise
log-affine envelope and compare it with the one based on the Gaussian envelope.

You are welcome to invent other envelopes and try to optimize the choice of
envelope. However, for the exam the focus should be on the implementations and
their comparisons and not on theoretical derivations of envelopes.

_For those interested, the distribution given by $f$ above is a simple example
of a Bayesian posterior distribution in a Poisson regression model._

# B: Importance Sampling

Let $X_1, X_2, \ldots$ be i.i.d. uniformly distributed on $(-1.9, 2)$ and define
$$S_n = 30 + \sum_{k=1}^n X_k.$$ Think of $X_k$ as the capital that a company or
a gambler earns or loses at time $n$. Starting with a total capital of 30, $S_n$
is the total capital at time $n$. We are interested in computing the _ruin
probabilities_ before time $n$: $$p(n) = P(\exists k \leq n: S_k \leq 0).$$

Implement Monte Carlo integration to compute $p(100)$.

Define
$$g_{\theta, n}(x) = \frac{1}{\varphi(\theta)^n} \exp\left(\theta \sum_{k=1}^n x_k\right)$$
for $x \in (-1.9, 2)^n$ where
$$\varphi(\theta) = \int_{-1.9}^2 e^{\theta z} \mathrm{d}z.$$ Implement
importance sampling to compute $p(100)$ by sampling from $g_{\theta, 100}$ for
suitable $\theta$. Is there an optimal choice of $\theta$?

_Note that you should consider number of samples as well as run time to discuss
what "optimal" means._
