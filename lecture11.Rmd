---
title: "Gaussian Mixtures and Mixed Models with the EM Algorithm"
subtitle: "Computational Statistics"
author: "Johan Larsson, Niels Richard Hansen"
date: "October 8, 2024"
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 5,
  fig.retina = 3,
  fig.align = "center",
  cache = FALSE,
  autodep = TRUE,
  dev.args = list(pointsize = 16),
  crop = TRUE
)

library(animate)
library(tidyverse)
library(patchwork)

xaringanExtra::use_tile_view()

set.seed(1422)

theme_set(theme_grey(base_size = 18))
```

## Today

Continue with the EM algorith. Two examples.

### Gaussian Mixtures

Hands-on example with Gaussian mixtures

### Mixed Models

Example on mixed models

---

class: center, middle

# Gaussian Mixtures

---

## Gaussian Mixtures

The marginal density is

$$f(x) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(x - \mu_1)^2}{2 \sigma_1^2}} +
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(x - \mu_2)^2}{2 \sigma_2^2}}.$$

and we will regard $\theta = (p, \mu_1, \mu_2)$ as the unknown parameters,
while $\sigma_1$ and $\sigma_2$ are fixed.

--

$$Q(\theta \mid \theta')  = \sum_{i=1}^n \hat{p}_{i} \left(\log(p) - \frac{(x_i - \mu_1)^2}{2 \sigma_1^2} \right) + (1 - \hat{p}_{i})\left( \log(1-p) - \frac{(x_i - \mu_2)^2}{2 \sigma_2^2} \right)$$

where $\hat{p}_i = P_{\theta'}(Z_i = 1 \mid X_i = x_i)$,

--
which attains its maximum in

$$\theta = \left(\frac{1}{n} \sum_{i} \hat{p}_i, \frac{1}{\sum_{i} \hat{p}_i} \sum_{i} \hat{p}_i x_i,
\frac{1}{\sum_{i} (1 - \hat{p}_i)} \sum_{i} (1 - \hat{p}_i) x_i \right).$$

---

### Simulation

```{r}
sigma1 <- 1.5
sigma2 <- 1.5 # Note, same variances

p <- 0.5
mu1 <- -0.5
mu2 <- 4

n <- 5000
set.seed(321)
z <- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p))

# Conditional simulation from the mixture components
x <- numeric(n)
n1 <- sum(z)
x[z] <- rnorm(n1, mu1, sigma1)
x[!z] <- rnorm(n - n1, mu2, sigma2)
```


---

class: center, middle

```{r, echo=FALSE, fig.height=6, fig.width = 9}
gausdens <- function(x) {
  (p * exp(-(x - mu1)^2 / (2 * sigma1^2)) / sigma1 +
    (1 - p) * exp(-(x - mu2)^2 / (2 * sigma2^2)) / sigma2) / sqrt(2 * pi)
}
xx <- seq(-3, 11, 0.1)
hist(x, freq = FALSE, ylim = c(0, 0.25), main = "")
lines(xx, gausdens(xx), col = "dark orange", lwd = 2)
```

---

### The E-Step

$$\hat{p}_i = P_{\theta'} (Z_i = 1 \mid X = x_i) =
\frac{ p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}}}{
\left( p'_1 e^{-\frac{(x_i - \mu_1')^2}{2 \sigma_1^2}} +
\frac{\sigma_1 (1 - p'_1)}{\sigma_2} e^{-\frac{(x_i - \mu_2')^2}{2 \sigma_2^2}}\right) }$$

---

### EM Algorithm

```{r}
source("R/em_gauss_mix_exercise.R")
EM <- EM_gauss_mix(x)
```

Download the [source code here](R/em_gauss_mix_exercise.R)
--

```{r}
EM(c(0.4, -0.2, 4.5))
EM(c(0.9, 1, 2))
```

--

What happens when evaluating the following, and why?

```{r, eval = FALSE}
EM(c(0.6, 3, 1))
```

---

## Exercise

- Profile the EM-algorithm.
- Set `n` to 500000 to get meaningful
profiling information
- Remember to source `em_gauss_mix_exercise.R`
  to make source code available to the profiler.

---

## Gradients and numDeriv

Recall that
$$\nabla_{\theta} \ell(\hat{\theta}) = \nabla_{\theta} Q(\hat{\theta}, \hat{\theta}).$$

--

```{r q-def}
Q <- function(par, par_prime, EStep) {
  phat <- EStep(par_prime)
  p <- par[1]
  mu1 <- par[2]
  mu2 <- par[3]
  sum(phat * (log(p) - (x - mu1)^2 / (2 * sigma1^2)) +
    (1 - phat) * (log(1 - p) - (x - mu2)^2 / (2 * sigma2^2)))
}
```

--

```{r numderiv-load}
library(numDeriv)
# Note that loglik() is implemented as the negative log-likelihood
grad1 <- function(par) grad(function(par) -loglik(par, x), par)
grad2 <- function(par) {
  grad(Q, par, par_prime = par, EStep = environment(EM)$EStep)
}
```

---

## Checking the Gradient Identity

```{r grad-identities-1}
grad1(c(0.5, 0, 4))
grad2(c(0.5, 0, 4))
```

--
```{r grad-identities-2}
par_hat <- EM(c(0.5, 0, 4))
grad1(par_hat)
grad2(par_hat)
```


---

## Fisher Information

Let $\hat{i}_X = - D^2_{\theta} \ell(\hat{\theta})$ denote the observed
Fisher information.

--

Then

$$\hat{i}_X = - D_{\overline{\theta}} (
\nabla_{\theta} Q(\overline{\theta} \mid \overline{\theta}))
|_{\overline{\theta} = \hat{\theta}}$$

--

$$\hat{i}_X = - D^2_{\theta} Q(\hat{\theta} \mid \hat{\theta}) -
D_{\theta'} \nabla_{\theta} Q(\hat{\theta} \mid \hat{\theta})$$

--

$$\hat{i}_X = - (I - D_{\theta} \Phi(\hat{\theta})^T) D^2_{\theta} Q(\hat{\theta} \mid \hat{\theta})$$
where
$$\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta')$$
is the EM-map.

---

## Exercise

Implement the three methods for computing the Fisher information.
Compare with (numerical) differentiation of the log-likelihood.

---

class: center, middle

## Mixed Models

---

## Mixed Model

$$X_{ij} = \beta_0 + \nu Z_i + \varepsilon_{ij}$$

for $i = 1, \ldots, m$, $j = 1, \ldots, n_i$, $Z_1, \ldots, Z_m$ i.i.d.
$\mathcal{N}(0, 1)$, independent of $\varepsilon_{ij}$-s that are themselves
i.i.d. $\mathcal{N}(0, \sigma^2)$.

--

The parameter $\theta = (\beta_0, \nu, \sigma^2) \in \mathbb{R} \times (0, \infty)^2$
is three-dimensional.

--

The model has two components:

$$\mathbf{Z} \sim \mathcal{N}(0, I_m)$$

and with $n = n_1 + \ldots + n_m$

$$ \mathbf{X} \mid \mathbf{Z} \sim \mathcal{N}(\beta_0 \mathbf{1} + \nu A \mathbf{Z}, \sigma^2 I_n)$$

where $A$ is the $n \times m$ matrix with $A_{ij, i'} = \delta_{ii'}.$

---

## Complete Data Log-Likelihood

Observing $(\mathbf{X}, \mathbf{Z})$ gives the likelihood

$$\prod_{i=1}^m \prod_{j=1}^{n_i} \frac{1}{\sqrt{2 \pi \sigma^2}}
e^{- \frac{(x_{ij} - \beta_0 - \nu z_i)^2}{2 \sigma^2}} \frac{1}{\sqrt{2 \pi}}
e^{- \frac{z_i^2}{2}}.$$

--

Or the log-likelihood up to an additive constant that doesn't depend on the
parameters

$$\ell(\theta) =  - \sum_{i=1}^m \sum_{j=1}^{n_i} \frac{(x_{ij} - \beta_0 - \nu z_i)^2}{2 \sigma^2} - \frac{n}{2} \log \sigma^2.$$

--

We recognize this as a log-likelihood for linear regression, which can also be written
as
$$\ell(\theta) = - \frac{1}{2\sigma^2} \left( \| \mathbf{X} \|_2^2 -
2(\beta_0, \nu) \mathbb{X}^T \mathbf{X} +  (\beta_0, \nu) \mathbb{X}^T \mathbb{X} \left(
\begin{array}{c} \beta_0 \\ \nu \end{array} \right) \right) - \frac{n}{2} \log \sigma^2$$
where $\mathbb{X} = \left[ \mathbf{1} \ \ A \mathbf{Z}\right].$

---

## The $Q$-Function

Taking conditional expectations w.r.t. $\theta'$ gives

\begin{align*}
& Q(\theta \mid \theta') = E_{\theta'}( \ell(\theta) \mid \mathbf{X}) \\
& = - \frac{1}{2\sigma^2} \left( \| \mathbf{X} \|_2^2 -
2 (\beta_0, \nu) E_{\theta'}( \mathbb{X}^T \mid \mathbf{X}) \mathbf{X} +  (\beta_0, \nu) E_{\theta'}( \mathbb{X}^T \mathbb{X} \mid \mathbf{X})  \left(
\begin{array}{c} \beta_0 \\ \nu \end{array} \right) \right) - \frac{n}{2} \log \sigma^2
\end{align*}
--

With $\hat{\mathbb{X}}= E_{\theta'}( \mathbb{X} \mid \mathbf{X})$
and $\hat{S} = E_{\theta'}( \mathbb{X}^T \mathbb{X} \mid \mathbf{X}),$ (Note: $\hat{S} \neq \hat{\mathbb{X}}^T \hat{\mathbb{X}}$)

$$Q(\theta \mid \theta') = - \frac{1}{2\sigma^2} \left( \| \mathbf{X} \|_2^2 -
2(\beta_0, \nu) \hat{\mathbb{X}}^T \mathbf{X} +  (\beta_0, \nu) \hat{S}  \left(
\begin{array}{c} \beta_0 \\ \nu \end{array} \right) \right) - \frac{n}{2} \log \sigma^2.$$
--

Which is maximized in
$$\left(\begin{array}{c} \hat{\beta}_0 \\ \hat{\nu} \end{array} \right)  =
\hat{S}^{-1} \hat{\mathbb{X}}^T \mathbf{X}, \qquad \hat{\sigma}^2 =
\frac{1}{n} \left( \| \mathbf{X} \|_2^2 -  \mathbf{X}^T \hat{\mathbb{X}} \hat{S}^{-1} \hat{\mathbb{X}}^T \mathbf{X} \right)$$


---

## Conditional Expectations

With $\mathbb{X} = \left[ \mathbf{1} \ \ A \mathbf{Z}\right],$ we need to compute

\begin{align*}
\hat{\mathbb{X}}= E_{\theta'}( \mathbb{X} \mid \mathbf{X}) =
\left[ \mathbf{1} \ \ A E_{\theta'}( \mathbf{Z} \mid \mathbf{X}) \right]
\end{align*}

--

and
\begin{align*}
\hat{S} & = E_{\theta'}( \mathbb{X}^T \mathbb{X} \mid \mathbf{X}) = E_{\theta'}\left(
\left[ \begin{array}{c}  \mathbf{1}^T \\ \mathbf{Z}^T A^T \end{array} \right]
\left[ \mathbf{1} \ \ A \mathbf{Z} \right] \ \Bigg| \ \mathbf{X} \right) \\ &=
\left[ \begin{array}{cc}  n & \sum_i n_i E_{\theta'}(Z_i \mid \mathbf{X}) \\
\sum_i n_i E_{\theta'}(Z_i \mid \mathbf{X}) & \sum_i n_i E_{\theta'}(Z_i^2 \mid \mathbf{X}) \end{array}\right]
\end{align*}

--

which amounts to conditional expectations and conditional second moments of
the $Z_i$-s given $\mathbf{X}$.

---

## Conditional Expectations

By computing variances and covariances of $\mathbf{Z}$ and $\mathbf{X}$ we
find that
\begin{align*}
\left(\begin{array}{c} \mathbf{Z} \\ \mathbf{X} \end{array} \right) \sim
\mathcal{N}\left( \left(\begin{array}{c} 0 \\ \beta_0 \mathbf{1} \end{array} \right),
\left(\begin{array}{cc} I_m & \nu A^T \\ \nu A & \nu^2 A A^T + \sigma^2 I_n \end{array} \right) \right)
\end{align*}


--

which gives

$$\mathbf{Z} \mid \mathbf{X} \sim
\mathcal{N}(\nu A^T (\nu^2 A A^T + \sigma^2 I_n)^{-1}(\mathbf{X} - \beta_0 \mathbf{1}),
I_m - \nu^2 A^T (\nu^2 A A^T + \sigma^2 I_n)^{-1} A).$$

--

So

\begin{align*}
\xi & := E_{\theta'}( \mathbf{Z} \mid \mathbf{X}) = \nu' A^T ((\nu')^2 A A^T + (\sigma')^2 I_n)^{-1}(\mathbf{X} - \beta_0' \mathbf{1})\\
\zeta_i & := E_{\theta'}( Z_i^2 \mid \mathbf{X}) = 1 - (\nu')^2 \left(A^T \left( (\nu')^2 A A^T + (\sigma')^2 I_n \right)^{-1} A \right)_{ii} + \xi_i^2
\end{align*}


---

## Maximizing Q

```{r}
max_Q <- function(xi, zeta, x, ni) {
  # t is hat{X}^T x, where hat{X} depends on xi
  t <- c(sum(x), sum(rep(xi, times = ni) * x))
  ni_xi <- sum(ni * xi)
  S_hat <- matrix(
    c(
      sum(ni), ni_xi,
      ni_xi, sum(ni * zeta)
    ),
    ncol = 2,
    nrow = 2
  )
  hat <- solve(S_hat, t)
  sigmasq_hat <- (sum(x^2) - sum(t * hat)) / sum(ni)
  c(hat, sigmasq_hat)
}
```


---

## Setup and Test

```{r setup-experiment}
beta0 <- 1
nu <- 5
sigma <- 1

m <- 20
ni <- rep(10, m)
n <- sum(ni)
```
--

```{r test}
z <- rnorm(m)
mu <- beta0 + nu * rep(z, times = ni)
x <- rnorm(n, mu, sigma)

max_Q(z, z^2, x, ni) # This gives the regression solution
```
--

```{r test2}
lm_test <- lm(x ~ rep(z, times = ni))
c(coefficients(lm_test), mean(residuals(lm_test)^2))
```

---

## Computing the Conditional Expectations

```{r}
cond_exp <- function(par, x, A, AAt) {
  beta0 <- par[1]
  nu <- par[2]
  sigmasq <- par[3]
  m <- ncol(A)
  n <- nrow(A)
  Sigma <- nu^2 * AAt + diag(sigmasq, n)
  xi <- nu * drop(t(A) %*% solve(Sigma, x - beta0))
  zeta <- 1 - nu^2 * diag(t(A) %*% solve(Sigma, A)) + xi^2
  list(xi = xi, zeta = zeta)
}
```

--

How are we going to construct $A$?

--

We borrow a block-diagonal constructor from the Matrix package.

```{r}
A <- Matrix::bdiag(lapply(ni, function(d) matrix(1, nrow = d, ncol = 1)))
```

---

### Visualizing the Matrix $A$

```{r, fig.width = 7, fig.height = 6, echo = FALSE}
Matrix::image(A, aspect = 1)
```

---

## Test (Approximately ...)

```{r A-ez}
A <- as.matrix(A)
Ez <- cond_exp(c(beta0, nu, sigma * 2), x, A, A %*% t(A))
```

--

```{r plotting-Ez, echo=FALSE, fig.height=5, fig.width=11, echo = FALSE}
dat <- tibble(z = z, xi = Ez$xi, zeta = Ez$zeta)
p1 <- ggplot(dat, (aes(z, xi))) +
  geom_point()
p2 <- ggplot(dat, (aes(z, zeta))) +
  geom_point()
p1 + p2
```

---

### Implementing the EM Algorithm

```{r EM-implementation}
EM_mixed <- function(x, ni) {
  force(x)

  A <- as.matrix(Matrix::bdiag(
    lapply(ni, function(d) matrix(1, nrow = d, ncol = 1))
  ))
  AAt <- tcrossprod(A)
  e_step <- function(par) cond_exp(par, x, A, AAt)
  m_step <- function(zz) max_Q(zz$xi, zz$zeta, x, ni)

  function(par, epsilon = 1e-10, cb = NULL, maxit = 1e4) {
    for (i in 1:maxit) {
      par0 <- par
      par <- m_step(e_step(par))
      if (!is.null(cb)) cb()
      if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon)) {
        break
      }
    }
    par
  }
}
```

---

## Test

```{r EM}
EM <- EM_mixed(x, ni)
```

```{r EM-run, cache = TRUE}
par_hat <- EM(c(beta0, nu, sigma^2))
par_hat
```
```{r lmer, message=FALSE, cache = TRUE}
library(lme4)

mixed_data <- data.frame(x = x, Subject = factor(rep(1:m, times = ni)))
mixed_lmer <- lmer(x ~ (1 | Subject), data = mixed_data, REML = FALSE)
```

.pull-left[
```{r lmer-effects}
fixef(mixed_lmer)
```
]

--

.pull-right[
```{r var-corr}
VarCorr(mixed_lmer)
```
]

---

## The Log-Likelihood

Since

$$\mathbf{X}  \sim \mathcal{N}\left( \beta_0 \mathbf{1}, \nu^2 A A^T + \sigma^2 I_n \right)$$

--

the log-likelihood when observing $\mathbf{X} = \mathbf{x}$ is

\begin{align*}
\ell(\beta_0, \nu, \sigma^2) = & - \frac{1}{2} (\mathbf{x} - \beta_0 \mathbf{1})^T
(\nu^2 A A^T + \sigma^2 I_n)^{-1} (\mathbf{x} - \beta_0 \mathbf{1}) \\
& - \frac{1}{2} \log \det (\nu^2 A A^T + \sigma^2 I_n ).
\end{align*}
--

It is straightforward but does involve the computation of a log-determinant.


---

## Implementing the Negative Log-Likelihood

```{r loglik}
loglik <- function(par, x, AAt) {
  beta0 <- par[1]
  nu <- par[2]
  sigmasq <- par[3]
  Sigma <- nu^2 * AAt + diag(sigmasq, length(x))
  log_determinant <- determinant(Sigma)$modulus
  (drop((x - beta0) %*% solve(Sigma, (x - beta0))) + log_determinant) / 2
}
```

--

```{r optim-par, cache = TRUE}
par0 <- c(beta0, nu, sigma^2)
optim(par0, loglik, method = "BFGS", x = x, AAt = tcrossprod(A))[1:3]
```

---

## Tracing

```{r EM-init, cache = TRUE}
par_hat <- EM(c(beta0, nu, sigma^2), epsilon = 1e-20)
loglik_min <- loglik(par_hat, x, A %*% t(A))
loglik_min
```

```{r EM-trace, cache = TRUE}
library(CSwR)
EM_tracer <- tracer(
  "loglik",
  Delta = 0,
  expr = quote(loglik <- loglik(par, x, tcrossprod(A)))
)
par_hat2 <- EM(c(0, 4, 1), epsilon = 1e-16, cb = EM_tracer$tracer)
```

```{r loglik-min2}
loglik(par_hat2, x, tcrossprod(A))
```

---

## Tracing

```{r EM-trace-plot, echo = FALSE}
autoplot(EM_tracer, y = loglik - loglik_min)
```

---

## Summary

### Gaussian Mixtures

### Mixed Models



